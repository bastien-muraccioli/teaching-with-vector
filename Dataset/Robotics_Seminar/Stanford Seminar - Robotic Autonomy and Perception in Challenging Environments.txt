a time is robotics and perception group large Bay away large prehistoric day away from here and so and I'll be talking today a little bit about some work that's going on in my group of robotic perception especially in challenging environment also types of different types of environments but before we did that we have to show a video about everyone seen this video it's like not do right mitosis ok that's everyone I expect and we look at this in like ok while it must be sorted by 10 engineering and stuff like this and it probably is in state feedback my tyres successful events trade so they probably had 100 of these videos and Richard failed previously and so this is but still pretty impressive even like the arrogance that the platform can show about putting a tent in the air play types of different examples for robotic autonomy vs some work out in Georgia tech starter rally car fully a few years old at this point using differential GPS to be able to drift around corners all type of vehicle 1/5 scale vehicle they have essentially you know like a small Mariana in GPU at all I'm bored it's up for the GPS can't of course this is hit the news congratulations my business so exciting to see really has captured the attention imagination of many people being able to do these kinds of aggressive Maneuvers on full-size vehicles is even more challenging than being able to do it on his toy vehicles like what we doing my group and this is some work that we do goes back about 6 years ago at this point being able to drive is small ground vehicles kind of terrain part types of environments and again every person in the room that is done any sort of research in Rabat that's ok pretty impressive manoeuvres but also they can't help but look at the motion capture system in realise that even through these types I like being able to jump and everything like that the Moorcock is providing as 1 kHz state feedback it almost perfect accuracy what we would normally crown true that perception of violence against obviously is to be able to remove all of the different external types of state estimation capabilities this type of problem and be able to today's results in wildinvite haven't been before where maybe we have been but we don't have any kind of external fiducial systems involved is Arthur Fairley Rosie Pig arceaux Fairly Odd work about 7 years ago being able to do a 3D reconstruction with a simple stereo camera and a song 3D models fully fused environment things that you can then import into game engines and be able to simulate physics over actually build models the environment like what we have here say how in a ball Fritz interact with this environment and if a ball then why not a car go back and if you will up to bed Hall control system that I was showing about that car that was jump doing the loo simulating this this model of a car over the 3D fused model that we have built from Prosecco give me that we're doing that state feedback through motion capture in case you're wondering what has happened you see the sky in sort of like you know straight from the vehicle at the front of it does all different like simulated world's what would happen if I just had a little bit more gas turned me the ride on this particular point in time talking of it and we choose our best winding frequently called stochastic model predictive control all stuff that many of us have seen before right and if you've been hurt is there different kinds of capabilities that have been there for years don't see too many places where we can play this to the wild show like this is actually fairly difficult to do it's not like this is a human operator attempting to drive the vehicle to the sloop Inn here's a well paid poster nearly getting his head cut off because of this card is going to run into on yeah I'm doing some superhuman capability as well I just to recall for you that's that's that's graduate student attempting to drive that car through that that loop a graduate student to a pharmacy dispenser little bit too much time driving this car around because it's so much fun where's the computer can do it without any problem it all right very reliable and robust Lane so the question then becomes where are these autonomous vehicles have given you some medication that you know there's quite a bit of promise see in the field of a time is robotics today we have reception in the all the great inventions that happened there we are controlling I've shown you some superhuman like types of control and planning capabilities so where are these the Times vehicles in let me show you another video of you know maybe the state-of-the-art if you will where you know everyone's seen this video from the darpa robotics challenge back in 2015 and some of the best teams that build robots trying to feel them in this really high Stakes environment and having failure modes in never seen before and not being able to relate to challenges they would hope and you see graduate students and postdocs in this video realising that you have to work for years longer degrees are pushed out that much further play my many of you probably working different areas hear some of your work in Perception then you're going to say these are up reception challenges or control and saywell inverted pendulum problems in uncertain uneven terrain a very difficult look at you know different types of human supervisory interfaces human factors the ways that humans are interacting with his robot provided commands are planning methods are all I thought so I can't really blame just one it's because the interplay between these components is so complex and it's really high-dimensional robbery dealing time why you no give a little bit more research and and is fairly human teacher's salary for off of course is a procession person I'm interested actually in this particular scene does anyone know what happened in the scene when did so there is no talk to balance the work that they replying and so they thought I'd over right kind of interface between perception and control right it's clearly there's some there's some Senses here and the sensor is got this robot to the red valve maybe weren't in the right place when can argue or maybe they were in the right place but no no there was some sort of behaviour rotary issue or something like that there is no reception fed back into the controller that said make sure that your past is actually on the red bell so it's all over vehicles which is something that is near and dear to my heart still we look at Proms like this when we say ok there's a camera may be multiple cameras and light are and everything else human we might be saying we could we could see that just looks fairly you know mundane kind of driving situation but with a little bit closer because of this lens flare that happens right here as you move forward then there's some weird effects of the auto exposure on the camera also revealing that their address head lice you're just a little tiny like problems associated with where are you looking how are you looking and are you actually taking into account the kind of things could result in catastrophe for you so this is more motivation on meant to be able to scale to environments of arbitrary complexity so driving in India never died before sensory overload we have here's how would we be able to program a tennis agent in environment where we have to each of the possible future directions of this kind of these agents here like sitting in a in a self-driving car yesterday and I was truly impressed that the ability that are able to identify all the the pedestrians in the scene like at one point there is like a head that was like going into a subterranean passageway and I was able to tell you even in there that there is this person it was like 50 m away doing the sort of the Catholic model predictive control overall future trajectories of these agents a step further asking questions like is this person angry can I apply rational models of Russia drive attention is this sort of like you're not on the phone or just scratching his ear vehicles Kettering because their weight down by certain amount of load ability and also questions like you know should I even listen to this stop sign is it visible by all parties and obeyed I'm out from all this is that the world is not like this nice all environment in many ways we've sorted look at stochastic model predictive control in the reduction of perception in state estimation into the inputs to that control system how old is Nat this you know sorry nice rhodri nothing's going to happen and maybe very few things were happening this play my David bit difficult for you living in California world is more like this weather is snow in different kind of disabled vehicles might sit on the road you have to start asking you know like can we actually model every single scenario in which for instance 2 vs 3 vs 5 people are out pushing a vehicle on the road if you're driving in this how do we know what kind of control is to apply the stochastic model predictive control framework that I was talking about before requires are knowing at least some idea about water friction coefficients are for instance ahower interactions between wheels and the ground again complicated scenarios for contact anyone Media Router types challenging to be older model end so I did now not buy self-driving cars but I would dark but he's paying me to be interested in truly remarkable my opinion type of environment the subterranean in select about before with this yuno entrance to a cave dedicate are you required to enter for the subterranean are putting teams of robots in two minds tunnels urban underground caves to be able to find certain artifacts which claimed is artefact which might be things like red backpacks fire extinguishers survivors sermon if you look at this environment say I am super excited to send robots into aqueous environments number one cause of catastrophe at subterranean challenge that just happened in the future once was water falling on our computer the environments look like this where they might not be this sort of flour in here but instead if you know there's going to be small and large types of that we have to negotiate I met you could imagine putting in a ground vehicle but maybe an aerial vehicle would be a better choice teams of robots in which means that we can choose what kind of Locomotion would like if you ever do any kind of private flying time is quite a flying in something like this you can imagine things small articles that you wouldn't be able to see necessarily with a light or even a very high density light I also want to in particular dry your attention to a couple of features on this image right here highlighted a result of a light being too close to the geometry of the walls here and then also reflections is a result of just specular invision frighten is the places where we don't have gard model more about that do I want to motivate just a little bit more about this subterranean challenge here we are in the systems competition in my team track a which is one of the Dartmouth on the teams we just finished this tunnel circuit which is going environments not two different than the one I just showed also that we have this urban circuit next and then finally a cave circuit 6-months after that this is going to be in February and then finally a year to sort of window down our methods and then get them to assign the final system there's also a virtual competition or not part of one of those were just in this one will track appear just explain to you where this sort of competition is this team is to sort of hop and chiefly solve some of the procession challenges associated with operate the normal ways that I will try to using senses Suites like centrifuge little bit more than just comment filtering we usually use optimisation based approaches in a few again work in this field the difference between the two is kind of saddle I suppose in the end it's taking all these different types of data today products that then we can do some sort of estimation over cameras brbc providers images which provide us information on both where we are and what's around us useful information if available give this only suit arranging positions photos might be on Wheels or something or be able to tell what sort of powder impute the way that the physical model of your robot would be able to motion prediction unit switch coming all shapes and sizes but in general they are useful for a rotation rate and acceleration different types of white our Solutions which give us Direct measurements on surfaces in in a 3D space potentially 2D space although we are used to regulators in Ireland in Ireland self-driving cars the truth like a very motivational case right I mean the bounds of what is available for centrifuge actually everything that you can imagine they also have GPS on top of this century sweet here for different types of capability capabilities provided that sort of really high highly accurate information pedestrian example but they took very far away may not be only dependent on a camera but also depending on sorry of tessellation of these different sentence they all work together if they were interested in what are the chief problems in moving a robot nano environment removing that motion capture Toolstation Aston to do this one of the typical tools that's used in well known in the literature analysis visual simultaneous localisation and mapping or Slam what part has a lot of different methods indirect methods sparse indirect methods which revolves around being able to compare computer vision frame being able to capture a frame from a camera calculate where there are features being able to match those features in time and being able to as you can imagine estimate Harry moving against those features in Anavar you might be able you navigation against corners industry environment like same so IKEA is the sort of features in as attract across friends then here is what we call progress this little bit of that sensor fusion technology I was talking about visual inertial Slam the Vision and the introduction of an imu places where let's say there aren't many features like on the ground right here is able to help us sort of work through places where we might not have any future correspondence between frames particularly being a cheers if you get motion or so let's just take a moment and appreciate those two sensors reminder to post constraints that is framed a frame tracking as well as information on structure that landmark data please provide a some strong short-term spatio-temporal constraints of the body for us to be able to afford propagate or it's frames collected in weathering that dropped a frame because there is entirely motion blur or maybe was overexpose examples of places that I am yours absolutely credit is Wonder all problem right in this is sort of going to get us introduced into the world of sensor fusion and perception today have an imu singing Summer arbitrarily on your body and you have a camera that is also sitting somewhere on that robot body ideally there's going to be some static transport between the two going to sensor calibration before almost every moving a reg right and being able to determine moving ring again and being able to address certain quantities of Interest being able to VR sets xy0 picture differences between the two maybe intrinsic of the camera as well chance for distortion may be focal point in Central what graduate students have to do every day that say that they want to be able to run when the procession legs it's extremely heroin and time-consuming so so introduces perhaps one of the least sexy but also one of the most lucrative parts of doing process nobody wants to do and everyone needs to do it in order to use multiple sensors papers and different grants and Cooperative research agreements between companies particular types of prams on the fly to tell you about one of them right now you would have to do this is ok fine let's say you're ok with an engineer doing that Hall sensor ripped out in the field decides that they would like to move one of the cameras with respect to the other sensors on the planet perhaps that you're doing some you not aggressive Manoeuvres in now you ran into something or you had a hard stopping that might actually result in a different kinds of different calibration are changing calibration forever something unexpected happens and as a result sensor supply and what you don't want to do is have to send your car or your rig back to the shop continue app Reading maybe not in this example trainer solver camera imu self-calibration problem authorises removing in the mind but also was helpful all the way back when you're working with Toyota in this case what computers camera imu rotation in Translation that say that your camera intrinsics are nice and I cannot change anything because we don't have any calibration target operating when you do this in real-time and robust to the sort of changes that happen on the fly solve this problem is by collecting the most informative motion segments to estimate the parameter changes that the gate Odyssey segments informative mess that is to say we asked the question is this new information that we just it disagrees with our previous hypothesis what are sensor calibration parameters where this was the I think one of the more robust Solutions that we came we are moving our robot just kind of evolving in whether we were moving to the cinema some sample of the motions of the robot and with those motions of the robot we taken estimate as to what are calibration parameters are this involves msr7 optimization which results in is area is indicative of water uncertain 2-years on calibration primers do a dad play these three motions emotions as well as the estimate for the calibration parameters into a priorityqueue like this as we've all the robot forward we're going to be collecting more and more motions some of which may not be informed legend in this case it's not always that every motion that you take is actually going to help you progress these parameters in sensor calibration here's one example in which patient was not sold wider spread on water estimation would have been for the calibration Prem turn the priority queue anyway then we continue to evolve this works until the priority queue is full which we run an estimation over all the different motions in this priority queue in order to get our calibration parameters continue evolving a robot just normally operating question he was 4 and we have this new motion information this new motion sample Howard formula business motion Singapore against previous previous versions you can say as you can see that the there is actually this one is slightly more narrowly picked if you will let us to say it's more informative it is less uncertain about that is than one of the motion symbols still in the priority queue so we popped out and we throw it in react play just one particular way that we came up unable to do this online self calibration can we throw away other less informative samples as we go can you take away here is that ok well we can actually run this robot listen to have somebody move the move the sensor with respect to the other senses and then be able to say so it turns out that all of our previous sensor calibration over previous motion samples are wrong that means the same now it's been a change and our calibration parameters need to be Regis actually does work in real-time Samba Bristol not losing tracking entirely we have normally we get to this point coronavirus live solutions are actually converging Butlins case reliable to recalibrate and move forward step back on a metal memo here maybe you don't care about such a calibration by many of us we just hope that it can be soft takeaway here is there action in form another certain motion goodnight I'm doing that we've sort of identified region ok we need a move certain way in order for us to have robust but also probably will you be able to get robust perception not a certain action is avail don't have it there's too much uncertainty on a map snap uncertainties will cascade into her being able a very good gases to have work in a certain environment what's the temperature that would fail Fairley like general taker sing I'm using this one example that I provided is just 11 sort of tableau if you will for this life another example of Woolworths unicentro calibration you cannot get miners ouigo foreign is again on the subterranean here's our source of most commonly used robot platform for prototyping is called parkour car in this environment it's dark just like the mind that we expect to be operating in attached a light on the top of the does it need to see where it's going right there's no light also have this camera the first images when the robot was sort of maybe a few metres further away from it now with move Closer and what we've done is catching another image what is volleyball play some happened now that there's a very Direct specular highlight on the volleyball if we were to look at this valuable and say where are my computer vision features you spot Buchanan this looks like a white surface suppose all the texture in features on this volleyball play Muse I mean because you saying ok well maybe the colours worship in the image s is that the way that the Bible looks apparently is any kind of Direct image matching you would do how does my scene look as I'm moving forward apply that anymore because the sea has changed merely by emotional that light on on top of you not I'm in circumstance in nimes you the same video of Us walking through mine any of your family with that disaster taken 1 flashlight which is sort of diffuse here is mine another example just walking to remind with a different flashlight lower setting stop brightness setting here is a few more sort of shiny surfaces a lot more texture he is dead but the throw of the white is not quite as good you so much about the geometry for the road projector you can see in the letter visibility range final when is you know if you've got one light was maybe what change the setting on one light four brightness here's two lights that you could use sort of one that's focused down the cabin and then one that's all play la wider angle to be able to hopefully capture both of them but that was even more confusing right we just looking at that video and it's like wow what's moving around me because of white removing with respect to one another the light please against the geometry here in order to create Shadows as well as be able to as the images being generate texture is what is different kind of geometry is what we want to discuss no-one's really look at this before because I believe no one wants use visual methods and caves show methods of sort of what humans can use so I want to be able to robots yellow scientists does it say I'm going to the testing any of my methods that I'm going to be developing here that we took was types of environments with your talk about shortly urban environment in an outdoor environment we also worked in with different types of sensors Catherine potato with a realsense d435i which if you don't know is a realsense d435 it gives set up by 722 of these Brits 34 second data on each one of those cameras thank you with work with one of these cameras that you sort of know where there's an RGB as well but the arteries rolling Centre I am use one of them is the one that's inherit on the realsense d435i it's sort of like the boss I am you in case you ever use that one but what's actually on it but it's sort of like that 250 400 a much nicer I am you but thousand dollars 3dmg x 515 from Lord microstrain give Shirley up to 1000 weather in Jarrow but it just much more stable in Lower bioscript as well as much as we could some position according to a like a total station I'll talk about the sec as well as a light that we change the different illumination settings on you can see here what sort of examples of environments that we took this dataset am I allowed in an hour and videos of dolls in a second and ok so let's talk about prism Acura in order for us to get ground truth for as long as the Leica where was available as you can imagine if we operate in mine's and we're going around corners move the Leica station that means that only portions of this photograph all actually have ground-truth according to the Leica I'm bored is camera the two cameras IKEA lots of batteries for all the computer and everything in the imu both in here as well as the Lord microstrain behind tell you this light year and in this light just a pretty typical LED with a massive heat sink that would create enough keep the minds that were in with a fan on the back to make sure that we didn't like overheat the ideas so where was Craig Douglas basically the Light of Day violence that we are operating in here is a steam tunnel type of environment in our underground University of Colorado this was taken at the brightness settings different lumen settings on a recreate the pad as well as possible but as you can see we are walking around this does have auto exposure enable the lights will be calibrated and also that injustice urban anagram a tunnel cave type of environment that we are operating in and then finally to make this the creepiest dataset ever collected project style Outdoors type of round here we tried to capture a bunch of different scenarios and which one want to do active illumination we tried to capture those different settings with different types of motion blur types of character instance if your operating at the brightest setting it might be that you don't get as much emotion buyer but at the lower setting it might look like you have the same image but there's more emotional are so there's us this is about the status like I mentioned as we did that light intrinsic because if you're wondering what we're about to do who is the light model that we can actually try to back out as a result you know I guess a pattern here how the light is sort of focused in the seen and then understand how that plays against the geometry in the scene to create images this is the normal setup that we did in 9 sweets of sent these robots collected these datasets with this reg over here here's our Leica prism being or like a total Station tracking a prison over here for as long as we can get it Parker ball results take the data that we want to compare and see whether current deficiencies associated slam methods that are available today review of use this technology three major competing versions of this one of them is called ok this vins mono of in stereo visual inertial type of system all of them today rely on this feature matching that I was talking about before is this herb or Slam what's the most commonly used visual slam front in today and that is no I am you only looking extraction types of matching is the Direct sparse odometry or DSL personality issues the use of indirect speech speeches instead looks at those those image intensities recall have volleyball changed and shifted right the movie tracking that volleyball and here is my let me break this down for you this planet up the saying the number of successful runs runs in total the number of successful runs for a particular method for Success is defined by the error in alignment which is essentially the number of m off at the end see that if success is defined by only being 1 m of the n is methods Downhere DSO as well as orb orange slime with a single-camera perform so well in fact they perform they never actually convert pretty interesting interesting is that over here there's this differentiator where have you use a second sensor stereo or an imu than you have clearly better performance in Morocco performance over time with some saturation if you want at 7 compared with what we would probably if you had no sensor it'll be more accurate promoting just a second of that stop the interest in dollars it's not just the is helpful it's that if we run the same sequence forward vs backward say like images for red vs running the sequence of images in the types of success rates good dad for a Direct sparse odometry is it worth starting over here sorry system forward visual slam system for this is there location of I'm a Rebel did the starting point and over here is our ending even though there's an apparent sort of meaning of the cave that we are walking in this doesn't actually happen that's entirely a result of the errors in the method hear this Direct sparse odometry method for running it in reverse that is restarting up here then we running the sequence of images back cabins in the reverse Direction why is because of this notion of the what's called the brightness constancy common things in Visual slam Direct visual slam is this statement that if you have an image can you capture an image after that the illumination should be essentially the same that's not the case when we have a light attached pretty much every method that you run director active illumination scenario will break person you know there's this huge all in the methods that purchase way of visualising these results I mean mean the show hears that actually has like we would expect her to be better results perhaps if we were looking at this dropping it's brightness constancy assumption introducing I am you than your sort of golden right and second camera that sort of also stops this scale drift from happening text messages are still very interesting to look at from a research perspective we still want to be able to say ok what happens if we were to consider using Direct method bar where the news are the number of time play I'm failed 10 runs it failed all 10 times independent of the initial seed if you are why are slam matter is even the stereo version fails why does this asymptote up here below 100% success always going to be a few frames in which we have complete motion Blur we will never be able to catch her and correlate different features from friend the point that here being Direct methods that rely on that and your direct message had is there a killeeshill this brightness constancy assumption so we want to be able to relax that to show you this you know another visualisation of the kinds of ways these methods fail here's this plot this one is probably the most true to form as well as metric information associated with one of the runs that week you can see me for the start-up here we go in down the back and then over here is there that happens of this post because of the scale drift from a binocular system any monocular system and we started looking at ways of addressing all of the problem it's an image is going to be generated based on geometry lighting some albinos or natural colour and seen as well as reflecting the ability for light to be reflected in certain way against the sun essentially are setting up for ourselves that we want to be able to use RGB and depth that we would have in this environment to be able to do this service as well as be able to reconstruct the geometry against names of methods like Direct sparse odometry that exist today we have is this lightness constant brightness constancy assumption or lighting being the same throughout this what this really should look like and using DSO as well as these typical techniques brightness to be constant when is we see these sort of artefacts and eaten artefacts associated with the rendering action of a circuit cell wavy behaviour of any of you abused anything like kinectfusion elastic fusion dynamic fusion this is the kind of behaviour that you get when something moves in the seen it sort of falls.off write in this kind of shaded way question to ask what happens if we actually estimate what how the the light moving is where operating in December estimate albinos that is the colour of the surface reconstruction which looks kind of nasty I'm sure many of you agree some of its this black sort of these are rendering artefacts does different shadings and drawings that are happening over there over the image ok well maybe it's doing a little bit better find out to you that I think that there is a lot of research to be done in this field in the geometry in Thai with the brightness courtesy assumption in the bottom relax that brightness constancy assumption to allow for a dynamic light source that sitting on top of the platform these layer areas are weather greater air from the ground truth in the geometry and you can see that this is sort of just proliferating massively these errors res down here it's sort of only exist of the ridges which is actually where the geometry is most uncertain the Promise in research of methods if only we could look at how we do light source estimation sophisticated music visual slam methods that means is in fact this video but that means that with better 3D reconstructions we can do better controlling you can do better reconstructions of how we're going to be in 20 be more reliable and be able to also fill in these gaps and such without having the geometry if you will change under us as we're running an MPC fasting is we've been able to sort of do some other kind of crow stuff with this including the estimation of light source location doing the same thing as visual slam except allowing for that light source to be able to be estimated in real time unable to do is so this is sort of animation if you will of how does light sources that potential light sources are Tessellate in the semantic segmentation in albedo estimation using fairly off-the-shelf type of deep neural network technique Leicester me cheating and then lighting based on pretty cool because now we can actually figure out where they are actual light sources may seem and understand how shadows are them can potentially even invert the shadows play this is this is all going to this mycenter perception-action inform one another moving camera active light source in this environment into a scene is going to be helpful for us it's only we understand how that light plays against one last example that I want to give to you in the same play 22 you have a fairly I mean there some text you on the ground but on the walls there may not be too much tax interested in using these indirect feature based methods of tracking frame to frame no where we should be looking news Jason from this is chicken is able to stabilize a Ted I seen this before how many must have amazing soundtrack show me the stop playing right now my apologies play the chicken what is it doing anyone wanna take a gas timble Inn Innocence is interested in this case and trying to focus on Mercedes-Benz commercial for stability and cars ok so focusing on a certain feature set in the environment it doesn't really the chicken doesn't like motion just like our systems don't like motion butterfly perfectly fine if it just stays attacked like it stays on one thing and it uses proprioceptive senses to be able to understand how it's moving with respect to that then Christians help with the snack ok well you know this is this is a fairly you know this is one step in the right direction so here we have I'm just showing you this video because you know it's the first step if you will sitting on a piano tune play absolution up here and what you can see is that will the track features on the robotic platform by doing this is not really like remarkable just because it can server well because of mechanical gimbal before the chicken has remarkable because the chicken moves as it's looking at some the cards as it heads ability to remain focused on something reaches its control limit is the chicken is head mousse for genetically and then but it's funny doesn't it doesn't stop moving right has a mission to a car go away is the idea we put the chickens head on and on but we would like to do is be able to say this time some setup informative features in the environment landmarks that give me information on state move the car round to be able to do whatever it is you want to do subordinate perception local planning against the global mish especially where you can imagine is moving this this robot around and being able to focus on things and that's the point of future work that we're working on right now Hanover show is sort of some of the simulations that we have in the formulation that we take we reduced our state bacteria just a translation velocity and some biases on the giro sing me an interesting brass sorry biases on the new four times and what we've done it with reduced the camera information to informative state vector is Informant of this on land generative model for Cameron sorry for this colour that but here we have informative this of the imu which you could imagine is being just something over saying can you give us information anyway so that means we're going to be moving the cameras field of view complementing the information we had from the eye lion mark we can actually quantify the amount of information this sort of total information you can also do the same if you will for some Future time Horizon age ok volume what we're going to be doing is saying ok that means our total information is are some of them information problem marks that are currently visible in this set the imu information that we get for free the iron is always going to be operating it doesn't matter recorder frame it no problem take the maximum over possible set sunset of Us within the field of view call visibility constraints during the fixation that is to say we're going to fixate on certain types of regions maximum over these regions s masters in marks that we can calculate information requires the introduction of some sort of information network we use the largest permanent information that trick on this a lot of examples about different ways the Fisher information matrix actually shows the amount of information that we might have we've been that we found to be pretty useful for application of purposes as well as just using it seems to represent a volume of information rather than major access trying to do is you know especially say if we were given this hallway scene look at all the features that are right here that might not be able to provide us information for very long after moving in this Direction the features may be further down the hallway or perhaps slightly to the lack of right because they'll give us information that complementar I am in a three-dimensional environment that is so cold hallway as an example just because hallways are nice and playing on some walls and have some features and others here's a video of that is the simulated highway trajectory in which we have our cameras frustum looking at these different landmarks in this environment visualisation in Visual slam world with the imu being information as well as the visual front-end actually being the thing that processing the images that is the only able to hear what's an in many of you might look at this by asking the question about here landmarks in the play marks besides these tiny volcanoes that we've said losing track play what a look at the panels as we're sort of flying across extraordinarily visually sparse environment variable pasta Estate estimate you call in variable to also do it by selecting subregions of images Fairley common term date is Tommy indoor dangerous being able to compare those rather than doing for calculations of Visual slam against the entire in oh sorry these kinds of safeguarding methods are even more powerful pain around two things that you can see what's coming up next appearance circuit so that's February 18th to the twenty go to setup business park in much colder there snowing end but anyway so we'll be up there for 10 days playing teams are Robots in similar conditions to avoid and before we've had a lot of fun doing this and I just want to remark that it working with dark in working on these sorts of projects has been very motivating for the team in for me it's sorted allowed us to pursue many different research directions at the same time to have undergrads and postdocs working with one another Building Systems actress you know like a caveman type systems in my opinion they're really like sophisticated what is networking for being able to do joint rehabilitation being able to do different kinds of task management etc also just building a robotic platform something that unfortunately doesn't get done very often enormous number of resources in hours to be able to build this platform one of the joys of my first 2 years of being an assistant professor at the University of Colorado to be able to teams of students working together building these platforms in running them not only in what part is interested in but also rang them and agricultural settings partnerships with boulder rescue wild Wildfire management services service hotshots etc which is been a real looking forward to going out there again just just a show-off we didn't actually filled some of our cars in the first competition me sort of them nicer wheels for working in mud here's a picture of our team in Munich include showing off a video of Ava environment which is a time-lapse video when I time-lapse it's the opposite it's reducing the video to 174 frame produce shows that we need a navigate against to make sure you're in a reference frame and again like the whole mission was to find the source Saxon things in this kind of different screwdrivers lights and everything that would normally cause issues with state estimators the sort of things that we've been able to do have really a robust if I damage it's like this one and being able to operate over different types of terrain and and co-operate with one another as robbery like I said a real drawing you know we did fairly well I'm the first competition here's us to Marble first place and we hope to continue moving up in the world explorers no OK when you're loved seeing you apparently and so yeah I look forward to you going to the next competition I'm happy to take questions and thanks for your attention stop the timer so right now we have only doing this against which he might say as kind of you know but we're planning on doing as being able to just have a very simple heuristic based on convolutional neural networks in fact their condition on the types of environments therein as a prior is different places that we should be book we are looking over currently playing Horizons so really all this the missing ingredient is where are the landmarks which can easily be just like I said a prayer and a probabilistic power distribution this is what we plan on doing but yeah we are currently doing playing over and time Horizons that are given to us by plane sorry a page out of the do colonies work on a tensioner anticipation in Bridgwater so slow how to say we have a plan and let yous thank you 