for the very kind it's a pleasure to be here the two next 2 minutes about the century what has been one of the main drivers during my PhD which has been the question of how do we keep autonomous systems in particular robotic system sounds self-driving cars home robot AI animation system how do we keep the system safe especially how do I keep them safe in a world that is increasingly complete forward appointment systems when they have to interact with humans and what does that even mean to have a safety insurances for the systems given that so much and certainty going on and did this inform our design only some personal and first which is the best Agadir also Premier League leaving you with some open questions so my goal at the end of the story that you'll be quite dissatisfied and hopefully others to cells all of you know robotic systems have really been gaining application since the last few years and it's a lot of excitement about self-driving cars drones for a variety of applications including transportation of goods there's actually having to pay for delivery right now on the Berkeley Campus and if you have the spirit stamp get from A to B it's Uncle your helmet on his actually are and people something have fun kicking them around we just thinking rubbing attraction problem applications like manufacturing in surgery lit opens a lot of very exciting opportunities which is why we want to be able to apply the assistance but at the same time important challenges in one of the most important is how do we keep the systems when we think about robotic systems that they have existed this is how we got about safety we just in a cage separated from humans and we sticker on it making sure that nobody gets me red or else this is will happen to you you know as we the possibilities everybody can we start deploying them on more open environments where there's more complexity and more and sincere it becomes very difficult CRD systems will be able to please stay safe this is video from the year they spent in industry before starting my PhD and as you can see where we're going about safety with the rather rudimentary method white pretty effectively want to make sure that the drone wasn't going to fly into the neighbor's house and it did the train but obviously wasn't principal method so when I started at work exploring exciting applications also with system that was able to fly following safety younger in his but every now and then we have something like this having which was quite quite terrible if you're gratitude and the tenancy you know that the next couple of weeks are not going to be a lot of fun you know we're really asking what can we do to make sure that we can run experiments for example running reinforcement learning algorithms are drones without the risk of these things happen and so will you know when back to the drawing board my proposal was ever use ropes but then generally fly so we went back to the drawing board then we came up with a vehicle safety guarantee that whenever the drone came into a situation where it was theoretically predicted that it or hit the ceiling we would actually override probably safe we've probably same guarantees are we going to be talking about this in the next few minutes is the word for as long as the reality that you're dealing with have to do with the model that you using for it so decided to put the stewardess by turning on and big fan of starring blowing air sideways have a Quattro is that when the system comes back down and flights into this airstream that is not part of the theoretical model that we basic insist on start to go a little bit wrong and eventually so far away from the operating conditions that it was assuming that you can see the safety guarantees no longer applying in actually history and this is only more true when we started trying to fly drone so here we have a little drone and supposed to be navigating around human pedestrian school we have a canonical human being was that whenever the human moved in a way that was roughly following the predictions made by the system then everything went fine but when the human did something unexpected like and it's going around this coffee spill that we drew on the floor the drone get confused doesn't know what it's doing actually crashing out of course you can see how she is play will we get from this is there we can compute the radical guarantees and it's a good starting point when we try to give assurances for safety for systems but ultimately any guarantee that were able to computacenter reading about it only as good as a theoretical play modelling error is something that is inevitable you're dealing with a sufficiently complicated system alternately artificially interesting system that you'd like to do anything with it there's only a problem that we're having in a university happening these days with the protons are being deployed issue of Mullingar it's a central problem in trying to give safety Assurance so and Lisa example I have two cases where the uber car and the Google car at the time unable to what's going to be in what the human was going to do that again putting in time turn the case on the left human driver make a lectern that wasn't even expecting there to be a human there but there they were and in the case of the right there was a bus driver who thought it was their turn to go and the car still there was it's turn to go on the end of a pan the sink mission of reasoning about the world and acknowledging that your models can sometimes be flawed is really important when it comes to giving something ineffective something that is worrying more more people you starting to see these AI and robotics technologies to get deployed and declarations made statements made in the last year to buy different governments using things like human beings will only be able to confidently and fully reap the benefits of AI system technology which commission a couple of years earlier the United States White House accepting must be predicted as these technologies are tested and begin to mature the deputy secretary-general is Chinese Academy of Sciences that to truly have is the you must first ensure it secure controllable and if you look really have the common factors here you can see the trust safety and tolerability security reliability quite pervasive even government concerns about AI convince you about 2 minutes if we wanted apply high-stakes automated systems were these are robotic systems in more general AI systems even going as far as ad optimizer you should be able to reason about what kinds of guarantees insurance as they can get and how reliable these guarantees are on the actual Evans that are getting from the world so how much should I trust my guarantees given at the world it's not identical to and so this is really a crucial distinction that I think we really need to make when thinking about safety assurance see ultimately a mathematical a formal statement about an abstraction of the world which is a model and it's not identical to the world assurance ask a high confidence statements about the real system same thing as me based on model guarantees what are you can make some statements about the model then you really need to reach you need to bridge this reality gap the model Angelina there is a way in which we can do it will be talking about them today I'm going to be talking first briefly about what it means to give guarantees about a mole of the system so they talking a little bit about safety analysis and then three different waiting which we can actually try to give guarantees start with safety verification how to give you a little guarantees on the safe of safe operation of the system and hereby saved we mean that we want to make sure we can avoid Bailey Estates you said I can be anything from an actual physical collision to the robot violating some rule that we specify for a free example we might decide that it's already a state evaluation for a self-driving car to be driving in the this is rude a systems Theory so we think of the state of the system which can be the robot but also other variables in the world like for example the positions of other vehicles or other agent reason about the evolution of the state with some differential and what we say yesterday evolution of the state can be affected depend on the it's already going on but also on some decisions that we make the control on some other external variables that we don't Sturminster encapture anything from the actual actions of other agents of the in fact any form of modelling error so if we're not sure about what really is going to happen we can catch up this disturbing uncertainty about something with the form in the form of some disturbance in conditions we can specify a set of forbidden or failure States about whether the trajectory of the summative also the time will at all times stay clear of the fortunately trajectory dinosaur in socket is fortunately on the decisions that we make free just asking will the system always be safe with brother is there something that we can do troll to keep the system safe I'm going to be using both face to refer to trajectories of Signals over time and regular font to the instantaneous control action or state we can capture whether or not the system is violet either using some form of metre for example some distance sometime distance when you're outside or the failure stage when you're inside negative the Deep Lee Valley because trains all the more positive the margin you have please answer this about the evolution of this distance over time or this morning no this is that taking an average of this margin or any sort of sun is actually inadequate if you look at the integral of these two Curves in both cases the Angelus positive however in one of the case of the system is really that later became an important point to make because a lot of the optimal control formulations that are out there including reinforcement Learning Centre reason about the sum of rewards over time which is really an addict nothing like the worst case the minimum reached by the function of a tank which is what we care about when we asked questions about sex would like to reason about the minimum of ill reached over time and it is often called the reachability we have is the minimum or the information or the signal over we can reason about this and we can obtain it through dynamic programming for example propagating things backward in time making the optimal decision at every instant who are the technical details of this for the can find also feel free to stick around and place for the top protection we see that this is ok it's a trajectory because the minimum overtime if we propagate in a similar way getting we see that at some point it becomes negative we keep this minimum value if you propagate the the value function to the that this is an unsafe trajectory because we will eventually hit the oh sorry that a failure message that we don't actually works when we go from an individual trajectories to The Fall analysis of the state space it was called hamilton-jacobi safety and we can visualise it here supposed to say that failure now look at it caravan 3D or going to have the initial distance function which is a cone in the schedule using Euclidean distance to the failure propagate the Dynamics of the system backwards it's more decisions and also considering the worst case of the disturbance that were protecting only about some nominal model but against all possible realizations of Arran propagation you can see that the value function has sort of and now we're left with this boundary separate safe States important notion of the safe set which is a set of state from which the controller can take some action that keep the system from entering the code the disc for all time if you're here if you're inside you already have a negative value which means that even under your best effort there might be a realisation of the system that will actually drag account this actually very useful because it allows us to obtain mathematically best that we could possibly make a key persistence American often reveals and uninsured strategies to keep safety I not be directly achievable by just having some engineer said down and hardcore them into the system so whenever we can compute the solution to tackle the extremely this was a project and we did with NASA few years ago where they wanted to be able to have it central Ian Air Traffic Management System that would scale better than just having human air traffic controllers the order of hundreds thousands or tens of thousands of vehicle play space for example over the San Francisco what about this one break down the system into sequential planning for the trajectories because in fact NASA was going to have a first come for a service West Ham the first vehicle the second vehicle come in requested ejector name something I would be saved without perturbing the trajectory already given to the first able to do what we actually developed a new theoretical tools for time-varying safety analysis this date was we first the first vehicle which was the highest priority vehicle trajectory including the safety and security the safely reachable set that we were talking about earlier and then the first vehicle becomes a moving obstacle for the second vehicle you see that the cuts into the backward time propagation of the second vehicle safety happens here for the first 2 vehicles be coming up to go to the third on the first three vehicle becoming obsolete the safety computations the advantage is that you can actually run each one in the state space of a single Vee discriminatory or blow-up that you would normally get if you trying to do what are the vehicles at the same the disk scales linearly with the number of vehicles because it's time you're only doing computation in the space of that single vehicle and also it gives you as long as things are according to the model gives you the optimal trajectories for these vehicles in in the sense of being the shortest the quickest trajectory to get from here to the priority ordering which is what vehicles you don't have to take my word for it that it works here with a 50 flying over San Francisco we have another one with 200 vehicles of the Bay area but I think it's actually listen to to hear is vehicles flying with different wind conditions and different time and condition all of them are actually able to compute their trajectory and complete them without coming into collisions with one speed is important because while we are taking this as a theoretical safety guarantee in fact this is something that will apply as long as error the error between the model and the real system is bounded by difference in your in your revolution of the of the position of the vehicle that can has wind speed of up to 11 metres really mean it has to be exactly 1 speed that could also be other things like gothic could be capturing the delay in your system when you try to make a turn but actually takes a while play capture under virtual wind speed of 11 m per second the guarantee actually apply segway into the next part of the top which is ok so guarantee how do we make sure that actually apply what extent can we make sure that the place of the physical system you that in fact there is no way to make 100-percent sure because fundamentally and more I love your two different things for the models of mathematical objects and then reality is something made of atom country Rae theoretical statement we can make an abstraction of the reality and the material things about that it's not it's a type error that you can't Rentokil guarantee but we can do things first thing that we wanted to do was see if we could apply some learning-based methods to this is something that is becoming more and more attractive because play Smells Like reinforcement learning you can see a couple of examples of here or becoming being proven to be very powerful at planning overlong Horizons with complex Dynamics multi-agent systems at this office are that was released by Deep mine mind a very recently open all of these reinforcement learning methods are doing is there finding inferring some structure about the Dynamics of the system and then computing good first effort approximations to the solution of an optimal control problem here you see humanoid which is a high-dimensional dynamical system and it's actually performing fairly well you know over irregular terrain jumping around pretty well given that it's not explicitly using a model of the world however the problem with these systems is that they are good average performance pretty terrible worst-case performance difficult to come up with any sort of guarantee that's when something like that is going to have we wanted to do with sword is this patient in a play exorcisms that can break and cause damage like robotics that we were talking about earlier which is if we only reason and optimise average performance we're not really capturing worse case and apply this cancer systems techniques to robotic systems where a worst-case outcome can actually be quite costly use a safety analysis structure that comes out of the safety analysis to provide some very nice around the learning system so it turns out they can use the safety analysis to provide what's called a safety advert that will basically reset a safety bubble within which you can do whatever you the boundary of the 60 bubble you take the safety action to stay in here we have a robotic system that's a sealing the floor and I'm plugging very good position on the vertical axis and Amazon Alexa so basically the more to lift you are the faster your moving down the most of the right you are a picture with this relatively simple Dynamics model where you are controlling the acceleration first compute these solar parabolic curves that tell you that your unsafe if you're near the ceiling and moving up 34 very fast give your somewhere in the that we have as I was saying informally is that if you start in any state inside of the safe set actually have a controlling variants it when you get to the band reapply the same faction you do whatever you want on the inside you get to the battery again and he replied with a faction that is guaranteed to push you back in all areas that you have play some initial worst-case bound that you had pre is we have for example of your Thomas would rather hear flying with some by consistent to try to do some simple reinforcement learning going to initialise all the feature waste2zero so it happens here is that we're going to start with really really bad guess as to how to fly what roller normally if we just let the photo to find these conditions was going to do it's going to crash into the Grey because we have the quad roller inside of the safety envelope so whenever the learning of great interest the crash at what rotor the safety overwrites it's not you the first the learning organisation have a good idea but after about 30 seconds or so it starts figuring out what it's to do fine and then supplying up and down following this trajectory reference not working the remarkable thing is not that performance in learning system training able to do this with a system with a learning algorithm that initially had a really really poor idea what to do and still we never crash having to end the experiment because they're quite country the limitation here is what happens when you add the fan what happens when your mobile guarantees are actually not accurate any back to the structure of the mind of other mind have some guarantees based on Newton's second law and first principles models we have the safety and protection but we're not really accounting for for example strong coupling between the vertical and lateral Dynamics or any other form of external perturbations that we just forgot to playground we are one way in which we can get around this is it turns out that the structural safety analysis gives us much more than just this one layer how we can replace a zero here by any alpha greater than 0 that the city policy any level set function that is above 0 so any of these nested sets control in bed rather than just a safe to say what we have can be thought of as a safety on it we can use City analysis function and therefore all of the layers of the sun is the data that we have taken at we are exploring the world to gate how much we should trust assist find where am I might be that outer layers of the Onion one actually be trustworthy because the model might be wrong as long as the model is right at a single layer we can use that layer to protect ourselves this is the approach the diesel how we reason about you know when the mother is right or wrong there's a variety of models that you can use that you can use some other form of the normally detect here is that when the water comes down the same that you that you so before but can is in doubt realises that the model is actually not trustworthy so going to see two possible future on that go state is the one that you saw initially weather system is blindly trusting it's model-based guarantee in realise that it's mother was not trust already so it's saying I'm gonna use these layers of the Onion cos I don't trust them I'm going to stay at this height and I won't go down until and unless I start getting new data that tells me that it's actually safe to fly I'm only sufficiently accurate and that reason that I can't take me to the reason that there's a centre sing and that I think it's a good thing to have in general is that it doesn't matter how good are models going to have some discrepancy between the model and reality and any safety guarantee that we have that is strictly based on the model play broken by reality and the real world is really good at making fun of us and our best one thing that we can do to make it a bit more resilient if they have it actively monitor gap between its moral and it's always guarantees actually going on in the world go to be correct everywhere we just need it to be sufficiently pleased to apply plan incorporating a possibility that it's model might be wrong or in the future automatically become so much more of a general principle that can be exploited in different ways and so I'm going to show you now how can we apply this to in particular the interaction between robots and play people and the person who had a stroke balance I don't know what they're going to do I'm going to do some work as an acid with a person could go in any Direction get something probably will get overly conservative The Hague tell them people while they are extremely difficult to model accurately actually follow very strict and behaviour a lot of the tank and cognitive science over the last 2 years has given us very powerful tool tomorrow weather reasonable degree of accuracy what people might or might not doing certain contacts connection between humans and robots is actually very tricky and historically there has been cases where it's been great to have the human and cases where it's been quite so it's to me interesting that both of the Saxons having a 2009 there's one that's usually known as a Miracle on The Hudson and movie about it that some of you might have seen Sally it's got Tom Hanks in it playing the ultimately what happened was that the plane took off little bit more about maybe in the discussion after the talk unusual happened which was that both of the engines play birthstrike an aeroplane we often assume that it's extremely unlikely that both engines are going to get taken down if the bridge strike a sufficiently large it's not impossible that both of the engine to all get taken down and exactly what Being Human pilot when was the key the key Factor in realising that none of the nearby airports were safely reachable but you could safely land in the room everyone and got saved unfortunately there was a tragic interaction between the human and the ultimate to a crash this was an Air France Air France flight That was flying from Brazil to a fairly non-critical failure happened with a pedo chips with which measure the speed of the aircraft autopilot said hey I'm confused I don't know what's happening you take over the pilots were very confused as to what was happening the error message was extremely long another plane might be stolen it was very low because if you don't use a frozen and therefore require reporting to the actually turn up the increase of trouser playing static climbing and eventually in and trusting the storm warning this when they want trusting the sensors and eventually 3 minutes after the original failure the Year of the after what was initially a pretty trivial fellow of the pilots and just get flying hear the is that we really have to be careful design the automation it should really be accounting for how humans are going to interact with very hot tomorrow I don't know if you have been following the case Boeing 737 Max but it's been possibly the most you know the biggest crisis that Boeing has gone Nissan recent with precisely a safety system sitting in the wrong way the crew making wrong assumptions about how the crew was going to respond to it already cost hundreds of flights fortunately these planes are now grounded sorry thing that it's getting boring to really think how they're going about their city and ice I want to show you this little example this plug-in that we ran were we had this drone flying around the Hulme is elite one possible way in which you can reason about safety accounting for how much you should trust you my love the of humour last year's there's been an effect of the last few decades there has been a model s been very successful and very useful human mod next models something people calling the loose introduce the original play mathematical psychology and econometrics found a lot of useful applications in robotics and artificial intelligence and so what this model does it says what I don't know exactly what the human is going to do at any given point in time but I can tell you is that there are certain things that are more likely than what reason about the likelihood of different acts send each action seems to be for the certain things that the human might want to Samsung are utility based model of the human subject they look like a human for example is trying to walk to the door and he we have stayed up from a Trojan what human might want to leave the room then the human is more likely to walk hello less likely to work to walk indirectly towards the door and very unlikely to walk in the opposite direction is work fairly well and robots rise and recline using to navigate around when humans started walking towards the door something happens that was not part of the model at all in the first place for example and be fighting to the room the robot probably can't even sense the presence of the be but if it could it probably doesn't really understand the very peculiar relationship that people have with bees this is a completely unexpected reaction do something to do things that are unexpected and they might violate all are such as experimental and it's complicated to do experiments with these so instead we had and then again it was a real coffee spilt the here is there can see that when human deviates play this video again is that the prediction that we had of the human going towards a goal is actually no longer accurate so let me go back here a couple of steps deviating because there's this coffee spill over there the humans go straight to the role and eventually they will get to this point where their trajectory is infeasible and it doesn't know what you can have something better than they should have a last Resort can like physics-based avoidance mechanism but really clear that point trying to me wrong and the Bridges me wrong can get you in trouble in the best case you will have to give the robots until like emergency of manoeuvre it's worth if it's a little drone but if you're talking about a car on the highway you can already be in a lot of trouble because you made to Romford the main principle that we are trying to be human is not following the model it doesn't make sense to get mad of the human for not following the mod kb more sceptical about the model that we have and ultimately by the way this is true of any model you can have a very simple can like my way Selby human might want to do one of 3 things and is how they go about it or you could have an extremely complex neural network based model where you've observed a million humans in rooms like these and generalized and it works very well 99% of the time but always its tail of the distribution they have to be very careful they can always be something that you're not catching another case of humans I probably don't the train tunnel network and it's a very good no network with me it's not really going to catch go about suspecting them the Gold Room behaviour of the human and we have some noisy rational model or some proper listing wall of what he was going to do but ultimately beast coffee spills any sort of and modelled in valid all of our son that in the Motors and reason about action probabilistically there are usually parameters are very helpful modulating the spread of the distribution so they speak in the case of noise irrationality the entropy of the distribution is directly regulated by this data parameter higher the the Beta parameter which is often called rationality the rationality coefficient correctly because it's like you're blaming the human whenever they don't follow the money because I'm just being a rat degree of confidence that you should have in there when you should expect actions to probabilistically concentrate a lot around the optimal ones worse when baby is very low should expect things to be a little bit more noisy and the human ability to take actions that are not actually that it a human is actually doing something else that your girl is not going to Kettering that's a fixed parameter which is how it's usually treated we say ok why don't we reason about this premature baby framework so we think of it as a hidden state that can change over can work very well for a while and it's only degrade relatively quickly when the Bee flies into the room so we're not expecting the robot can now reason about the posterior distribution of this confidence parameter given the prior distribution that we had a moment ago and then is that the human will take this action high value vs open Halo the Sunday because it's perimeter we can actually do this even numerical in very quickly over time and we can do it in real time and so what happens if the robot very quickly updated distribution about what human is likely to do that's not acting according to your model your distribution becomes much more cloudy and uncertain and as a result of the robot becomes more conservative and getting into big trouble with now a question that is that is legitimate is probabilistic notion of humans going to move with some worst-case analysis about never when we're talking about the diameter of a quad roller before bounded disturbance about the inn of the physics of the robe this particular model but there's only going to be this much Grand Theft Auto or delay in your role I think that you can do if you can actually put the two together and say I'm going to read how much should be at a given point in time but in practice Suzanne about weather human mind what is not exactly going to be at this point but I can actually compute using safety analysis a worst-case tracking error the robot will be no different ways you can do that this reachability based model that actually another method that I don't know where are you working on which also gives you some some bounce on the on the amount of air that you're going to have on YouTube these methods what you can now do it to go to work ok the robot can be in any of these locations over time connect this said on to the probability distribution of where the human might be over time and when I integrate the amount of probability maths essentially the probability of crashing into the humour the worst case tracking errors that I could call motion of the other techniques that turns out that you can actually provide some real time assurances about likely the quadro is going to be to stay clear of the human whenever you are train to physically transfer good about this is that if the human starts behaving in a strange way this distribution will automatically become much more blurry and so your automatic computation of these trajectories will automatically become more cancer lamp human move around like this this and this gives we going to keep confidence fixed so we're not actually reasoning about how well the mullahs before what's the first goal everything's fine starts moving to the S logo and the marvellous able to make sense of it and now he starts with ringing to a third goal that we have no idea about and you can see the difference already doing anything useful the robot is convinced that the humans going to turn around and really struggling to the school doesn't realise that the humanist no longer following the model in a reasonable way simple model but even if you had a more complex model ultimately this thing is why is going to happen your mother will make wrong predictions and still those predictions even if they're probably stick will be over recon reasoning about whether or not you should trust you tomorrow confidence method that were using here you'll see a very similar behaviour first long as humans going to one of the known girls the human starts moving to the third goal the same recorded 28-30 the human you can see that suddenly what's more what decides to take a more conservative even the human story strange things that the model does not understand there won't be a Conservatives Conservatives that you need your robot to exercise based on how confused it is about what the human is do I'll just show you a final video of the of the end result this is actually shot by wire how much better than us at making cool videos I just want it's a very nice you can see exactly what's happening as soon as Andrea starts deviating from the behaviour that you can that the robot is already moving to stay clear get out of the way even though it doesn't exactly know what she's going to do Conservatives the second realises that it's model is not really being very good at this video twice 1 an elephant in the room in that Hall analysis that we doing and some of you might be thinking about ok but loud interactions of human here appears to just be doing it staying her thing is that going to affect the human does is a very important question overlocking in that in that work extremely important and things like autonomous driving and you no robots in the what is think about the human thinking about what the robots going to do there for them all of the robot should really have a little bubble inside there with the humanist thinking back about what they were doing and then you know of the humans mother of the robe this can get you into this sort of infinite regress where the human is thinking about what the robot thing that is going to think it's thinking unfortunately this becomes very very quick even you know the company's supply self-driving car systems are trying to you this kind of treatment as much as possible but what you can see is that when you just read everything that people are doing their thing waiting conservative and so there is a video I'll pay it again it came out last year some Person onto vehicle conversion to a highway that has some traffic but it's not terrible can't find an opening because nobody is going to let it go obviously in order to be able to drive confidently you need to be actively reasoning about how people are going to respond to you and this is how all of us who released driver's licence reason when where where are driving right so we really need these systems to be able to do this confidently but also without being overly aggressive and just have to let me go ultimately very important by the way people are much better at coordination than we typically realise there's a little experiment that I like to run at least it surprised me a lot when somebody am the first time so we're going to play a very quick game which is basically heads altogether is to say the same thing 223 after 3 will going to say we win if everybody says the same totally Taylor and no winky no nothing 123 and going to be 123 heads can you find percentage of something I haven't hear is that we don't have time to get the whole details quick inference any more likely to say one thing then the other I can maybe I'm taking this and everyone thinking they were totally going to say heads ok let's say the lights for robots reason about place an important part every time when I go skating in 2 sec turn the things that we use game Theory to try to reason about this sort of the interplay between what I think you're going to do and what I'm going to do based on what you're trying to make me do it Centre it turns out that there are gave your exhaust that you can apply without you know thanks going up and becoming interactable one of the one of the things you can do it you can have this tactical level and which you're reading about trajectories and you can have a relatively simple interaction model we assume that the humans going to respond to what you do but they're not necessarily also strategically trying to affect what you're doing and that's why nowhere short Horizon of about half a second so you know the typical trajectory reason in this kind of level but if you trying to plan a complex manoeuvre like you know Oran overtaking manoeuvre or reading about who goes first before overtaking a truck you really need to reason on a longer Horizon save 5 or 10 when you're using when you're reading over that kind of prize and it actually becomes very relevant loud house strategically influence me to you something coupled with your decisions and the Decisions of others are coupled and that is Dynamic game Theory due to keep things tractable in this case as we say ok let's do her rocker call analysis where we use the full dynamical model here with a simplified interaction model and then at a high level will you dynamical model where we don't have all that you know all the details of the tyre in contact with the ground and what and have a more accurate interaction model between the automation and the human model that might be fraud this works for the strategic planner to give you a quick picture you don't need to look at the order of the tomb we just retire the world into another state how much the human and the robot like each of the states that robot Harrison yellow if you go one step back in time and you can see each other states for example this robot has a bunch of different actions that I can take more or less happy with the situation I will be more than happy to take different action so you're now reasoning probabilistically as before about how the human might respond possible actions that you could actually don't like it very much let me try a different action to see how the human might respond probabilistically and by doing this you determine the best action that the robot should take at each particular stayed at each point in time and you can continue doing this for all the states and then you can do awkward this Rover here can be done simultaneously so you can actually exploit massively parallel architectures so you can actually do this whole competition very quick because you have a relatively simplified model of the world we have this thing computer value function for you which is based on the game Theory contract now trajectory optimisation that I'm using with my Standard that maybe Google waymo are utilising in there planning to hear my trajectories and hear the human trajectory is value a terminal value term that captures the strategic value for this is similar to what you do if you're playing a game of chess on you do some to expiration and then ultimately using heuristic to determine what you think is going to happen from now turn up the trajectory optimisation where you're actually saying will where do I want to end up at the end of May action going to be more beneficial to model of this interaction that is accounting for the strategic to play actually works fairly well I want to see you a couple of example in the first issue we having the overtaking manoeuvre here is a colour coding of the values that the robot sees related to the blue is high and red is low when the rubber part of the human this little tiny blue staying over here that is actually going to start growing that is that is coming it at the robot know that there's some some ability to push the human to inform as a human to incentivise arranging so in the right case the human changes Lane in that case she doesn't what you can see if there is a gradient of value what's sucking the robot into the minus family planning Wythenshawe temporising which is what you can see here with the little transparent cars on Facebook again the value is actually already the value function here shown in in colours is already giving information about how the interaction is going to go when you go for actually make the human and then the value extensively here here you just switch because you have this blue carpet that's pulling potential for overtaking intractable and reasoning with the longer Horizon but also doing this accounting for the internet play Radio 4 that you have on the human goes down so that you doing something like what we're doing before I'm just saying how the human is acting in a strange way and if we're going around goes down the human becomes more likely to do any your model and then this value actually becomes much less pronounced and in fact I get to the point where they comes unseen Hoover and the rubble avoid getting close to the human because it doesn't understand what the human the kind of behaviour that you would intuitively all that we really wanted to to cover and with this and almost at the time was what happens when you're getting to the typical situation where you're going to overtake a truck you catch somebody who is accelerating on the other Lane and they're both tri-cut lights for you everything that has happened game for your kind of situation like you sort of feel that the pressure here in the turtle you like well I can accelerate turn off and make it clear that they can't go or I can decelerate 11 go with a little bit of magic doing what I'm doing and maintain speed but then probably still going to go for it and it's probably not going to be that when you throw this kind of analysis in this case you'll see that the wiper here decide to slow down another yellow cargo in this case it actually starts accelerating and the sites that is not going to list any changes with the initial conditions depending on who's going faster and how far away you are from that basically these two kinds of strategies do in Fatima the indicator white car is really trying to affect behaviour of the of the address I want to train with a quick reflection about you know we've been talking about robotics AI systems are you no more pervasive than not only in the form of robots in the form of a platform software interaction and very simple model where they assume that this is us and this is them and all they're doing is it providing services one's at the one-time thing failing to acknowledge that there is a closed-loop dynamic between us and the sister and you know everytime you give a recommendation this affects the opinion that the person has put that the person gets you and every time you give them you know recommendations sometimes four-spotted is negative or you a certain kind of content over other effects exchanges the way the person the way the person is interacting with the model sorry where the automation which in turns in turn affects the model that the automations forming of the there are some important and overlooked effect from the technical point of view talk about this in the news about socialization through social media etc but I really think that there's you know here there's an opportunity to try to analyse fortunately an important thing to feel time is very interactable very quick this thing is having on a human actually also having effect on political polarization social inequality to do but I really think that we should make an effort to reason about Close loop between automation systems AI systems and people to reason about what are the what are the consequences of these interactions and to what extent we can try to design systems that will prevent certain kinds of damp underside inches of time going to skip a couple of them in the end the very do you want to say that a couple of assumptions that break what are assumptions made by companies commercializing automated vehicles for example the assumption that the person will take over whenever the system makes honest we have to take over in less than a second lights this is something that like this person here to grab the wheel right before the car autonomously drives into the vehicle in the oncoming Lane but something is not true and they have been accidents precisely due to this failure to acknowledge between the human and the robot is simply not dad is in all that I do em Casa so really think that Boeing is learning lesson the hard way it's not ok to just assume that the pilot will immediately take over especially when like in their case they didn't even about the existence of this system in the first find reflection I think we can make these robotic Naas is more alive you can do it actively reason about how I can have hold Lisa sharing system these guarantees will it rain the validity of the assumption that we make in light of the observed in light of the observed data Tumble especially when predicting human behaviour even figure out with human values are and what humans want the automation system to do think relevant to disgusting Media automation etc finally I think that one of the reasons that this kind of analysis is very useful and can be used for going forward is that it really helps us think hard and flesh out what are the assumptions that we're making and what are the conditions under which we think it acceptable for a system to four play the except that if both engines get taken down by birds at the same time the plane my go down and maybe that is just ok that I'm not sure we have said that one thing that will only accept is that they both pilots on a plane have a heart attack at exactly the same time are we getting planes every little pretty kind of implicitly accepting that if both pilots die or become incapacitated at the same time we might really be in is extremely unlikely and that if this were to happen make peace with the fact that this is a possible failure what are the symptoms what are the conditions under which a system might fail in the useful in terms of deploying automation system Perth a reliable when you're supposed to be reliable I'd much rather that sells me this car is going to drive safely unless you know it gets attacked at the same time but the person to the right important to the letting the person in front it says this guy is save 99.9% of 8 kebab when it comes to designing fantasy we get these things right we will be able to have a future all of our automated systems will be so trustworthy that will be perfectly ok with letting your children go outside and play with how to get from here we said very much mighty gap sound with I'm happy to take any questions starts complaints yes TV drone what do you want can you do when a very good question what I would argue that you should always be recompute what the human is going to do also know that this is not sufficient because when they re computing them all of what he was going to do you're going to do this within some space of possible models right this space of models is fundamentally what might not contain reality and so you might improve the model but that model might still not be able to capture what the human is actually doing and what human is really going to always have an uncertainty region around the Hulme with this with this adaptation you will is your changing the show even when the humans behaving pretty accurately according to a model you do have an ascending region around you that way I think things that a robot or an automated system can ever do a perfectly clear cut prediction about the humans going to do right that can that is always going to tragically sooner 