ok let's start guys so alpine for today's to catch up set a little behind so it's ok so today I want to talk about my plan is to talk about that for and then after that I want to talk for 10 minutes remember I like a bent over relaxation tell me everything over that again and then the lesson if I want to talk about the project and plan for the Prado this is coming up so this is an optimistic let's see how it goes alright so what's going to it so what are the certain processes that start with a question especially this just my hands or do I need to go to the website so the question is it's Friday night and you want to go to Mountain View what you want to do if you want to get to Mountain View with the least amount which one of these modes of transportation how many of you would like how many would drive the popular Mountain View tell trainers some people with a tart rain sounds good over and lift we have had a good like distribution yes I want to play I Spy in cars are becoming a thing like this could be an option in the future there what actually start up sorting on flying cars but I think about this problem with the way you think about it is is there a bottom uncertainties in the box but it's not Ana saw your search problem you could bite and you can get a fat tire and you don't really know that you have to kind of take that into account if you're driving that could be traffic if you're taking the Catherine the offers of the way with the Catherine uncertainties that exist in the world and then you need to think about that so it's not just a few research problem where you pick your out and then you just go with it right there other things that can happen that can affect your and that kind of takes us to work out decision problems for everything was determinacy and now you're talking about this next class of state beis conference with her Markov decision processes and the idea that is it take action but you might not actually end up for you expected to cause there's this nature around you and yours this world around you that's going to be honest and do stuff that you didn't so so so far you talk about search as you started and then you taking action and you deterministically end up in a noose you remember the successor function successor of SMA with always he was as prime deterministically and so if you have us and you decide to take this action one you're going to end up that's how you're gonna the solution to research problems for this we had the sequence of actions because I know if I take action one and a Cancerian action to you know liquid is a pattern I'm going to end up so when we think about our conversation process is that is the setting where we have uncertainty in the world and we need to take the idea of it is you start a state you decide to take an action but then you can run away and oven you can randomly end up in a swan primer because there is just so many other things that are happening in the world and you need to you need to worry about that random and make decisions based on that and it actually comes up pretty much like everywhere in every application so I just comes up in robotic so for example if you have robot that wants to go and pick up an object you decide on your strategy everything is great but like when it comes to actually moving the robot and getting the robot to do the task that the actual yours can fail or you might have also some obstacles around you that you didn't think about so there is uncertainty about the environment or uncertainty about your model like you're actually there's that you do you think about an in reality they're affecting your decisions and just comes up in other settings like resource allocation so in resource allocation maybe you're deciding what to produce what is a product you would want to produce and and that kind of depends on what is the customer demand and you might not have a good one off dad and that's on certain products customers want and what they don't I know I have a model but it's not going to be like and you need to do resource allocation on those assumptions of uncertainty about Seymour thing is an agricultural example you wanted decide what sort of what's your plans for the body and you might not be sure about the weather of is going to rain or if the crops are going to yield or not so there's a lot of uncertainty in these decisions and they make these problem did they go beyond search problems and we can problems where we have one search so what's another example so this is a volcano Cross so we have an island and you're on one side of your eye and what we want to do some black square we do and what we want to do is you want to go from the splat square the side of Yolanda we had the scenic for you and that's going to be with a water reward and my goal is to go from one side of the island to the other side of the caveat is here is that there's no Kano in the middle of the island that I need to act and finally ok now I'm going to get a -50 reward well I can't imagine you're getting a -50 reward if they fall into so alright so so I have this thank you and decide so if my son probability is 0 which is I'm sure I'm not going to find you ok now should I cross the island I should cross the island because I'm not going too far out like I'm not going to find that - 50 sleep probable 20 get to my 20 rewards everything but the thing is like even talking about how the world is stochastic and Sopranos He is not going to be maybe maybe this 10% so there is 10% chance of going to into the volcano how many of you would with still cross the Isle number so the optimal solution is actually shown by these errors here and yes the optimal solution is still to cross the eye your value here the value here is basically the value you're going to get that beginning like so kind of your talk about it if you told me that you're going to get it's going to go down because there is some probability that you're going to know what kind would still like the best thing to do is to cross the about 20% how many of you would do it filtered dadiyata more strategies to cross 30% so it's 30% that's actually the point Dad any kind of nut crossword who's on the types of problems you're gonna who is like a valued they rewarded you're going to get that steak and then value conclude that you'll probably get it back we'll talk about that in details on and how to computer alright I was just an example so that was an example of a Markov decision process 110 doing this lecture is we're going to like again model types of systems as Markov process when you're going to talk about inference type so how do we do in France how we come up with this best strategy and in the middle and go talk about policy evaluation which is not an inference algorithm what is a step towards it and it's basically this ID up if someone tells me this is a pass can I evaluate how and we'll talk about evaluation we should try to figure out what is the best model what's the plan for today the next workshop really want to talk about going first morning we don't actually know what the reward is and we don't know what with the transitions are so so that kind of the learning portal part of this mdp the reader is going to actually do the do the lecture ok so let's get into let's get into work of this evening example actually I do need volunteer service so in this example we have a button round and the idea is you can at any point in time you can choose to or you can if you decide to quit I'm going to give you $10 actually I'm not going to give you a lot of money I'm going to give you and then you'll end the game and the site of stay then you're going to get $4 and then I roll the dice if I get one or two will end the game otherwise you're going to continue to the neck so who wants to play with do you want to stay or quit Larry David ayatollah Dyson ordered idea here right so so you have these actions and then with one of them but if you said to create you deterministically will get your $10 on your done with the other one it is from Lostock and I kind of want to see which one is better and what would be the best policy to taking this setting will come back to this question your former lysis and and so then you need to actually compute what is the expected utility right that's what we want to do right so so you might say oh I want to I want to say and then I get my photos and I want to quit and I get 14 and maybe that is the way to Dr Quinn strategy but for doing that right if you're going for doing that you're going to define what we did the Ottoman one other thing that for this particular problem you're going to keep in miners define the policy that is it's a function of state so if you decide to stay that is your you decide to not stay that is your policy like you're not allowing shooting right now as a talk about this later put my daughter come back if you if you decide that your policy the things you want to do this to just stay a keepsake this is the probability of the total rewards that you're going to get so you're going to get for with some problems and then if you're lucky you're going to get 8 and then you and if you're lucky you're told and if you're lucky you'll get 16 where the properties are going to come down pretty mice like really so the thing we care about in the setting it is expect writing expectation these possible past that I can do what would be the value and for this particular problem it turns out that in expectation if you decide to say you should get 12:00 you got really unlucky but in general expectation you should decide to stay and then we actually want to spend a lot of time in this life there thinking about how we get that and and how to go back computing at 6 and I'm based on that hard to decide bike ok I don't if you say 2 to quit than expected utility there it's kind of obvious right cos that you're quitting and that's with probability one you're getting $10 so you just get $10 and add it so it's you when you roll a die I said if you did one or two yeah you you say and then if you get the order of the two-thirds of 80 continue so so it's a 1/33 ArcaMax running examples I do want to finish him now as that's why maybe I'm rushing things we are going to buy this problem like don't worry about it if it's not cleared the end of it alright so you do want to formalize this problem the way I want to find this problem is using an m so I want I want to call my sister Markov decision process maybe I can so in one of the setting process is similar to search problems you're going to have sticks this party chord game I'm going to have to stay am I in the game I'm out of the game so I'm in an end Theatre everything we ended you're out of the game then when I'm in this state I'm in each of these States I can take an axe and if I mean instead I can take two actions I can either the site of stay where I can quit if I decide to stay from instead that takes me to something that I'm going to call a chance node no chance no is a no that represent state and act things are my States but I'm cleaning his chance note as a way of kind of going through the example to see where things are going these blue States are going to be my face I'm in s the Chestnuts are over state so basically just told me that I started within and I decided to accounts note here basically told me Dad I started within I know the side gymnastic Beam I'm going to dry out with the with that comes from Mike and snow I am going to save it I'm going to get $10 what if you're in the state this is actually the state where interesting things can happen two-thirds I'm going to go back to in and get $4 or with probability one third I'm going to end up in end and the four corners so that is my Markov decision so I had Intu tracker a list of things your defining in this lecture service find States and then he said what we going to have this chance notes because funniest chants no it's probably think we're going to come out of them what happens Anisha kind of decide which one you're going to end up and I'm based on alright the more formerly we had a bunch of things when we define and then search for albums by the same thing so in this case my state or in and end we have a start-stop I'm starting it in so that it's my starts t I have actions as a function of state when I asked what are the actions of the State my actions are going to be stay or quit where are actions event and it doesn't have any you have these transition probabilities to transition probability what's am I take a sleep an action and the new Estate so hairspray tell me what is the transition probability of that and then I have a reward with Salisbury how much defining defining my MVP play the new things on the transition probability he shows me if you're instead I take action and what is the problem I'm in in I decide to stay and I end up in end with the probability of that what's my third I decide to quit I end up in what's that and then or the same stayed actions reward restores me how much money did I get how good so was $4 if I decide to quit I got 10 and if you remember in the case of such problems are talking about I'm just flipping the sign he wanted to minimise cost and maximize system so so that is weather report we also have this isn't function against a more to search problems just text if you are an entertainer or not and in addition to that we have something that's called the discount Factor value gamma which is between 0 and 1 and talk about this later don't worry about it right now what think of me define so how do I compare this these are the things that we had in a search problem he had this accessory functions that would deterministically take me to Ace prime and had discussed one turned out would tell me what was the cost of being status and taking to the major things that are changes that in-service excessive function I have transition probabilities this dad dad basically Turner what's the probability of starting an S&S fine and then the cost closer of the major difference search things are so that was the former that now I can define any any markets I don't want things just wanted to point out this transition probabilities the sea basically Spotify is the probability of ending up in in status prime if you take action a in a status probability so for example I don't like we've done this before but just do it under slide again if I'm in state I take action quote I end up in end what's the probability of that 1 and then if I'm in I take action stage ind in again what's the probability of that open again and then if I'm stay in stay I end up in end up with your dad probability so would I mean stay need to kind of add up to 1 but one thing to know this is well what is going to add up to one of the things are not going to head up to one the thing that's going to add up to 1 is if you consider all possible this different crimes that you were going to end up those probabilities are gonadal so if you work hard the stable if you were sad deciding being stay in and taking action stay than the probabilities and that the 4:30 difference a sprained or 2/3 and 1 and those two are the things that I cannot in the first place if you're in stay in and you decide to wherever whatever this finds opening up in this the more formal word that means if I'm coming over a surprise is new states that I'm going to end up the transition probabilities need to add officer what are the what are the things that can then distribution probabilities are going to be non-negative is there fog so that's a search problem that's actually formulate another search for this I was actually trying to kodi so what is a search problem this is the tram problem remember the tram Prague I have Black Swan through n what I want to do is I have two possible actions I can idle from CS first aid Xbox One or I can pick a magic fan that taken from state to state two if I work that cost 1 minutes means reward of that is -1 if I if I take the tram that cost 2 minutes damn you said the reward of that is - 2 and then the question was how like how do you want to travel from from 1 to n in in the rooster nothing here is probably I'm going to add an extra thing which says the tram is going to fail with probability when do the side maybe you take take a trial at some point and that tram can can feel if it fails and actually I can discuss you're so me you're still losing 2 minutes so if I decide to take the tram I'm going to loose women maybe you're better maybe alright what's writing formulas this we're going to take our tram problem from two lecturers ago so this is from search One copy Dad so this was what we had from my Sammy I just transportation problem get all these like algorithms to solve the problem we don't really need them get rid of I know I just want to formalize initialisation what's ok dirty looks ok I'm starting from is end looks ok the thing I'm going to change the first up on you to add this actions at what will action still it's going to return list of action potential actions in Heanor I just copy paste so it's going to return a list of actions what are the actions I can take I can either walk or I can tramps I'm going to remove all these extra things that skip it either walking around taking as long as it's a no that looks right the only thing we had was a successor and cost so so now you want to change that and return is transition probabilities and and so it's basically the successor probabilities I'm putting those two together no I'm returning pub water so what does function is going to return as it's going to return this new status as prime I'm going to end up and the probability value for that and reward so given that I'm starting a status taking action a then what are the potential as frames that I can end up bad and what are the probabilities of the atlas and what is t of SAS and what is a reward what is the reward up I want to funk alright so I need to basically check like 432 one of these actions I can offer action walk what happens to Action walk what's new state Xbox One the deterministic action song and end up there by 1 who was the reward of that -1 1-minute what's -1 then for action we can do the same thing when you have two options here I can I can end up into us to understand fail.in into US what is 5 - 2 or the other option is I'm going to end up in status because I don't go anywhere because we probably 0.5 the TranZit fail and dad that nerve and that's pretty much that is my my so I can just find this for a city I need to have this contractor but we'll talk about that later I'm writing this other States formalised it does the right thing so maybe if you want to know what are the actions from state what are the action 3 we need to remove this utility function from before having in The Fall what were the actions from state 10 blocks state three I can either walk or did the right thing maybe you want to just check if this success or probability and function does the right spelling so maybe we can try that out or stat3 83 and action walk then what do we get will be in Dublin for and Dad at that is in trouble 21 weather report the straight out for tram can feel something to get two things things I'm going to get for tram I'm going to either end up and 6 with probability .5 with the reward of minus or I will not go anywhere I'm still Rover 2.5 and that is with the reward alright so that was the tram and informal I said as an MEP again the reason is 10 mbps is that the tram can fail Republic Adidas define transition functions and our problem and reward I want happy how your defining MVP producing water search for albums so now I have define an MDT that's free the next question that in general we would like to answer is to give a solution right so there mdpv apartments said you just so so when you just depend on the state and this current state to find out state of our state is sufficient for us something optimal decisions the Mark apartments sete the interesting question would like to do is well 1050 solution I want to go out what is the ultimate remember search problem the solution to search Rob on this was just a sequence of actions said that like a sequence of actions that whistle and the reason that was a good solution was like everything was deterministic so I could just give you the and then that was what you would fall but in the case of them deeply is the way we are defining a solution is by using this notion of a pulse a policy at Dad here define Dan but now I want to say what is a solution of solution is a party pie and this policy basically goes from any state and it tells me what is the initial action that out yet does the policy the function it's a map from each state s instead of other sports States due to an action in the setup indicator walking Crossing I can have something like this I can be in state 11 and then the policy of that state could be going south I can be in state to one and a policy for that if this was a search problem I would just give a path I would just say go south then go to go instant if I decided for the policy at 11 is to go staff there is no reason for you turned up at 4 this thing is the best thing I can do is for every state so you was the best thing you can do for that and that's why your defining a policy as supposed to get giving like policies the thing if you're looking and ideally I would like to find the best policy that would just give me the right solution but in order to get there I want to spend a little bit of time talking about how good a policy with the so and that's kind of the tidy up in value during a party so middle section I don't want to try to find a policy I just assume you give me a policy and I can evaluate it and tell you how good that is so that's it I want so so far all I've done is a define and ndp it is very same autotroph so how we evaluate a party if you give me a policy which basically told me and every status take some action when the Pulse is going to generate a random I can get multiple random Pascal metre behaves differently and the world is uncertain so I might get a random Nando's all random variables Lancaster Underpass I can I can define a YouTube so what is a good torch utilities just going to be the some of the words that I'm going to get over that I'm calling good as the discounted some of the rewards remember that this can't will talk about that exactly we can discount a future but for now just assume it's this is someone are you talking the utility that you're going to get is also going to be around and if you think about a policy pause is going to generate a bunch of random pass and and you talk to you is just going to be the some of the words of each one of those around so so if you remember this example so I can I can basically have a past that tells me starting in and then stay and Adams this is one random this way to go around your password what is it I'm going to get understand what's 1 possible things if my if my policy is do they stay like there is no reason for the game to end right here and have a lot of different types of random past I can have a situation where I'm staying three times and after that ending and you told you that it's 12 we can have this situation we have stay staying and that's the situation the song I'm so so so you're getting all these utilities for all these random pass so so these utilities are also going to be just right only for your own with utility that's not telling me anything or just telling you something with a random variable I can after my step we need to define something that you can actually play Roundwood and that is the idea of a van which is just and expected it so so the value of the policy is expected utility and then that's not around anymore that's actually like it number and I can I can compute that number I can compute that number for every state and I'm just playing around with value define particular is a policy basically telling me what is the strategy for all classed as a function of state rights and value the same thing is the function of state I might ask what is the value of being the value of being in is following and following policies stay is is going to be the value of following policy stay from this particular stage it is expected which is basically that I could ask for about any other state to so I can be in any other state and then say what about you and I'm going to do well you Tracy actually need to compute this value for all states I have an idea and that is untold of seeing it's 12 but we haven't shown how to get 12 so actually let me write this in a list of playing so we talk about the policy what else did we talk about so what is Utility utilitywise said it's summer Awards so if I get like reward one can I get reward to to the scattered some other words I'm going to use the scammer that discounted all talk about in little bit time to work soon what's gamma squared x 130 utilities you give me a random and I just some of the rewards of that imagine if there was one I'm just coming up the reward is not sunny good value utility values expected you give me a bunch of random I can computer utilities I can just send them up and average them and that gives me that's very good question I get back that's so so so in general in it is a secret it is fine but if you had acyclic graph you want your gamma to be left and we'll talk about that we get to the Convergence alright ok so what's go today this particular volcano Crossing example so in this case like I can run the scheme and every time I run it I'm going to get a different utility because I'm going to end up in some random pass some of them end up in the volcano that's pretty bad so I get different utility that I use with the value which is expected due to the United changing really it's just around 3 points the average of I can keep running this getting these different utilities with values this one number that I can I can talk about and that's the value of this particular and I told me you like what would be the best utility that I can in expectation from alright we've been talking about the city of actually written that already on the so utility is going to be a discount at someone and then you're talking about this discount Factor the discount Factor is I wait like care about the future differently from how much I care about now so so for example if you give me 4 hours today and give me $4 tomorrow and if that followers tomorrow is the same kind of amount and has the same value to me then I might it's kind of the same idea of having a discount counter of 1 discount you're saving for the future the values of things in the future is the same as give me $4 now give me $4 10 years from now it is going to be $4 I care about it Ford Ollerton and I can just I could also be the case I can be in a situation in a particular mbp30a about the future as much maybe you give me $4 10 years from now and that's that doesn't have any value for that so so then if that is the case and you just want to live in the moment and you don't care about the values you're going to get in the future the next kind of the other extreme the scam with this that is a situation that if I get $4 in the future I only care about right now living in the moment what is it reality or somewhere in-between write like you're not just this case we're living in the moment but also not this case said that everything is just the same amount so I can right now and if you like ballislife is a setting where we have some this contractor it's not euro it's not one that actually discount now you're the Future the future maybe doesn't have the same value as now but the future is still something in and I naturally beautiful that's between 0 that is kind of design choice I can bring my problem you're in you might want to choose a different game discuss you could you could think of it doesn't it's not really an assessment of risk in the problem in the particular problem I do want to get now using the future I have like some sort of wine term goal that I want to get to and I care about the future Jane solving like I don't know what I'm like a robot manipulation problem like it might just be very different like this compact examples with using this class who just threw the gamma Tesco's 21 usually like for a lot of problems at the end of dealing with cameras like 0.9 usual for usual part 1 like a different problem we don't care about the open yeah I'm ok so that's good what is gamma hyperparameter I will take out with a design choice that's not a hyper premier necessarily that will do the right thing you want to download that kind of works well with your problems and then yeah 10 is kind of your greedy like your picking like what is the best thing right now and I just don't care about the future it doesn't properties of like it's about the root word it's not about how they stayed affects the next day it it's basically affect how much reward you are going to get or how much do value of Ward in the future it doesn't it doesn't actually like this what you're getting with the report yeah but it's working because the iPhone status and it take action a I'm going to try and that doesn't depend on like so ok so so you've been talking about this idea of someone comes in and kills me the party does the policies pie and what I want to do is I want to figure out with the value of the policy and again now you was just expected you reply of SE6 between received by following his policy pie from so I'm not doing anything can see I'm not even trying to figure out what part is all I want to do is I want to see value if you tell me this is pi how good is that wasn't so that's what value function is so value of a posse is his reply expected utility of starting some state arm this year animal reply is the value expected utility of new starting in some state and this has value of Pi and if someone tells me that while you're falling asleep I then I already know from status reaction on going to take is pi of Us that's very clear I'll take this if I take higher verse I'm going to end up in some chance transmit is a sidaction note going to be s and the action I've decided the action is pi of Us define this new function the few functions to pay of essay which is just expected utility from the chance so so we've talked about now you how is expected utility from my actual state I'm going to have my tuba values as expected you too ThunderCats so after your committed that you you're taking action and your phone policy pi then what is the expected utility from that what what is it from this time you're in the chance now so many things that can happen because I know boys nature is going to play and roast and anything in happen and they're going to have my transition essay and transition probability on going to end up in a new state and then hold that rhyme and the value of the expected utility of that state is me pi of 8 so ok so what is actually since I've just the fire value as expected utility Q values expected utility from a charity what what are the actually I'm going to write a recurrence bedroom used for the rest of the class turn the lights to magenta please time different when is the only one you're committed to one and the reason of the findings to just writing my reference is going to be a little bit easier because I have the state action now then I can talk about them and I can talk about now but I get branch name from this I'm going to bed it's not hard but it's kind of the basis of the next light ent lectures tell me a pie M&S what is adequate that is going to be 20 this is end of s is it going to true expected utility that's it that's it otherwise well I did fall asleep Ita someone told me take policy Fighters value is just equal to rupee of S if someone comes in giving your policy just equal to q pie of s destroy Trust eco the next question one might ask is raise a little closer from space this is a quarter Q pie what is Daddy what is cube of sa this so now I just want to know what is Q value what is Ali if I'm right here then there but different things that can happen right and I can we use different that's fine so if I'm looking for the expected utility then I'm looking for the probability of me ending up in this state of this state the promoted meaning of music x the total so so that is just equal to sum overall possible is France that I can end up at of transition probabilities of SAS Prime ending of a new state x immediate reward that I'm going to get reward of essay as fine boss the valley here when I can buy the discounted that are you so I'm going to reply is this this is the recurrent that you're doing impossible you should get remember someone came and gave me policy Python riding is called sleep someone gave me fall asleep I I just want to know how good party places I can do that by Computing the pie what is reply you call to someone told me you're falling asleep I so it's going to be a call to just Q pie what is cubed equal to it's just some of all the expectation of all the places that I can end up had that's some over ace primes transition probabilities of ending up in a sprain x the reward the total where do I getting because the immediate reward what's the Scouting and then following policy so ok so far so good so that is how I can evaluate this so I have used to recurrences if I have these two recurrences I can just replace the sky here hands imagine you're in the case maybe I can use a different colour of hair I'm just replacing what is replacing the sky what's right imagine you're not in an entity if you're not in an industry then we pay of us or what is that equal to latest equal to sum transition probabilities SAS Prime over a spine immediate reward I'm going to get what's the skeleton depay is America and said I have my little I just combined if you're not in it not in Ansty this is a return I have a pie here have you play on this site that is nice and that is kind of the place I can completely Pi maybe I can do it early or maybe I can actually find a closed form solution for some problems with that is basically what I'm going to do I have as a function that depends on your best friend and I can just salt for this I haven't figured out a new paw all have done is evaluating what about you alright ok so let's go back to the city apple so what say that someone comes in and tells me about the policy you gotta fall is my policy is to I want to know I want to see value I want to see that when you're doing course evaluation you got a computer reply for us let's start with we pay up that is equal to 0 because we no reply at Interstate is just no I want to know what to be pi of what is adequate Leicester City quarter Q pie oven and stay reply is just equal to 2 pi of so I'm going to replace that that's just caught a 1/3 time to meet with words which is 4 what's the value of the next state I'm going to end up at which is end in the sky plus 2/3 times that you needed what I'm getting what's the value of the thing I'm going to end up that is just add some reply 0 so let me just put that there I only have one state here to just had the satisfaction of this one's in so having an equation I can find the cause for solution of meat pies what things around of water bed and then I'll find out that we pay up in is this because of Dad how's your day told that I've been you just found out that if you tell me the policy to Bose that is a posse then the value of the policy from speak in is it yeah so it's all the policies only had one stay that's interesting to hear that that's what state is I need to Wi-Fi my policy I need to kind of the same policy for my policy says in in order of size so you can basically do the same thing using an iterative algorithm to so so here like in the previous example which kind of simple I just sold the close one solution but in reality like you might have different states and then it might be a little bit more complicated so we can actually have an iterative algorithm that allows us to find these that is restart with the values for all states to be causes and then this is the first iteration so so I'm going to count my operation so I'm going to initialise all the values for all states to speak I'm just going to iterate for some number of time weather in America then what I'm going to do is for every state again remember the value need to be computer for everything for every state I'm going to update my value by the same equation that I have and the same equation depends on the value of the previous this is just an iterative algorithm that allows me to compute new values based on previous values sounds like everything 0 and I keep updating values of Us basically that equation but think of it eyes like an iterative update every you don't run this for multiple rounds every round you just update yourself sort like you're really looking imagine you have 5:30 or initialise all of them to be equal to zero the first round you're going to get some value going to update it and then you're going to keep running this and then eventually you can kind of see that the last two columns are close to each other and your Converse the truth so against something gives you the policy you start without musical 20 for all the state and then you just how long should we run this we had a heuristic 22 kind of figure out how long we should try this way 24-hours one thing you can do is you can kind of keep track of the difference between your value of the previous time step vs this time step so so the difference is below some fresh or the kind of course hi Don and unfavorable I found the right now in this case you are basically looking at the difference between Valley at is racing TV adoration 2 - 1 and then you're taking the max of the overall pass because I want to is it so I'm going to talk about the Convergence and talk about the gamma Factor and and the discount Factor and also how are you should run this to get this is also difficult problem and it depends on the properties of undp so if you having a garlic if you haven't got again William Turner was a heart problem France Jenna and another thing to note here is I'm not storing at home till the only thing on story is is the last two columns of this reply at the Regency and weep I a Generation X another like the only thing I'm sure because that allows me to computer file conversion that kind of allowed me to keep going cause I only need my previous out you stop in terms of complexity but this is going to take order of tea x x x why is that because I'm narrating over 2 x 6 and I'm reading overall my state and I'm something over or last so so because of that at the complex yeah and was thinking this year is it doesn't depend on it doesn't and the reason it doesn't depend on size of actions as you've given me the policy you're telling me about this giving me the policy then I don't really need to worry about the number alright resistant like the same example that you've seen so I did a relation to PO21 in is going to get 4 and is going to get 0 adoration to it gets a slightly better value and then finally I get a Russian 100 reject the value for this example Example you were able to solve it likes all the clothes for movie of the Apostles staying from white arm you could also run the iterative algorithm and get the number of actions of the side of it now because you might end up in very different different states this depends on your property the best musically you're going from every state the summary So Far Cry so you're talking about MD pieces are graphs with States and chance nodes and transition probabilities and words are you talking about policy as the solution to an MVP what is the function that takes a state and give us an r so value of a policy is the expected utility of of the policy so so if you have like utility like these random now used for all these random path that you're going to go through every fault the value of utility assistant expectation over all those random cancel formula 30 website to compute what's the value of a state if you give me some how good is that now you I'm going to get so that has been or assuming that give me the time play Lonely or something trick is it going to change the value of the prices because then so in this case also so far I'm assuming that the set of actions is fixed I'm not like adding new actions even with search problems with my search problems with your defining and I am so like I'm starting with the set-up stayed suffix action suffix I have staying with those already only actions I can rewarded fixed transition problem then what is the best policy I can take and best bosses just from the set-up already defined next step your real talk about unknown settings like only half transition probabilities that are not known and how we go about learning and that would be there in next valuation so so that the valuation so that was so now what I'd like to do is I want to try to get the maximum expected Utility and find the set of policies that gets me the maximum expected to do that I'm going to find a screen that's called and optimal are you so pretty I just want to be octopus it is the maximum value attained by any bunch of different policies I just want a policy that maximizes TV off so back to this example so I'm going to have this in parallel to this example of policy so I'm going to start from state s again DS has reacted I've s that is what I like to find you're heavily Pyrus if I'm looking for reactive us then I can have multiple actions they can come out of here and I don't know which one to take any of them take any of them if I take this guy that takes me to a chance nude have sa and then I'm looking for q optiv sa I'm from here it's actually pretty similar to what we had cancelled anything in happening nature place I need some transition probability of essays Aspire I'm going to end up some new state Aspire and I care about if I'm looking for this optimal policy it comes from this afternoon are you that I need to find react find me off well that depends on what action on taking here take one of take one of these trans nude I have Q after my saying that chance nude and from that point on with whatever properties I can end up in some I want to write the recurrence for the sky it's going to be so I'm going to start with you because that is easier what is QR now that just seems very similar to this previously what is addict what was QPR you play was just some of transition probabilities sounds it's oh so what is so it would just be basically there's a question except I'm going to replace meat pie so funk you up I can end up anywhere like and all possible places that I can end up bad I'm going to get an immediate reward which is RSA a prime discount the future the value of the future is vrt so far so good that's cute what is adequate it's going to be 0 if you stayed before Tenderness miss truth then then it is 0 otherwise I have I have options I can take any of these actions and I can get any to you so which one should I pick which way up should I take the one that maximizes an action set up actions did maximus cue up the only thing that has changed year before someone told me where the pass is I just took a queue of that yeah I'm just picking the maximum value of dad actually told me what act what is the optimal policy what should be the optimal policy I'm going to college I am what is varicose it's going to be the the thing that maximizes V right which is the thing that maximizes the skew because I give him the action it's going to be the Old Max keep off of sna a is alright this one pause evaluation someone gave me the policy then I had a little bag with them to do thing this is called evaluation this is find the right policy duration to find a posse how do I do that but I have a value that's for the optimal value that I can and it's going to be maximum over 2 actions I can take B&Q value similar to before so I have this recurrence now and an optimal policy is just an iPad your 5 minutes over and if you want good news is a slight or the same thing that I have on the boards so the queue up it is just too cold to the sunbed you've talked about VR I just had an axe on top of you up identify what the policy that I just do that that gives I can have any of them that does the same thing it's actually quite similar to data about with my evaluation I just start setting everything to CT20 I ate for some number of times I go over all possible States and then I just update my value based on this new recurrence that had a where is the water before I just do this one thing is the time complexity is going to be order of tea x x x 8 is fine because now I have this maxvalue over all possible actions so I'm actually you reading over or possible policy evaluations I didn't have a because someone would give me the pass alright so so what's the cat coding etc quick ok so we have descend if you problem finding it was a tram problem it was proposed everything about it was great so now I just want to do an algorithm section and inference section where I code evaluation and I can call a valuation on this mdp problem to get the best I'm going to call you later initialised the all the values are going to become my wife so are you going to initialise all the values to zero because all these values so I defined a States function so I for all of those two values just going to be quite a 0 so what's that then you're just going to anyway and what we want to do is you want to compute this new Value given all iterative algorithm we have old values you just up you are used based on them so what should I be quarter so we trade over our States if you're in an in-state than what is value go to 0 you're not alone and stay then you're just going to do that it and that recurrence new value of a state is going to be CO2 Max of nu-v is just Max of Q's what's 7 actions now I need to define queue what does cuter you have stayed in Action is just equal to that song other over a sprite it's going to return some it's going to turn the same over as prime define this success of property and Report function that gives me new state property and rewards I'm gonna it right over there call Dad mob given that I have a stick in Action I can get Newstead property and Report whatever is something your summing the probability the transition probability times for the immediate reward which is right here time is my past my discount time review just the Old value of over ace prime over when you that is my cue that is my v and that's pretty much done you just need to check for convergence to check for convergence be kind of the same thing as before you check if value of y and new VR close enough to to each other that became cold you can basically check is b - new VR within some threshold for all states and if they are than B is it you need to read the policy so party is just out Max of q so I'm going solo the policy is just going to be born on if you're instead and otherwise it's just going to be alright Max of IQ values I'm just reading IMAX I'm just returning the action that maximizes spend amount of time getting there printing working still get actually so I'm running this function and handwriting how use and then pie the policy starts off walk this is the case where we have 50% probability of transfer 150% probability of transfer value and the policies still walk until stat5 and then take the tram from from ST5 the policy of the search problem was the same the thing we can do is you can actually remove this all before we can actually defined as fail probability which becomes just a very well so I can play around with this if you pick different file properties you're going to get different policies so for example if you pick a field probability that is more than probable you like the policies going to be and never take the tram because they're trying to spelling all but if you decide to take probability that cost 0 then this is your optimal policy play around at this the code is online this was just now evaluation you understand skip this one alright so yeah and then this is also going like however multiple iterations you can kind of get to the get to the optimal value and evaluation it hasn't seen it yet so it's thinks that the value of values 1.85 he hasn't updated the value so like three directions it gets better but it hasn't still updated it still thinks it can't get to the other and this is the probability of Tempest but if I get to like I think 10 then it eventually the best policy get a 20 and values 13.60 and if you go even higher iterations after a point is just the values are play around the volcano ok so where does this convert so discount Factor is less than 1 or your appendix graph is a sequence in this is going to convert so if it's kind of obvious you're just doing dynamic programming over your full thing so so that's going to let's go if you have psycho you want your your discounts to be because if you if you're if you have Cycles and your discount is what's a 1 and we'll see you're getting the rewards from then you'll never going to change you're never going to move no from your city always going to be stuck in and if you have noisy or as you're going to get the sundowner door then keep going as you have psychosis and it's just gone suggest a good rule of thumb is picking down by that's less than alright summary so far as we have and diffuse now you've talked about finding policies rather than policy evaluation is a survey of Computing like how good are policy and the reason I talk about policy iteration which uses policy evaluation and we done disgusted in the cars not equivalent but you could use it in a similar manner as evaluation it has its pros and support evaluation using those set do not leave please we have we have evaluation computers optimal value because the maximum next time you're going to talk later informal warning alright that was my peas and and I'm going back to last Alexa just to talk about some of the stuff my last time if you remember last time we were talking about search problems we don't have a way of just making things and we talked about the safety of relaxation switch was a ray of finding good so as I had a serious if you stick with an estimate of future cost you wanted to figure out how to find his hero 6 I had a dream about finding it and what idea was just relax everything that allows you to come up with an easier search for what is easier problem and that helps you to find weather here cancel so we talked about this idea of removing constraints and when you remove then you can have a nice situations like in some settings you have a closed-form solution in some other settings easier search problem then we can solve that and some other settings you have independence can you remove constraints than then you have easier problem you can so that is your problem and that gives you a funeral you're not done yet radio you have here is that you and then change your course and I just rung uniform concert on your original problem so so sorry your problem is like you're not done when you're so pleased you're probably just hope to do things that helps for the naughty step examples of daddy's if you have one remove all the walls you have an easier problem if you saw that easier from on that gives you a hero and in this case it isn't like when you're not around his was an easier problem you have a closed-form solution for it you don't need to do anything fancy you don't need to do uniform cost search and just computer Manhattan and I'm glad you see if you were sick with that you were sick you go on Sofia Richie that was 1 another example is can you remove constraints you have an easier search problem so you don't have closed-form but you haven't easier search problem so you might have a really difficult problem with jerking strange there are hard to do Ramona constrict so when you removed again straight you have a relaxed problem which is the storage or problem without the that's a search problem you can sort that search problem using uniform cost search for that program and then solving god allows you to find the heuristic again you're not done yet tried to take the risk and then you go to the original Prague change the course I just want quick kind of example here was when you completely Deezer relax problem the thing you want to find this the future cost of this relaxed and then to do that you have this is your search well and you still need to run uniform cost search or dynamic program if you decide to run uniform cost remember uniform cost search computer past in this case I really want a computer Future car so you need to do a bunch of Engineering to get that working in this particular case the relax Robyn m reverso reversing past cost of the reverse relax problem becomes future cost of the relaxed the way I'm reversing this is I'm basically saying start to say this n and stated one my tax and text me to s minus one instead of 1:30 and my transaction takes me to Estover to instead of x x and the whole reason I'm doing that is is that the pass cost of this new problem is the future cost of the non-reverse because I may need to use a Different Class I run my uniform cost search that gives me if you was sick and accuracy give me the future cos of the relax problem and everything another example is I can have independence problems using my hero 6 so in this case like the other styles they technically cannot overlap instead what we are allowing is your allowing and overlap safety allow them to independent stop problems that I can solve problems give me the rest a bunch of examples the idea was reducing at like when we are coming up with this relax problems or reducing Edge cost from infinity to some finite I'm getting rid of walls before I couldn't cross that it was infinity Castle that was infinity but if I get rid of the wall and making a finite cost so this type of method what are the general framework generally you can talk about the relaxation of a search problem relaxation of a search from I'm going to call that Dr or Dr it's going to be a problem for the cost of the relaxation for any state accent is less than or equal to cost of state action autoquest alright so so that is a relaxed problem the causing about that is if you're given a relaxed from one then you can pick you up you're a stick to be the future cost of the relax and that is called relaxed us this is kind of a recipe a general framework if someone asks you find it good here what are relaxed problem future cost of the relax problem is a humorous and causing about Dad as it turns out that that that bad future cost of the relax problem deciding to be a hero stick is also totoro this consistency properties and how you want to find the receipt to be consistent for the solution to be correct and how in the world am I going to find a consistent here here is here is one way of finding consistent here pick your problem make it relax making relaxing instead of course that's relax problem because it is worse than the cast of the original problem and in future cost of that relax problem is just going to be and it's gonna be profundity wines and think about this news about this is no the trade-off there's a trade-off between efficiency and type making things relax and removing can you have this problem and you just told me that everything is great about it but it's not like there is kind of a trade-off between how tight you want your hero remove to many constraints if you remove too many constraints than your here is like is not a good estimate of future remember you're here mystic is supposed to be an estimate of future cars so so it is not a good estimate of Future Past and it's not tight between how much you're removing your strength and how Dad mix finding the hero 6 easier vs the fact that you want your hero 6 to be tight and be close to your feet so so don't you know everything leave some construe and and and you can also do things like if you have to hear it's legs are both consistent you can take a Max of that and you can take them out so bad it's a little bit more restrictive maybe maybe that is closer to your future course and that is and then you can actually show the maximum that is so we talk about like realisations a star thing I want to mention that was a very clear your last time it's just that I will be so quick things on that structure perceptron actually convert there was a question that if we have if that is a pad that is with a walk tram and we end up recovering another path that is tram walk Zadar toys that the cost of both of these past or the same thing so if I end up getting a scarf that's perfectly fine to that is also with the same after my weight an example that you're showing in the tram example I don't think you're able to get to pass that look because of the nature of legs nothing's to remember from strictest it does convert it does converting a way that they can recover the two wise but it doesn't necessarily get the exact w that we sort last time I like you might get one for you make your phone it's like as long as you have the same relationship that it that is enough what what you're going to be able to get the actual wise and it does so with that project conversation is going to be next I'd do you say couple card you think I'll get the website all the information and products on the website so you start thinking about it weekend the project page 