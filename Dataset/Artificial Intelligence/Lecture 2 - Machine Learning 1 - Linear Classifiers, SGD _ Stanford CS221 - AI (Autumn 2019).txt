ok so let's get started with the actual technical content so remember from last time we give it all of the other class we talk about different types of models that were going to explore reflex masterpiece models variable baseballism logik portable switch see throughout the course but underlying all this is your machine learning what allows you to take data and two in the premises of the models you don't have to work as hard as I can so in this what's wrong going to start with a simpler of the Moro reflex base model I'm in show how machinery can be applied to this type answer other class are we going to talk about different types of models and how learning help with them so there's only three parts Wigan Park railway new predictors which includes classification regression what's minimisation which is basically sodium the functions of how are you I want to train your machine on model and then stochastic gradient descent which is an algorithm that allows you to actually do the work so let's start with perhaps the most example of so you have to do stamp classification so the input is x email message anyone to know whether email message is a spam or not so we're going to know the output of the class Part 2 by which is in this case either spam or not Sam and our goal is to produce a predictor app right so predictor in general it's going to be a function that Maps some import Access to some output y in this case it's going to take an email message and map it to whether the message is spam or not types of prediction problems final classification is the simplest one where are the output is 1 of 2 possibilities either yes or no and we're going to do this + 1 or - 1 sometimes lc10 regression where are you trying to predict on numerical value for example but it housing price there is a multi-class classification whereby is not just two items bum possible 100 items may be three different categories there's a ranking where the output is a permutation importance come useful for example if the input is a set of articles or products or what pages do you want to ring them in some water to show Toulouse structure prediction as well why the output is an object as much more complicated perhaps it's a whole sentence or even image it's something that you have to construct you have to build a scene from scratch is not just a label line and there's many more types of production problems but under my orders no whenever someone says I'm going to do machine on in the first question is asked it's OK what's the date because there's no learning so unicorn example XY pair is something that specifies what the output should weather in Porto and a training data or a set of examples the 20 said is going to be simply a list or multiset of exam so you can think about this is a partial specification of remember what time to design A system that has certain types of behaviours and we're going to show you examples of what I should do if I have some email message to have cs221 that it's not spam but if it has lots of things then Ibiza and so remember this is not a 4 specification these are 10 examples or even a million examples might not tell you what exactly this function supposed to do is just examples of what the function could you Undisputed ok so once you have this data so we're gonna use detrain to denote the dataset remember it's a set of input a we're going to put this into a learning algorithm or a learner and what is the Lonely album going to produce it's going to produce a predictor predictors are app hair on the predictor remember is what is actually itself a function that I'm text an import tax a map set to an apple prices of Two Lovers can you understand this in terms of the modelling inference someone is about other question of what should the types of predictors consider our inferences about how do you compute y given x learning is about how are you today introduce a predictor so that you can do in France any questions about the sofa this is pretty high low and abstracting generic on purpose because I want to highlight how general machine learning as before going to the specifics of so there's an abstract ok so what's the digging of about 2 this actual an actual problem so just say simplify the email problem what's consider task of predicting whether a string is a email address or so the import is and is a string and the output is it's a binary classification prometida one if it's an email or -1 that's what you want the first step of doing linear prediction is no speech extract in a question you should ask yourself is what properties are the import tax might be relevant for predicting the April writes up really High Line might be but you're not trying to include the actual set of Rules that solve the problem trying to do it princess for learning your kind of taking it no more the backseat you're saying well here are some hints that could help you ok so formerly a feature extractor takes an input annales put a set of featuring name feature value pairs go to Example so I've abci gmail.com what are the properties that might be useful for determining whether well you may consider in the life of a star no greater than 10 maybe long strings are last likely to be email addresses and shorter ones and hear the name is 9th Grade in town so that's a label of that future and the value that future is one representing it's true so it was 0 if it's false another feature of the fraction of alphanumeric characters what is the number there might be features that test for particular no letters for example so that as a your future because there is an outside ends with a comma and that is 0 because that's not true so you can have many many more and we will talk more about Peaches and I'm next time I said of properties you have distilling down this import which is could be a stringer could be your images could be something more complicated into kind of your ground up dash in that later we'll see how I can take advantage of ok so you have this feature bachata how's is a list of feature value and their associated names of labour but later will see that the the name stone matter to the lawyer so actually why you should also think about the feature vector is simply a list of numbers I'm just kind of honest I'd make a note that all this in your position number 3 corresponds to contains at and someone right so I have to still the email address mecl gmail.com into the list of numbers 08410 45110 that's a feature extraction and complex objects into list of numbers which will see it what further kind of the lingua Franca of these machinery ambulances ok so I'm going to write some concepts on the border of Concepts I'm going to introduce and I'll just keep them up on the board for reference so feature vector is going to report an ocean and standard fee of axon Infinite Sofia self sometimes you think about you call the feature map which takes an input and Returns a vector and this notation means that Returns in general dimensional Factor so list of the numbers and the component of this feature vector we can write down as vy p2r the way to free ivax ok so this notation is your convenient because we're going to start shifting our Focus from thinking about the features as properties of employed to features as kind of mathematical objects in particular the effects it is a point in a high-dimensional space so you had two features that would be appointed two-dimensional space but in January might have a million feature so that so future it's a point enough Amelia International so I might be hard to think about that Facebook well we'll see how we can later in in a bit if so that's a feature Becker you take an employed and return a list of numbers and now the second piece is awake actor so my right down way back there so why Factor is going to be noted w and this is also a list of Dean so it's a way to think about the weight Factor is that for each feature J so for example of Alpha we're going to have a real number WJ that represents the contribution that featured so this contribution this point what is the score 0.6 mean so so the way to think about this is how you have your word Becca can you have a feature vector of a particular input the score of your prediction is going to be the dot product between the way doctor and the future Factor ok so that's why which is Ronaldo's basically looking at all the features and multiplying the future value times the weight of the future and something about those numbers this example it would be - 1.2 that's the weight of the first feature time someone that's the feature value are slicer 0.6 x if I vent until you get this number 4.51 which is happens to be the score for this example so the question is is the feature extraction manually so off c is going to be implemented as a function like in code arm you're going to write this function manually but no the function itself is run automatically on examples later we'll see how you can actually learn features as well so you can slowly started do lots of the menu at her but we're going to hold off until next time for that question percentage change so the question is about interpretation of wi sometimes weight can have a more precise meaning in general you can who is entitled to read the tea leaves but I don't think there is a pier in general mathematical process that you can say about individual weights of positive and negative negative and the magnitude of his house drawn without other person feels about the prediction right so you know contains AS3 because maybe like at signs generally do work email addresses but you know the fraction alphanumeric characters sylvania were focusing on linear classifiers the question is what happens if you have an owner with more layers there's going to be more dark part ex but there's also good it's not just adding more features that there's going to be other components which will get you in later life yeah so the question is do the weight have to add up to something that are showing the answer is no there's a restricted settings where you might want to normalise the the way to something but when you know cancel that right now later will see that the manager of Windows tell you something ok so just to summarise it is important to note that the the way back there's only one way back to where you have to find one set of parameters for every everybody but the feature vector is poor example every employee you get a new feature vector dot product of those two wait accommodation features is this is a score if so so now let's try to put the pieces together and define the actual Project I say remember we have the same box with Ethernet which takes x and Returns why so what is inside the box hopefully giving you some into a shadow my god you're bored and write a few more things so the score remember is wwx and this is just a number and the projector so when you actually let me call this in more precise is a clean your classifier not just a predictor of hours does a projector that does classification solinear a classifier that we were used to the predictors wgs means that this predictive depends on the particular set of weights this predictor is going to look at the score and return the sign of that score so what is a sign the sign what are the scores is it a positive number if it's positive I'm going to return plus one if it's remember I'm going to turn minus one and if it's 0 then you I don't care you can return plus one if you want it doesn't matter so what is doing remember this the score is either is a real number so it's either going to be like kind of leaning towards large value values towards us large negative values and thus I basically says ok you go commit are you which side are you on are you on the positive side or on the negative side and is it ok ok so let's look at a simple example because I think a lot of money before his kind of more the former machinery Behind the Mask behind how it works but it's really useful to have some geometric into it because then you can draw some pictures ok so let's consider this so we have a wave vector which is 2 - 2 - 1 and a feature of Vector which is a 20 and another feature vector 02 24 there's only two dimensions I can't do that care so here is a two-dimensional flat and lots of the first ok so wave actor is going to be at 2 - 10 that's this point in a way to think about the way back here is not the point but actually the vector going from The Origin to the four reasons I become clear later that's the other so we have 200-202 here and 24 is ok so how do I think about what is when doctors is doing so just remember the classifier is looking at the sign of WWI so let's try to do a classification on the street so why write it off when I do WWF it's alright because this is 20022 products for and take a sign what's the sign of four ok so that means I'm going to label this point as a positive right as a point ok what about 0 to actually sorry this is just between this I get -2 and then the sign of minor serious -1 ok so that's a minus this morning so what's the top right there it's going to be 0 ok so so this Pacifier were classified this point as a positive this is a negative in this one I don't know ok so we can still a more points but but yeah they might see kind of medium or general Patton and one had to fill in in power board with classifications yeah so so what's tried to drive the orthogonal ok ok so let's draw door final this is a right angle and that man said is that the point over here because it has a cute angle with W is going to be a classified as positive so all the stuff is positive and everything over here because there's up to single with W is going to be -4 everything over here is negative and then everything on this is going to be 0 so so I don't know ok this line is called the decision boundary which is a concert not just for linear classifiers but whenever you have any sort of classifier this is your boundary is The Separation between the regions of the space where the classification is positive versus negative and in this case it's it's separate because it's we have in your past the singer band Rea Street and we just separating the space into a house if you were in three dimensions this vector would still be just so you're better but this decision boundary would be a playing so you can think about it is coming out of the board if you want but I'm not going to try to drop that that's that's kind of the geometric interpretation and how linear classifiers yes so that's a good point so the observation is that no matter if you scale this weight by 2 is actually going to still have the same as a decision boundary so the magnitude of the way it doesn't matter it's the direction that matters so this is true for just making a prediction when we look out learning the magnitude of the way will matter because we're going to consider other more new one Star Wars pumpkins ok so let's remind me questions about any predictors so far away we've done is we haven't done any more notice we've just simply defined the set of predictors that would interest so we have way back together get a score and then you can send them to function I've used my new car suppliers but there's no specification so now let's actually turn to do some learning so remember this framework learning needs to take some data and return a predictor in our predictors are specify by a way back so you can only think about the learning output in if you want for linear classifier I'm unpack the lawyer so the only over them is going to be based on optimisation which we started reviewing last luxury which separates are what you want to compute from how you want a computer so we're going to first define an optimisation problem we just advise what properties we want to have in terms of a date and then we're going to figure out how to actually optimizer and this modular is actually really really Powerful yes and it allows people to go ahead and work on different types of criteria for different types of models separately from the people who actually develop general purpose algorithms ok so what start with optimisation problem so there's an important concept called a loss function this is a super general idea that's using the statistics function text a particular example x y and a white actor and Returns a number number represents how unhappy we would be if we use the predictor given by W to make a prediction on x when the correct output is why this basically is trying to characterize if you had me a classifier in I go answer this example and tried to classify is it going to get it right or is it gonna get it wrong so how was his dad you don't want to lose low losses good so normal 80 loss is that the best you can hope for ok so let's do figure out the loss function for binary classification so just some notation the correct label is dry and the predicted label remember is the score centre of the sine function and that's going to give you some pretty label and that's a look at this example so we Course 2 - 13 of x = 20 and why so we already find a square as an example is a WX which is how coughing and where predicting my plus one that's the way to Nottingham ok so what's the score of the for this procedure again it's for which means positive the question is whether the output of loss function is usually as single number or not in most cases physical practical cases think about the single number there are cases where you might have multiple objectives that you're try autumn but in this class is always going to be you care about you know both tired and space or accuracy but robustness something sometimes you have more time objective ok so we have a score define margin so let me so what's action do this so want to talk about classification I'm going to sleep regression then in a bit so score is that affects this is how confident we are about plus one and the margin is the score at x y I'm in this relies on weidian + 1 - 1 so there's my CIMA love in mysterious you know just say so in this example the score is 4 so what's the margin you x - 1 is in the margins interpretation is how correctly right so imagine are the correct answer is have the same sign then you're going to get positive number and then the coffee then the more caffeine you are than aimbot why is -1 and the scores positive then the margins can be negative which means that are you going to be confidently wrong which is bad ok so just to see if we can understand what's going on so what is a binary classifier making mistake on a given example and so I'm going to ask her can I have a Show of Hands how many people think it's when the margin is are less than 0 I guess you can kind of stuff there used to do these online quizzes where it was well I do it ok so yes the margin is less than 0 when emergencies Latin zero that means why and the score different signs which means that you're making ok so now we have the notion of Martin let's define something called 010101 very creative me so the last function is simply did you make a mistake on this notation let's try this out for a bit so FX here is a prediction when the input is x and magical wires saying did you make a mistake so that thing as a bullying and this one bracket is I'm just notation is called a indicator function that text I condition and Returns either 1 or 0 so if if the condition is true then it's going to be turning while and the condition is false to return 0 all this is doing is basically returning 1 if you made a mistake and 0 if you didn't make a mess and we can write as follows we can write that the margin last equal 200 that we've made a mistake and we should incur a loss of 1 end of the margin is greater than 0 than we don't make a mistake in witchery control alright so it will be useful to draw these last functions pictorial like this ok so on the accent xx is here we're going to show the margin remember the margin is how you are and on the y-axis we're going to show the the loss function which is how much you are going to suffer for it ok so remember the margin if the margin is positive that means you're gonna be alright which means that the lot is 0 what are the margin is listen 0 m long in the losses one thing that you should have in mind when you think about her yeah so there's this kind of boundary condition of what happens exactly as I'm trying to sleep on the right because it's not terribly important cure it's last 0 to be honest with you don't know you're also Gonna Get Along otherwise you could always just Returns 0 and that's that ok so is it any questions about kind of binary classification so far so we predictors inactive find azaron loss as a way to capture how unhappy we would if we had a classifier that was operating a particular data point so Justice I'm going to go on a little bit of a depression in talk about when you're regret on land and the reason of doing this is that what minimization is such a powerful and general framework and transcends you know all these questions so what kind of emphasise the overall overstory give you a bunch of different examples I'm classification regression side by side we can actually see how they compare and hopefully they're the common denominator of emerge more clearly ok so we took a liberal international last Watch right so is the new Russian sentences simpler than the because if you have anaemia predictor and you get a score wwfx is already a real name turn linear regression you simply return that real number and you called that your prediction ok so now we let's move forward to find you lots sunshine so there's gonna be a concept that's going to be useful it's called a residual which is against kind of how wrong you are I'm so cute is a particular linear predictor linear regression and making predictions or long enough for different values of x I'm in here the data point of fear of x ok to the residual is the difference between the true value y and the predictive value y arm and in particular the mall by which the prediction is overshoot you're the so this is this difference on and if you Square the difference you get something called the square moss so this is something we mention last luxury residual can be either negative or positive but either if you're very positive very negative that's bad and Squirrel makes it so that you're gonna equally for carers in positive Direction price of the square glasses that residual score solicitor Roscommon support snap also here we have our way Victor 2 - 1 the feature vectors 20 what's the score at 4 why is Mary -1 so the residual is 4 - -1 which is 5 5 squared is 25 so the squares on this particular example queso what's price so just like we did it for a 0 on us what see what is like function looks like so the the horizontal axis here instead of being the margin is going to be this quantity for regression called the residual it's going to be the difference between a prediction and the true target I'm going to arm and dislikes function is just so with the residual zero then the last is 0 is as residual grows in either Direction that I'm going to pay I've something for it and as a quadratic penalty which means that inaction pros pretty fast so if I'm your the residual is 10 then paying 100 ok so so that's the square loss there's also another loss our throw in here call the absolute deviation last if you don't know about the question you might immediately computer is basically absolute difference between the prediction and the the actual true target the square last longer discussion about Norwich last function on your makes sense the salient points here are there the absolute deviation last is kind of hazards are kinky and it's not smooth sometimes it makes the heart an atom but the square last also has this kind of thing that blows up which means that it's I have it really doesn't like having out wires or really is marshmallow because it's gonna pay one for it but I just love I just think about this is a Different Class which combines both of these is smooth and also grows a new answer correctly ok so we have a both classification you're my precious define margins and residuals regards different and now we want to minimise the ones search for one example this is really easy right so if I if I told you ok how do I minimise the last mortgage $0 so that's are stupid and this course was the fact that you know if you have a trying to fill 1.4 so that's colorpoint are the point of missing on as I have to fit over remember you only get one one way better you have all these examples of what you want to find one-way doctor that kind of balances errors across although any general you might not be able to achieve whatever stop light colitis so you have to make trade-offs in a witch examples are you going to sacrifice for the good of other examples and this is actually a lot of where you know issues around furniture machine learning actually come in because in cases where you can actually make a prediction that's equally good for everyone how do you actually know your responsibilities but no that's all that's all brother topic let's just focus on trade off the fine by the simple some overall the example so we want to minimise the average loss overall examples ok so what do you have his last weekend if your average over the 23rd you get something watch working at Coltrane loss and that's a function of w any questions about this ok so there is the discussion about which regression what to use which I'm going to skip you can feel free to read and notes if you're interested the punchline is bad if you want things that look like the mean-square loss if you want things I look like a media news absolutely umbrella for now when do people start interview question why interim last minimisation as some regression has least squares regression is from like the early 1800s so it's been around for is no can you call the first machine that was ever done if you if you want I guess the last minimization framework is it's hard to pinpoint a particular point in time it's kind of nada I'm is that like innovation in some sense it's just Morgan at least right now it's kind of all pedagogical to organise all the different methods that exist do you mean that yeah so I don't want to get into this examples about briefly if you have 3 points are you you can exactly perfectly you if you use absolute deviation then you're going to find the median value basic predictive median value and if you use the square large you can predict the mean value I'm happy that I can't find them ok so what we talked about so far as we have a wonderful in your predictors which are driven by feature vectors and web actors and now we can define a bunch of different loss functions the capture no how you care about regression and classification and now let's try to actually do some real machine learning how how do you actually optimise this remember the learner is going so now we talked about the optimisation problem which is minimising the training loss I will come back to their next year and then and now we're going to message and so what is an optimisation problem ok let's just abstract away from the details worry about if it's all the square Los or some other what I think about it someone dimension the training last my love something like this you have a single wait in for each way you have a number which is your lasagna anyone to find this so I threw dimensions it looks something like this because I think you'll be useful ok so into dimensions what atomization works like as follow so I'm going to I'm now plodding W1 and W2 which are the two components of this two-dimensional which actor for every point I have a y Factor and that value is going to be so what the training loss and it's it's pretty standard in the settings to draw vertical level curves so what's to do this soeech Kirsty is a ring of points work at the functions are you is identical see you look at your name apps those are love her so you know I'm talking about so this is a minimum and as you can grow larger and larger right and the goal is to find the minimum how we going to do that so there is a last Pumpkins there is no necessary as single minimum I'm just doing this for simplicity attend out to be true for many of these linear classifiers ok so last time we talked about gradient descent French and understand is that what I don't know why they said so let's just sorted 0 as good as any place and what I'm going to do at 0 is I'm going to computer gradient so the gradient is this a vector that is perpendicular to the level curves so the gradients is going to point in this Direction that says in this direction is where the function is increasing the most dramatically and rear sensors goes in the opposite direction because remember to minimise loss so I'm going to go here and now I'm hopefully we do smile function value not necessarily but we hope that that's ok now we compute show me this morning this way so go in that direction and maybe an hour this way and I keep on going this is a lot of them made hopefully eventually I get to that I'm kind of simplifying things quite a bit here so there's hopes that studies exactly what kind of functions you can optimise and how great in the sound when it works and when it doesn't I'm just going to go through the Mechanics now and that kind of photo proofs of when this actually works until later ok so that's kind of the schema of house read the sound work so in cold this looks like they're sorry initialised at 0 and then leave and some number of iterations which nuts for simplicity just think there is a fixed number of iterations and then I'm going to pick up my weights computer grading moving opposite direction tendencies going to be a step size that tells me how fast I want to make progress and will come back to you know what the sub size ok so that specialises to at least squares regression so we can do this or last week just a review so the training loss 4 least squares are requesting is this so remembered the average over the loss of energy examples in a lot of a particular example is the residual squared so that's the expression and then all we have to do with computer grading no if you remember your calculus it's just use the chain rule so this to come down here if you have the you are the residual x the derivative of what's inside here and the gradient with respect to w is a Fairfax so last time we do this in Python in one dimension so one-dimensional hopefully of you should feel comfortable doing this because this is just calculus Kieran we have we is a vector so we're taking the rivers Google taking gradients so there's no some things to be wary of I'm in this case it's something I've useful to double check that all the gradient version actually matches the single dimensional version because last time remember we have Excel I'm in one printing Oak here is that on where is the prediction - target that's a residual so the gradient is driven by so yeah the prediction equals the target what's a gradient can be 0 which is what you want if you're already getting the answer correct than you shouldn't want to move your whites so often you know we can do things and and everything will work but you know it's often a good idea to write down some objective functions take agreed and see the green in the sand on using these gradients are you computed is kind of sensible thing because there's many layers you can understand and get intuition for this stuff that account abstract logo optimisation or at the algorithmic lover like a pickup an example is it sensible to update the radio when a prediction close the target ok so so what's take the code that we from have from last time I'm going to expand on a little bit and hopefully set the stage for doing the cassegrain ok so the last time we had cancel remember the last time we define a set of points we defined a function which is the train last year are we to find the derivative of the function and then we have created so I'm going to do a little bit house cleaning just don't mind ok so I'm going to make this a little bit more explicit what this album is Green Day Centre pans on I'm a function of the liver function and I like to say dimensionality and I can call this after in this case it's ok you know what to separate this is like an algorithms this is your mama so this is what we want to compute and this is how we cook underskirts ok alright so what I'm going to do now is upgrade this to Vector remember the experience just a number so in Python import a store which is nice vector and matrix library and I'm going to make some rays here which this is just going to be a one-dimensional race Attax I'm so this WWE cons arm and it stink and we needs to be alright so that's still run actually sorry this play song remember last time we ran this program and it starts out with some weight and then it converges to point in the function value can alright so what's try to no it's really hard again see whether this is any interesting because we only have 2 points so how do we go about no because I'm going also implemented at Tesco's to see this this cannabis technique which I really like and it's you know which is to call in a generator artificial data main idea sad know what is learning your learning is your chicken status sad and you're trying to fit a fine for the weights that best fit but in general I generation if I download the days I have no idea what I think where I go back where they say ok the side weather right now what's a the right answer 12345 SE5 mentioned I'm going to generate some data based on that so that this with actor is kind of good for that Escobar my price so I'm going to generate reports can I see more efficient as you can join as much as you that's the question oh so true we just means like for the correct the ground truth so we was a wetty so this kind of going backwards remember I'm on the way back for a bud I'm just saying this is the right answer so I want to make sure that I'm actually recovered Tennyson random so there's a nice function random that ran Dan which generates are ran dimensional vector and why I'm going to set what I said yeah I'm going to do refreshing so I want to do food WWE X-ray so and everything about it this data this is true we is the right thing the world dead zero lighter but I'm going to make your life a little bit more interesting I'm going to sunrise pronova dad looks like also I should added dinner set I'm so crystal this is my data Cairo Tower restaurant look at the code you can assure yourself that this data has structure in it ok so let's go Rivers w train and see what happened so what's one thing I forgot to do so if you noticed that objective functions that are you're in town they haven't / the number the weather average last night the song the song then things got really big blow up so that's ok so it's training strain actually so let me do more I did 100% of 3000 so on your the photos going down that's always something good to check and you can see the weight or slowly getting to know what appears to be 12 34 so now this is a hard proof by the evidence of this learning algorithm is actually ok so now let's see if no more points so now have 100000 points slower mineral operator chair any questions so what I do here I define what functions to the derivatives the green defenders will be implemented last time annoying different I did that time is generated datasets I can't check weather Greenwich question the question is whether the fact that the gradient is residual as there are them to learn from under overpredict yeah so the gradient is if you think about it yeah that's good intuition so if you look at if you're over predicting that means the grinners the terms of this is like so that means this is going to be positive which means that if you up that way you can overpredict tomorrow morning in Tramore so my subtracting a gradient and pushing the weights and same for when you're so that's good what is the effect of the North the final noise that makes the problem no harder so that it takes more examples to if you shut off the noise then it will have prison way old learning no the noise ISM ok so so that's I have 1000 examples no that's quite a few examples and you know this argument runs so grinder sander in ab so how can we speak in sap hello that was a problem well if you look at the what album is doing iterating and each iteration in its Computing the Green overtraining the 20 laws is an average of all the points which means that you had to go through all the point in computer gradient the Lost In and everything that's what it is expensive your text time so you might want to know how how can you avoid this and if you want to bring us and you have to go to all your point and the the key inside behind the castle radio sound is had well maybe you don't have to do that so maybe you're here something to wash it right so what it what is this gradient so this great is actually the sum of all the gradients from all the examples in your trainers so you have 500000 point aliens that so actually what this great is is is actually kind of some of different things which are maybe Pony and Sally different directions with all average out to this Direction so maybe you can actually not average all them but you can average just a couple or maybe even in extreme case you can just like take one of them and your Martin address socials idea behind the category so instead of doing where can I change outbound to save for each example in the training set I'm just going to pick it up and just update you know it's like with all the training samples and pick up one it's not about quantity it's about quality world's best like last time but it seems to work in work in here another is also this question of what should the step size and in general and it's actually even I've been more important because when you're updating on in each individual sample you're getting home noisy and if you haven't asked me like my step size and there is no formula there are formulas but there is no definitive answer who sang general guidance so if step size is small thyroid closer zero that means you're taking tiny steps that means that you take longer to get what you want to go but you're kind of preceding cash receipt so it's like we're going to know if you messed up and going around Direction you not gonna go too far around Direction conversely if you have 8 of you really really large then you know it's dry Roy fast but you might of just can't dance around a lot I'm so pictorial ey time this looks like is that you're here is maybe a moderate step size but if you're taking steps really big steps yeah you might go over here and then you jump around and then maybe you may be around at the right place but maybe sometimes you can actually get flung off out of Orbit and situation so there's many ways to set us up so I can't stand it usually have to return it or you can set it to be decreasing going to wash and being that as you optimise and get closer to the optimum you cannot want to slow down right like a few you're coming on the freeway driving with fast but once you get your house you probably don't want to be like driving 60 mph ok so actually I don't do that so let's let's try to get to Castle Green ok so so the interface is stochastic gradient changes so right so ingredients and I need as a function is a scanner computes the sum of all the training sound sensor Casa gradient I'm good just as for the category I'm going to take an index I to update on iPad something on my computer last on that and same for that it's the riveter can look at ipoint just computer gradient and distribute ok so now in serv-u ingredients and posterior stochastic descent I mean I'm going to pass on an SSD dian the number of pi cos I need to know how many copy greenis and it's basically the same functions just going to stick another four stochastic gradient descent it's going to take a classic functions classic radio I'm the dimension audio and ok so now before I was just going through number of iterations now right I'm not gonna hiya computer values at all the training examples I'm going to loop over all the points I'm going to call just evaluate the function that point I a computer gradient at a point instead of I mean everything else is the same and one other thing I'll do here is that US use the different step aside schedules winder bye-bye number not updates so I want it so that the number of other step sizes County crossover so am I start with 8 = 1 and then it happened it's a third in IT support sometimes you can put a square root and that's more typical in some cases question yes no question is the word stochastic means that there should be some randomness no technically speaking the the cassegrain understand is where you're settling up random point and then actually I'm going to bed because I'm in training overall the point your own practice if you have a water point and you randomizer no it is similar but there is a kind of a technical Hi-de-Hi so this is The cassegrain Descent iterate over all the points and just so what's this word I don't think I'm maybe Man City what happened did trailer 100000 an upset anyone see the problem also I'm printing vs out add Aberfan duration so that should be fine Sabrina the sound was working right maybe I'll try probably not the best idea to do Dragon alive b m graded the sound works ok so that was working right KSI stochastic gradient descent fast and converters I don't Converse the right answer yeah but that's in fermented wine so that might be ok so I do have a version of this code that does work so what am I doing here someone someone ok this version works lots lots alright now works thank you I'm sorry yeah this is a good all lesson is that when you're dividing this needs to be one this is not a problem but I'm sorry I'm fine thank you for some reason but they should be 1.0 / update otherwise I was getting ok so why is it ok what side go back to 500000 ok so what for sweet potato is the same amount of time but you noticed that immediately already converges to 12345 right so this is like way way faster than greatest remember I just cannot compare predecessor is you Ryanair after one stop it's not even close what noise level do you have to have until gradient descent becomes better sewed it is true that if you have more noise ingredients in my team and some can be unstable they might be waiting mitigator with step size choice but yeah probably you have had a lot of noise I mean this isn't Samsung's you know if you take a step back and think about what's going on his problem is the 5th dimension five numbers and I'm feeling it half a million data points right there's not that much to learn here so there's a lot of redundancy in the day said and generally actually this is true like a large data set is going to be a lot of redundancy so going through all the data and and trying to make informed decision is your pretty wasteful where sometimes you can just cannot get a representative sample I'm one example of Morris comment to do with mini batches we maybe grab 100 examples and you up on that so there's a waiting somewhere between the cast ok let me move on ok somebody is so far we have linear predictors which are based on scores so they predict include both classifiers in request we can do lice minimization and we can if we implemented correctly we can do ok so that was I'm kind of switching things I hope your phone alarm introduce by the classification and then I did all the optimisation for linear regression classification and see if we could do to castle Grange ok so first classification remember we decided that the landlord is the thing we want we want to minimise the number of mistakes in your who can argue with that remember what is 01 last look like looks like this so what happens if I tried to run stochastic gradient descent on this I mean I can run the code yeah it's in work right and why won't it work 2 popular renters are it's not different one problem but I think the biggest problem is what is the what is the gradient 080 basically everywhere except for this plane with no doesn't matter so so as we learnt that if you try to update with a grading of 0 then you don't move your words to someone that work on this are wise so that's kind of Unfortunate so how do we fix this problem yeah let's let's make a green-eyed 0 excuses so there's one last which I'm going to introduce carbon hingeless which does exactly that is basically is zero here when imagine is great and risers literally so if you gone in a correct by a margin of 1 so you could have pretty safely on the air side of a correct we watch hard you anything but as soon as you start in this area we're going to charge you a cannibal in your mouth and your loss is going to grow when your way so there's some reasons why this is a good idea so it upper bounds the zone Las it's has a problem known as convexity which means that if you actually find the greatest and you actually can convert to the global optimum I'm not gonna go into that Aldi so that's me that's a hingeless so what remains to be done is to compute the gradient of this your homeslice how do you complete this gradient so in sometimes is a trick question because the gradient doesn't because it's not everywhere but doesn't exist ok so so what is this is actually two functions right there is 80 function here and then there is like this or 1 - x function so what my plan is here on finding them the margin and so this is a 0 function and this is a 1 - wxy and it's just the maximum of these two functions just taking the top function so that's how I am able to trace out are this alright so if I want to take the radio this funk you can try to do them that thing to read what what is the gradient with the gradient is zero distributor whatever gradient this function is right so general when you have a green of us as it's Max you have to come pick it up in the cases and depending on where you are on you you have a different case so what is equal to if I'm over here in what's the condition for being over here is the margin is greater than one right and then otherwise I'm going to take the grading of this with respect w which is going to be - 3 XY otherwise so again we can try to interpret the gradient of the hint lies so remember your insta cassegrain to send you have a way back and you're going to pick up an example and use a computer green and move away from it so if your dinner an example right then the greatest euro don't move which is the right thing to do and otherwise you're going to move in the direction because you're - - of fear of XY which kind of imprints this example into your way that you're so and you can follow that is actually increases your margin after you do this yeah what's the significance of the modern by this is just on value and support vector machines you set it to 1 and then you have regularisation on the weights and that gives you a some interpretation so I don't have time to go over there right now but there's another lot function you have a question origin at the mosque why you choose the margin so in classification were going to look at the Martin because that tells you how long will your predictive are crackly in regression you're going to look at residuals in Squirrel also depends on what problems you just really quickly some of you might have heard of logistic regression logistic regression is this yellow last funk so the point vs saying that this last minimization for Marcus really general in a lot of things that you might have heard of least squares regression or a can of special cases of there so if you get a master how to do last minimisation you kind of I can do it all ok so summary basically got on the board here if you doing classification you take the score which comes from the WWF x and you drive it into the sign-in then you get one of my Swann requesting you just use a score now to train you have to assess how well you're doing classification there is the notion of a margin in Gresham is the residual and then you can define lot functions and only took about 5 licence but there's many others especially for account structure prediction or drinking problems is all sorts of different functions but they cannot based on these simple ideas of know you have a bounce 01 if you drink classification and some for a square like air for your progression and then what you have your life functions providers 901 you can atomizer using the which turned out to be a lot faster next time we're going to talk about Fairfax which was kind of last as someone just answer to you and talk about what is it true objectives and a really to optimizer training so next time 