this luxury is going to be on reinforcement learning I will interests this time skip the quiz so so the way they think about how the important marine fits into we've done so far is remember this class has this picture rising we talk about different models and different algorithms inference algorithms to be able to predict using his models and answer queries and then we have learn which is how do you actually like this right so every type of model we go through we have to collect the boxes for each of these pieces I'm so la luxury we talk about market decision process this is a kind of modelling framework allows you to define models for example for Cross volcanoes or dice game orange triad taking tram where abouts in France so what do we have here last time we had value iteration which allows the computer auto policy and policy re-evaluation which allows you to estimate the value of a particular policy so Deezer algorithm that are operate on it play and we look at these over on last time so this letter is going to be Bal I just put or Alpha now Dr H not an algorithm refers to the family of alarms that fits this week but that's the way you should think about it RL allows you to either explicitly or implicitly please and then once you have do all this I can French algorithms to figure how were the atom party so just a review so what is a MVP the clearest way remember to think about it is in terms of a graph to have a set of states turn the Sky skin we have in so we have the services from every stage you have I said actions coming out so in this case stay quit actions take you to chance nodes where the you don't get to control what happens but nature does and there's randomness so out of his trance now are transition detransition taking into a state has some probability associate with it so two-thirds in this case also has some reward associate with would you pick up along the way so that's why this has to be one third four and last time this was probably why so and then there is no the discount Factor download what's the number between zero and one tells you how much you value the few if I can think about this one offers simplicity cancel this is a Markov decision process and what do you do with one of these things we have a notion of a policy Anna policy I'll read another so policy if they know the pie so policy hi is a mapping from States to Action it tells you a poser when you apply says when I land here where should I go shy do say I quit if I land well I mean this is kind of a simple and usually be more States and forever blue circle tell you when I go when you run a party what happens you can apply which going to con episode so what you doing are you starting a status 0 that will be in in this particular example you take an action A1 let's I stay you'll get some reward in this case over before you end up in a new state S1 and supposed to go back to it and then you take another action movie that stay reward is 4 again and someone so this sequence is a pot or in oral speak episode a racist this is an episode and until you hit the end state and I what happens out of it episode you can lochaline utility we're gonna do not new which is the discounted some of the rewards along right so if you you stay three times and then you would have tutorial 4 + 4 + 4 + 4 so the last lottery we don't really work with the episodes in the utility because we were able to define set of recurrences computer is the expected YouTube so remember that we want we don't know what's going to happen so there's a distribution and optimise something we have to turn into a number that's what expectation does so there's two of concept that we have from last time what is the value function of a particular policy some vpilot is expected utility if you fall apart from us what is that that means if you take a particular task that steak and I put you there and you run the policy so stay can you traverse the sky you have different new toys coming out in the average of those is going to be the pi of Us similar leaders rq value I'm expecting you to see if you first take an action from state as an envelope I so what does that mean that means if I put you on one of these red chance now Enya basically play out of an average result and utilities that you get what number do you and we started recurrences that related to these two so vpls is you request the nearest account delegate to some simple problems you first lookupvalue sausage in ass that's Pius and that takes you to chance note which is a comma pious and then you say how much utility am I going to get from and thoroughly from the Chestnuts you have to look at all the possible successors the probability of going into the successor of the immediate reward that you get on on the edge plus the discount reverso of the kind of future when you end up in a spring so any questions about this is a review of Markov decision processes last ok so now you're about to do something to four ok so if you say goodbye to the transition in rewards does cold reinforce so remember might have the same processes I give you everything here and you just have to find autumn now I'm going to make life difficult by name and tell you what rewards and what are transition ok so just give her kind of flavour of what that's like let's play a game so I'm going to need a volunteer I'll give you the game but this one here you'll have to have a lot of a grit and I'll persistence because this is not going to be an easy game has to be one of those people that even though you're losing a lot I'm not give up ok so here's how the game Southridge round articles 1 2 3 4 5 6 who's going to choose A or B red or blue calor gas you moved to a new state so the state is here and you get some rewards which I'm gonna show here and the state is 5, 0 that's a nationalist so everything clear about the rules of the game does ria Forsyth you don't know anything about how ok so how about you and a friend ok hey mdp play Every Time a word OK I'm glad it's because the last time I took a lot longer about so what did you have to do and you don't know what if I say try and and then hopefully your Building and DP in your your head right ok smile night and you have to figure out how the game works so maybe you noticed that he is no documented in and beers are coming up but then there's other that gets so can you figure this out in a process you also trying to maximise reward which I guess was doesn't come until the very end because it's a cruel game ok so how do you get them to kind of duvets and how do you think about I'm doing this so just the kind of make a contrasting empty please only and personality Sharp so Marketing Services are offline so you already have a metal of how to work world works that's that's all that we was in a transitions and Estates and you have to find a policy to collect maximum reward you have it all in your head so you just kind of really hard about when you're what is the best thing is if I do this action and look at the probability the weather for Sunny is very different don't know how the world works you can just sit and think because thinking isn't going to help you figure out the work so you have to just go out and perform actions and and in doing so you hopefully you're learning something but also you're your gets resources formulas the the paradigm of RL you can think about is an age that's all that's you and or do you have the environment which is everything else that's not ageing agent takes tax action to the environment environment just sent you back rewards and a new state can you keep doing this so what you have to do is figure out first of all how to am I going to ask if I'm in a particular state as 2 - 1 what action should I take that's one one question and then you can get this reward in Observer new state how what what should I do to update mental model ok so these are the main 2 question we'll talk first about how to update the pram and then later in the letter I'm going to come back to how do you actually go out ok so much here but in the context of ok across just the kind of things through things every time you play the game right you're going to get some Nutella you take off so this is episode over here so ars you're going to sometimes you your phone to a parrot sometimes you go too hard and based on these experiences if I didn't hadn't told you what any of the action do and what's a slip probably or anything how would you kind of go about I'm kind of Sadness that's so that's ok so there's a bunch of algorithms I think there's going to be 1 2 3 4 for algorithms I'm going to talk about with different characteristics but they're all going to each other and some the first class of Abra is Monte Carlo so ok so when you ever you're doing or any sort of learning the first thing you get is you just have data lots of lots of clothes that you run even a random policies because in the beginning I don't know any better just going to try random actions the process you're going to see hey I tried this action and a lot to this reward and hence so in a country example just to make clear Chris it's going to look something like and then you take your you did colour so you're in you do stay any idea of free water for and then you're back in you do a stay and then you get for and then maybe your ok so this is an example episode I'm just to make this is 0 ay are Wang S2 S1 Romanian to A2 R2 ok so what should you do here right so any ideas moebius model you will be down we will have them here we have data so what can we do play let's try to build an MVP from that arm as the idea is so intuitively we just need to figure out what the transitions and rewind so how do you do the transitions so the transition says if I'm in state as an I take action a what what I don't know what happened but let's see in a day what so I can look at the number of times I went into a particular last prime and then divided over a number of times I attempted any this action from that state Undertaker race and for the reward this is actually fairly you're easy when I because when Observer reward from s a n s prime I just write it so that's the ok so the country is now Paul what is this look so remember now here is my graph I don't know what the the transition distribution or the rewards are so what suppose I get this trajectory what should I stay stay stay stay in them ok so first I I can write down the rewards of 4 here and then I can estimate the probability of your transition is a 3 out of 4 times I went back to in 1-hour 4 times I went to and so I'm going to ask metres is 3 ft I don't suppose I get a new data points so I have stay saying so what I do I can add to this counts is there everything is Karen cumulus O2 more time sorry one more time I went into an another time I want to end so this becomes for r620 Enterprise I see another time when I'm just going to answer I'm just going to increment this counter and now it's 3 out of 7 cyprian ok so For Reasons I'm not going to get into this process actually in your converges to the if you do this kind of familiar times you get pretty that we don't know the transition of work but we know the number of days and the rewards but yes you do know the son of steak and the act set upstairs I guess you don't have to know them Owen vans but you just observe the message directions you need to know because you are age ok so cost like there's a problem with variable rewards if the reward is not function of sa as prime you would just take the average of the rewards that you see ok so so what do you do with this so after you have to meet them upto you need is a transitions and rewards the now have ended in the exact right mdp because this is estimated from data to match exactly banana we have his toys from last time you can do valuation to compute the optimal policy on it and then you just know you're done I am practice you were probably can interleave the learning and the optimisation for simplicity we can think about is a two-stage where you got a bunch of data you asked me to ok there's one problem here anyone know what the problem might be with your best possible thing on Netflix for the brands of the world Kanye 100 right so so this is this this problem this kind of actually pretty big frog that unless you have a policy that actually goes and covers all the seats you just won't know this is a natural because you can always be you're a ladder reward hiding under the kind of one state but unless you see it you don't ok so this is a kind of key idea the challenge I would say in reinforcement learning explore so you need to be able to explore the state this is different from normal machine I mean where they just comes in passively and you don't function and then you're done you actually have to figure out how It Gets and that's that's kind of one of the the key challenge aura so we're gonna go back to this this problem and I'm not really going to try to stop right now I'm for now you can just think about PI as a random policy because of random policy eventually yo head everything 4 mil finite source more ok so so that's basically end of the first album write a story here algorithms we have model-based Monte Carlo in the morning this is referring to the fact our estimation model that in particular mdp the monocle part is just referring to the fact that word using samples to ask I'm basically applying a policy motor what and then that's amazing tomorrow based on averages ok so so now I'm going to present a different album enniscorthy Ahmad of free Monte Carlo you might from the name Jess what we might want to do is maybe we don't have to and why is that well what do we do with a smaller what we did was presumably used valuation Sue your computer optimal policy in the remember this recurrence for computing QA I'm it in terms of tea and reward but today are you need a sku what if I told you to artists a which is what is the qotsa it's the the maximum possible utility I could get if I am in Chestnut sa and I fall off the clear I knew that they were just producer open policy and I don't even need to know understand the revised ok so with that insight is model-free learn which is our just going to try to do as to make you up the Radcliffe sometimes it can be unloaded confusing what is meant by model 3 so cute off itself you can think about it as a model but in the context of mbps and reinforcement learning generally people when they say model 3 refers to the fact that there's no mdp model not that there is no model and Jenna ok so so we're not going get a QR I'll come later and make sure it's warm up so user data staring at us remember let's look out of related quantities so cute pie remember what skip hires 25st is the expected utility if you start at us any first take action eight and then follow policy pie so ill in I guess another way to write this is if you are at a particular of time step 2 you can define duty as the the discounted some of the rewards from that point which is the of the reward immediately that you a gas plus discounted in the next time Star Plus in square discounted in r-truth timesteps in the future answer and on what you can do is you can try the estimate Cube from the sooty right so this is a utility that you get out of protect your time step so suppose you do a fine so it suppose you average are you toys that you get only on a time steps where I was in a particular state innate in action ok so you have a little boy you have a bunch of episodes right so here pictorial what's here's another way to think about this episode I'm going to do some abstract I'm trying so every time you have USA shows up here or maybe he was a pure maybe it shows up EuroMillions Google look at how much reward do I get from that point on how much would I get from here how much reward do I get from here and average there is a carnival technicality which is that Sao pure and also pure after 8 then I'm not going to count that because I'm try if I do both a double invited works both ways by conceptually it's easier to think about just taking NSA the sim you don't can't go back to the ok so let's do that on a country example so cute pie let's just ride cupy sa is a thing where trying to ask me this is a value associated with every chance known as a some particular I've drawn it here I need a value here and I'll about you here ok so I suppose I get some data I stay in the night go to that so what are you today Star Trek 4 yes summer fair ok so now I can say ok that's my basket so far haven't seen anything else maybe at so what happens if I play the game again and I get 44 so what are you told here so then I did this to the average of 4 and 8 do it again I get 16 tonight average in the sea in again you're I'm using stay so I don't know anything about this in practice you would actually go Explore this in figure out how utilia c turn particular notice I'm not updating the rewards or the transition because I'm not afraid I just care about the cayuga use that again which are the values that sit at the no it's not on there ok so one caveat is that we are estimating cube 9RQ what will be visit us later in another stain toucannon note is the difference between what is called on policy off-policy so in rainforest you're always following some policy together and that's Jeremy call the expiration policy of a control policy and then there's usually some other thing that you trying to usually the the value of a particular policy and that cause it could be the same or could be so on policy means that we're estimating the value of the policy that were following the original RC means that word so turn particular is a model free Monte Carlo on policy off policy it's on policy because I'm estimating cupine art what's ontology and our policy what about model-based Monte Carlo tell me a tale of it all Simon weird question bad in model-based Monte Carlo where following samples you've made me a random policy but we are estimating the transition then rewards from that we can compute the optimal policy if you can you can think about it RC boat can clearly see ok any questions about 1 - 3 Monte Carlo so let me just actually right what is the small base model called is the estimator the transition and reward model 3 monthly columnist trying to estimate the 0 pi and Justin doesn't know I put hat on any letter that is supposed to be a quantity that estimated from data that's why I asked assistance to differentiate between whenever I Q pie that's the truth r value of the your policy which no I don't any questions about moral through Monte Carlo what are these are the albums are pretty simple right look at the data and take average not trying to off myself it's your fault so question is is model free changes to a party here is a text boxes so this version I've given you is only for a fixed party the general idea of model 3 as well see you later I you can also automize ok so so now we're going to do is Wigan do Seaman variations on a model 3 multicolour incase slightly different ways so help us generaliser square mile Alison Proms world model 3 is better than model days is actually interesting question right so you can show that if you're a model is correct if you're more of the word is correct model bases counterweight ago because it'll be more simple efficient Mini that you need fuel datapoint but it's really hard to get to mala correct in the real world so recently especially within your deep reinforcement learning people have gone a lot of mileage by just go in model 3 because jump your hello but you can know this is a kind of a deep neural network and I give you a strongly flexibility and power without having to solve the hard problem of me caf tracking down ok so so there's kind of three ways you can think about this so the first we already talked about it this average idea so we just looking at the utility is that you see whenever you Catherine sna you just after ok so here it is an equivalent formulation and the way it works is that for every SAU that you see it every time you see a particular essay USA USA so I'm going to perform the following update existing value and I'm going to do it or have a convex combination so no one has a Down playlist until 1 so it's your kind of balancing between two things housing between the ordinary they had and the the new utility that I song and the other is said to be one over 1 plus the number of so do you think you'll make this very clear what's what's going so suppose my data looks like this so I had 4 arm and then a wine in a so these are the utility is that a year I'm not ignoring the acidity I'm just assume that some some ok so first all assume that you buy is 0 ok so the first time I do on that's the number of updates I haven't done anything so which one 1 - 0 0 x 0 + 1 time 4 which is the first movie that comes in ok so this 4 ok so they're not what about the next data point that comes 1/2 now x 4 + 1/2 x 1 which is new that comes in and that is I'm going to write this 4 + 1/2 ok so now ok just to keep track of things this results in vs this results in and there now banaras Facebook so no third one I do two-thirds so I am 4 + 1/2 x 2/3 + I guess I should do two-thirds to be consistent two-thirds x 4 + 1/2 which is the previous value that city and QPR plaster 1/3 x 1 what is the new value and that gives me 4 + 1 + 1/3 you can see what's going on here is that you're each oh this time I have this your some over all the Jews have seen over the number of times and this set so that next time I kind of cancel out the account in the new car to dominate and I kind of all works out so that at every time step what actually is in Cuba just the plain average over all the numbers this is just kind of algebraic trick get this original formulation which is the notion of average into this formulation which is a notion 10 if you're trying to take a little bit of them sing an adult so I guess I'm going to call this I cast combination I guess so that's a second interpretation is a third interpretation here which arm you can think about it in terms of stochastic gradient descent so this is actually a simple algebraic manipulation so if you look at this expression what is this so you have 1 X Cube pi a report out and put it down here and I'm going to have mine as a x Q pie that's this and then I also have a dog are you I'm going to minus you here in this is are inside this yo do the out of you can see that these two equal so what's the point of this right so wherever you can have seen this before something like maybe not enough this exact expression but some any ideas yeah when you looked at as the cassegrain is sending a contest the square lights for the new request B A remember we had his Updater all look like and a prediction - harder which was no the residual and how was your second up so what would interpret this is this is literally trying to do stochastic gradient descent on objective which is a square arliss and I'm the the cube high value they use you're trying to and you which is the new piece of data to think about in regression this is the way is Ry the water output you this is a model that's trying to predict anyone so views on basically this idea incremental will become clear way in ok so now let's see an example of moral premodifier in Action on this the volcano games and remember here we have this Hello Kitty and arm I'm going to set the number of episodes see what happens so here ok so what is this kind of grill extraction of triangles so this remember is a state is this to come over so what I'm doing here is the boy into 4 pieces which corresponds to the four different action so this triangle is 2, one North this triangle is 2, 1 kissing a number here is the cube estimating along the way cancel that the policy I'm using is completely random move random and run this 1000 x and we see that average utility years -8 which is Amazon but this is an estimate of how well the random part so as advertised your random you would expect your phone to over and you can run this and sometimes you get different results pretty much stable ok any questions about this before we move on to different tomorrow base Monte Carlo estimated mbp13 Monte Carlo we're just asked me in a queue values of a particular policy for now ok so so what's revisit what are moral free medical history if you use a party pie ecostay for the dice game yeah you might get a bunch of different trajectories that come out these are possible episodes any episode you have a duty associate N1 model free Monte Carlo is doing is it's using these Utilities to update boards.ie Prezzo in particular like for example this you saying ok I'm in I mean the instate in I know taken action stay when you're will happen well in this case I got 60 just case I got to quite a bit of on average that actually does the right thing right so on definition unbiased estimator is a million times on average do just going to get the right value which is 12 the variance is used for that boy only Jewish if you times you can get 12 you might get some I'm so how can we have cataract all this this variant the key idea behind bootstrapping is is there we actually have you some more information so we have this cube high that were estimating along the way so so that she was saying ok will try to make you pie arm and then we're going to try to basically progress against the state that were seen can we actually use coupe herself to reduce the variance saws idea here is I'm going to look at other places where on your starting in and I take stay I got a 4 ok so I'm going to say I got a 4 but then after that point I'm actually just going to substitute 11 in is going to where I because normally I would just say ok what would what happens is Karen on average is going to be right but you're on any given case I'm idealized from your 24cm end the hockey result by using my current estimate which is it going to be right because if I were right I would be done but hopefully it's kind of some more and that will you're better than using the the kind of roe Valley you have question update at the end of each episode yeah so question is would you update the kernel after app store yes all these over there I haven't been explicit about it is a used in episode you update after that you get a new episode sometimes you would even update before you're down with that ok so let me show this this algorithm so this is new abadam it's called salsa does anyone know why it's called salsa oh yeah right so if you look spell star and that's literally the reason why it's called suicide so what is this album say so your inner state as you took action are you got a reward and then you ended up in state as prime and then you took another actually prime so for every kind of quintuple that you see you going to perform this up what is this update so this is the Comics combination remember that we saw from before where are you TiK ToK part of the old value and then you tried to murder in with the new Val what is the new Value here this is looking at just to immediate reward not a for you totally just a media reward which is for here and you're adding the discount which is one for now of your estimate and remember what is the estimate trying to do estimate is trying to be the expectation of rewards that you get in the future so is this where actually cupine asked if I had then this will actually just be strictly better because that would arm just reducing a very nice but you know of course this is not exactly right there is fire so it's 11 at 12 but hope you know this is not no biased so these will be the kind of the values are you updating rather than these kind of rather ok so just a curve compare them well any questions about what's arceaux the movie Oliver something to try to be helpful here so cutie pie estimate QI based on you starter is still cute what is space-time reward Plus I think so it should hi Hat this is not like valid expression but hopefully it's some symbols that were involved are the right man on so let's discuss on the differences so this whenever people say can bootstrapping in the context of reinforcement learning and what they mean is that instead of using you as a prediction target you using R + Q pie and this is kind of your pony up yourself from a bootstraps because you're trying to make you pipe your gnocchi pie using you pie 2 ok so USB someone pass inside your base on the estimate which is based on or your previous Camp experience which means that this is a biased mawsley medical biosphere articles has large very Sara has small barriers and one I guess a consequence of the way that are set up as a model for Monte Carlo you'll have to carry out the entire game is it complete damart mdp until you reach the turbo seat and then you can now you have your you 2 up where is Arthur win or any for bootstrapping algorithm you can just immediately update because all you need to do is you need to see others like a very local window have saorsa and you can sit up and I can happen kind of anywhere you don't have to wait until the to get the Valley ok so just a quick sanity check which of the following algorithms allows you to Aston rqr queso model-based Monte Carlo ARMA 3 Monte Carlo or salsa so I'll give you me the 10 seconds to under this how many Aviemore do my time ok let's the report think I didn't reset it from last year so this is last year's our Somali B Monte Carlo Elisa GQ because once you have them maybe you can get whatever you want morphle magic Hollow estimate Q Pi doesn't have to make you are and saucer automatic sku pie but doesn't alright so so that's so kind of a problem like I mean these algorithms fine for estimating the value of a policy but you really want autumn Apollo in fact these can be used to improve the policy as well because you can do something called policy improving which I didn't talk about once you have a queue values you can define a new policy based on The Cube but there's actually a more correct way to do this ok so security the kind of the way mental framework you should have so there's two guys cupine QR to an mdp we started policy valuation allowance you get valuation an hour doing reinforcement model free medical and social idea get Q pie and now we need I'm going to show you knew I will I'm cold q-learning that allows you to get through out so this gives you queue up and it's based on reward Plus Archer ok so this is going to be very similar to source it's only going to differ by essentially he gets the same difference between policy evaluation and value ok so is helpful to go back to the MVP recurrences so even though I can only apply when you know the MBP driving reinforcing L'Oreal with it's they can give you inspiration for the ok so remember ECU up what is secured to the queue after considering all possible successor as a probability immediately Warwick our future ok so the q-learning is it's actually really kind of clever idea and it's a crossover called sorry I guess but maybe don't want to call and what does a follower so this has a same form the convex combination of the eye and the right so what is the new value so if you look at cue or queue up is looking at difference accessories reward plus VAT what we're going to do is well we all have all we're not going to be able to some of our successes because when are reinforcement learning setting and we only saw one particular success so what's your shoes axis taranis accessories get the reward to or is a stand-in for the actual report is the setting for the reward that reward function and they have camera x and then VR I am going to replace it with our estimate of of what we are what is a vitamin of the off so what release VR 2qr maximizes pick you up YouView define VR to be the mass of all possible actions of qrs India then this is ours right so cute saying I'm not a chance no what is optimal utility I can get provided I took an action clearly the best thing to do if you're at a status just choose action that gives you the maximum rq crystal that's just your q-learning so let's put a side-by-side with salsa ok so sourcer these two are very similar remember updates against are + occupied now we're updating again star plus does maxova queue up and you can see that sort of requires knowing what action I'm going to take next Canada one step look ahead a prime minister president to here jewellery it doesn't matter what you talk because I'm good just going to take the one that maximise right so you can see why sorted it is estimated the value of policy because what a prime I shows up serious your function of a pulse I'm sure I'm kind of insulator from that because I'm just taking the maximum lol the same intuition is for evaluation a party or any question q-learning vs source security on policy off policy is our policy because I'm following whatever policy employee and I get to estimate the value of the policy which is probably not the one ok so let's look at the example here so here is sorcerer arrived for 1000 Innovations and like model-free Monte Carlo this I'm that the average sodium getting as - Tony in a particular values I'm getting all very negative because this is cute this is party and following which is the random if I replaces with q what happens so first noticed that the average utility is still - 19 because I actually haven't changed my Explorer I'm still doing random mcilwraith well I'm still doing random exploration bananas at the value the queue up values are all around 20 and is it because of them has to remember is just doing this is a slipper by 0 so hello how's you just to go down here and get you thank you I guess it's kinda interesting a q-learning I'm just blindly following the policy run you're asked the Clifton to love Aquino all the time your I'm going something I mean I'm learning how to behave after me even though I'm not behaviour and that's the kind of hallmarks of off ok so any questions about this for our times tomorrow face Monte Carlo opera Monte Carlo azimo are the Q value of the policy based on on the actual Returns are you get the actual some of the rewards sources bootstrapping that's the same thing but with another one step look at thank you learning is like so that except for an ester meaning optimal fix auto Oscar off I'm policy because I'm alright ok so now let's talk about Karina so these are the album so this point if I just hand you some data if I told you he was a fixed policy clear some data you can actually Bernal there's a question of exploration which we saw was really important because if you don't even CO2 says how can you possibly Akhtar so so which expiration policy should you use so here are they have to experience the first extreme is let's just set the expiration policy so so cute on you now do you have this queue up those that are true qrb had asthma make you up the right thing to do is just use accurate figure out which action is best in just always do that what happens when you do this is on you are don't do very well so why don't you do very well because initially while you explore randomly soon you'll find a 2 and once you found a two you say I well two is better than 000 so I'm just going to keep on going out at 2 which is newer all exploitation no accelerate do you realise you dad there's all this other stuff so in other and a direction we are no exploitation or exploration here you can't have the opposite set up I'm running to learning bright so as a result before I'm actually able to ask me other the queue up values sorry Lorna lol what's the average utility which is the actual motilium getting by playlist is pretty bad invigorates the utility you get from movie so don't know what you really want to do it your exploration S42 so just got over conversation or commentaries that I really feel reinforcement learning and kind of captures because in life there is no you know what going on you want to get rewards and you won't do while but at the same time time you have to learn about how the world work so that you can improve the party is everything about going to restaurants or finding the shortest path better way to get to the score to work or in research even when you're trying to figure out I'm a problem you can work on the thing that you know how to do and will definitely work or yeah would you try to do something new in hopes of you learning something but maybe it won't get you as high rewards hopefully rainforest morning is I know it's kind of a metaphor ok so back to concrete sorry series one where you can balance exploration export so it's called the Epsilon greedy policy this assumes that you're doing something like you learning so you queue up an idea that no with probability why are my steps on website music to give our export music do give you a TV you have and then once in awhile you do something round certainly not a bad policy to acting like someone's maybe she's do something wrong so if you do this why when you get ok so what I've done here is I've said absent to be starting with Y so one is all exploration and then I'm going to change the value a third of the way into a .5 and then I'm going to 2001 A change it to 0 so if I do this then I have to estimate the values really really well and also I get utility which is so pretty 32 and this is also kind of something that happens as you get older do tensira Explorer flask ok alright so that was exploration Sol put some stuff fortilla turn on the cinema ok so covering on now and so we talked about all your exploration Epsilon greedy other ways to do this 3DS just kind of the simplest thing that actually no rush from Michael you're well giving a serious system so the other problem now I'm going to talk about is your journal so remember when we say exploration well if you don't see a particular state then you don't know what to do it you think about for mom and dad kind of unreasonable because your life you never going to be an exact same your situation and yeah we are we need to be able to properly write so general for them is that a state space that you you might deal with in a can of real overall situation is in North and there's no way you're going to go to track down every possible stay cancel this space is actually not enormous but this is the biggest I can draw honest I'm anything see that the average play Bad ok so what can we do about this so I guess crocodile large date is it this is a problem so now this is where the second the third interpretation of monochromatic colour will come in handy so does take a look at q learning so in the context of a CD Volkswagen right so it's a camera gradient step we take it all deluen -8 and something that kind of looks like it could be a grey what is the residual so what did a notice under the kind of formulations of oculary nice hot about so far this is what we call a Roland which if you were your two weeks ago where they said is ridiculous because it's are not or generalising alright now it's basically for every single sitting action I have a value if I have a difference in action include different value I don't I don't there's no kind of sharing of information naturally if I do that I can turn lies between States and action ok so here's the key idea that will allow us to I'm actually overcome this so it's called function approximation in the context of reinforcement learning I'm in normal machine on its just called normal machine so the wire works is this silver going to define sqr sa is that going to be a lookup table it's going to depend on some frandars here define dysfunction tube RC sa ok so I'm going to fire a speaker Factor very similar to how we doing in the machine on his actions except for in-service day we had xx and now the weights are going to be cannabis ok so what kind of feature is now you have you might have for example features on your actions so these are indicated features hey hey maybe it's better to go east then to go west or maybe it's better to be in the 5th of row or is it going to be in a 6 things like that so I'm you have a smaller set of features and you try to use that kind of generalize across and you're all the different states so what is looks like is now with the features is actually the same as before except for now we have something that really looks like I know the machine orange light is that you take your way back to Anew do an update of the residual x the feature vector ok so how many of you this looks familiar when you regret alright so so just a contrast so it was a before we were just starting a QR to values the residual is exactly the and there's nothing over here and now we're doing is updating at the queue that is without in the weights the residuals the same in a film that connects the the the cube guys with the residual with the weight is the kind of feature is a Sony shake this has the same dimension this is a vector this is a scalar this is a vector which has the same dimension audio ok and if you wanted arrived this you can actually think about the employees objective add simply regression you have a model that's trying to predict a value from employer I'm at ASOS is like X and who it is why and then your target is like the why don't you trying to predict and you just try make this prediction close to that or now yeah so good question so what is this a door now is it the same as before so when we first started talking about these algorithms right it was supposed to be one over the number of updates and some arm but once you get into the ashleaf form my first then now this is just behave as a stop and you can turn it to your house alright so that's all say about these two challenges 1 is how do you do exploration you can use Epsilon greedy which allows you to come back exploration with exploitation on another second thing is that for large databases hungry isn't going to cut because you you're not going to see all the states even if you tried really and you need something like function approximation to tell you about new state so you finally have so summary so far online lorry wearing online selling this is the game of real person you have to learn and take actions in real world what are the key challenges is the exploration exploitation resource for algorithms Ducati key ideas here when is model Carla which is that from data alone you can basically use averages to estimate quantities that you care about for example transitions rewards and values n a s t ideas this bootstrapping which shows up in sauce and jeweller in which is your update pause a target that depends on your estimate of why are you trying to predict that's just the kind of writing ok so now I'm going to step back a little bit and talked about reversal learning in the context of some another so there's two things that happen when we went from binary classification which was your two weeks ago to reinforcement is worth of decoupling what is state in ynys feedback so the idea about partial feedback is that you can only learn about actions you take Addison Rae if you do it don't quit in the sky you never know match Mario yoghurt and the other ideas the notion of state which is that you reward the pen on your previous axe if you're going for a volcano you have to there's a kind of different situation depending on where you are in the map and there's actually kind so so this is kind of you can draw 2 by 2 grid where are you go from supervisor learning which is stainless and full fibre there's no said Every Generation you just get a new example arm that doesn't have yeah there is no dependency in the in terms of prediction on the previous examples arm and for feedback in because it symbolises Lauren you're told which is the correct way you even know what housing labels4u just told which ones are crackly and now in real for both of those are made harder there's two other interesting points or what is committee armband is caravan you can think of one off to reinforcement learning worry there's part of feedback but there's no which Mercedes and there's also you can get 40 back but they stayed so in structure prediction for Subway machine translation your toll what do translation output should be clearly do our actions depend on previous actions because you can't translate words and hassle ok so one of the things I was just mentioned very briefly as you know this different person has you're very popular in recent years a lot of interest in the 90s where water the algorithms were kind of in theory and then there is a fair wear and tear not much happy and since I guess 2013 there's been a revival of reinforcement research I guess at deepmind where they publish paper showing how they can do who are use reinforcement learning to play Atari so this will be talked about more in a section of this Friday but the basic idea of different person learning just to confirm mystify things is that you using your own fireworks 4QR thanks that's what and there's also a lot of tricks to make this kind of work which are necessary when you're dealing with enormous so one of the things that's different lot different first morning people are much more ambitious about handling problems where the safe spaces are can be now for this the status that just the pixels right so there is in your fuse number Pixar hair and wears before people were kind of what is known as a tablet case there's are the number of states you can have enumerate there's a lot of kind of details here as care one general common is had room for 7 years tattoos is really right because of the state fullness and also the delayed feedbacks just are when you have maybe thinking all final projects I mean to require rear but no don't underestimate how much working some other things I won't have a time to talk about sofa with talk about method sell a trailer estimator the Q function there's also way to human do without the key functions and just tried to estimate the policy directly that's called methods of policy gradient there's also methods like actor-critic that try to combine Avis value business and policy business these are used undermined alphago and offers 0 programs for crashing humans this actually work 52/3 to next week's xx because this is a in a context of just about about the applications on your you can fly helicopters playback an intersection early examples of early samples in the 90s on my success stories of using reinforcement the Old soft play alarm for 9 try for something can be used to your scheduling and managing data centre ncy ok so that includes a section on Markov decision processes which we are we are playing against nature so nature is random but the O'Carroll neutral next time where can I play against an opponent where they are out to get us so what's you about that 