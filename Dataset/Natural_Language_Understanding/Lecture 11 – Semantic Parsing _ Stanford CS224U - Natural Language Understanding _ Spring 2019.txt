the topic of semantic parsing and this is my favourite topic of the class and it's one of my favourite topics in NLP altogether when I was a Google my team was responsible for doing semantic query parsing for Google Search and for the Google assistant ibis many person techniques that work in a look at today to billions of queries every day and in 40 major languages I'm at apple and we use very similar techniques for query understanding for Siri nothing is highly strategic for Google and apple and Amazon in Microsoft every hot Topic in academic research over the last 5 or 10 years reporting is a big topic is a complex topic I think more than any other topics that we've covered it draws on Concepts from linguistics and logic so I could possibly do the topic justice in the next hour that we have if this topic catches your interest enter material on the website that can help you go deeper with a series of four code books that introduce a simple semantic parsing system called sippy.cup Chris has made a few screencast that explain the main ideas in very simple and approachable terms terrific paper that Chris co-authored with Percy we young in 2015 conveys the main ideas of semantic parsing very clearly and very concisely today's lecture I think the best that I can do is first to give you some high-level motivation for why this is a problem about why this is an interesting and impactful problem and S to to describe the standard approach to semantic parsing in in very high level terms bye talk a little bit about the motivation for semantic parsing of course you might be asking yourself to be doing natural language understanding but I still don't know how to build C-3PO how to build a robot that can really understand look at so far in vector space model the meaning sentiment analysis relation extraction aspects of meaning or Fragments of mean a lot better missing we still don't know how to generate complete precise representations of the meanings of full sentences play so many aspects of meaning that we don't know how to capture so for example things like hire early relations with multiple participants or tempera last are Barack and Michelle Obama got married in 1992 in Chicago logical relationships as in no-one me into the building except policeman or firefighters cable try to imagine building a natural language understanding system that could enter logic puzzles like this one it was actually the very first project that I worked on when I was a young eager PhD student became before became disillusioned is a type of logic puzzle that appears in the Elsa exam and used to appear on the exam as well typical example in this is basically a constraint satisfaction problem as csp 6 sculptures which you need to assign to three rooms well respecting some constraints that are given there think about this type of puzzle is free humans shop for computers difficult is completely different for humans and for computers for humans understanding the language is easy stacked the authors ETS has taken great pains to ensure that the language is clear and unambiguous they really want to avoid any misunderstandings well like the human reader just didn't get it today work hard very clear and unambiguous change the languages easy but solving csps doing the logic part of it is hard and that's what this is supposed to be test the other way round for a computer solving csps is easy intact it's trivial understanding the language is hard the model that would automatically translate this text into formulas of formal logic can do the rest trivial for a theorem translation translating this into former logic is far more difficult than it might at first appear I want to take a close look at just a few specific challenges that might arise build a system that can translate these words into formula for first order logic is there many words have somatic idiosyncrasies example the settings sculptures D&G must be exhibited in the same room in the formal logic you probably going to get something like glass over some details here are like I didn't worry about existentially quantifying variables X and Y to convey the basic idea is the annexe turn Exeter room if he is in room living room y then X and Y must be equal to each other seems straightforward but the point want to make is adjective same kind of adjective into make that clear lemon compared to some other algae sculptures D&G must be exhibited in the red room logical form for that would look like sing like stop the same way if there's an accident using why then is red and why is Red by amusing shortlist look like like syntax here standard first-order logic syntax but hopefully it's quibble with my logical form here because properly account for the semantics of the terminal which is a very rich topic in itself but I think that's a issue that we can safely put to the side it's not really the main point Red Room bedroom maybe will be better I'm just searching something about prn Deezer and whatever run it's gotta be a red as well look at other adjectives like if I change it to large room play Run have the same it's going to be the same mapping into a logical phone is everybody by that these logical forms with more left right issues like the definition of tremor large and smelly are interceptive adjectives they have semantics which are interceptive is a funny kind of adjective it's called an NFL rookie additive that refers to the idea that the semantics of sane fur back to something earlier in the since the same room is not a specific removing a run with the specific quality is just whatever run you successfully built machinery kinetic interpretation of this sentence in this sentence in this sentence they can successfully handle or nearly adjective Gary is going to break down when it gets to waxing tickly looks identical but semantically behaves very different there are other examples to if you're an eagle young PhD student you might think oh ok ran into a problem no worries I'll just fix it I'll just make my model know about saying problems like that there's other airport adjectives like different your properties and you might say ok why just had another epicycle to my model when I can hear that one as well encounter more and more and more Corky semitic phenomena that are not airport adjectives that have other accounts of idiosyncrasies that you haven't accounted for in in advance different kind of challenge the challenge of scope ambiguity and to introduce this AJ but it is a good example of scorpion start woman gives birth every 15 minutes didn't know that but all right then he continue find that woman and stop her joke hinges on a semantic ambiguity when you read the first part you like ok every 15 minutes woman who gives birth the second part that percent of different reading instead there is a specific woman who gives birth every 15 minutes ambiguity the question of whether the universal quantifier every word the existential quantifier takes wider scope language nerd examples of is that a perfectly normal thing to do another nice one team structure there is a time and place for everything has the the the existential quantifier in the universal coronavirus you know the state of Reading sometime in place reading is is 1 specific time in place which is the time for everything and a joke continue old college turn my resonate with you guys because you're ready University this is this is the idea of scope ambiguity has it relate to the Elsa puzzle that I just showed Elsa puzzle with this no more than 3 sculptures may be exhibited in any room interesting because 1password reading here even two possible readings at least three possible readings you rented them as first order logic different logical form in a logical consequences are possible and from a computer's perspective in advance which one is the correct Reading most obvious one the one that the authors intended you to get play Run The contains more than three sculptures Pascal readings here another way of reading this in any room it would have a different logical form that has different logical consequence cancel that alarm set an alarm at 3 who sings let's all of the ones I was thinking about you touching on something about whether we're talking about a single point in time overtime whether the assignment of sculptures trims could change over time turn off the no more than first captured yeah you could you could be no more no more than three structures may be exhibited it doesn't matter what river exhibited reading of this sentence is maybe a little bit less obvious and little bit less accessible than the first one I think they're ways of contextualising that make that reading plausible there are because next one a possible world something as possible configuration it's consistent with the first letters Disney's room into sculptures in that room assistant with the second reading which says free maybe maybe exhibited logical consequences depending which one you been third reading which I think it's even harder play I claim is still a plausible reading of the first at most three sculptures have the property that they need may be exhibited in any room is there restrictions on allowable 4 seconds now no more than 3 sculptures no more than 3 sculptures read in any room restrict reading that the most obvious reading but it's a plausible reading and active of a computer algorithm like intuitions about which one of these things is more plausible or less Plaza when you're building assistant to map the sentences into logical forms your model probably needs to account for all of those possibilities because in different contexts any one of them might be the correct interpretation so hope that your model will be able to make good predictions about which one is more likely and less likely a little bit later we'll look at ways of tomorrow which will help you who is among multiple possible parses multiple interpretations are given in anything more about the Elsa puzzle but I could go on there are lots more challenges I'll just mention one one of the of the top might in the problem itself when we go back to the problem in the problem can only be in one place at one time is a sculpture can't be in two rooms at the same time Nazi centre first-order logic in a give to a few improver the film that will happily put one sculpture into different rooms sense knowledge that are being to different places at the same time a human just feels that and automatically without even thinking about no they do that so you'll have to if you want to make this work you have to also supply all kinds of background knowledge which is kind of understated in this problem as I mentioned at the beginning my PhD and we worked on it for about like 3 months and the further we got into it the more we realised can I solve this problem that was like 15 years ago and as far as I know this problem remains and soap today this is a really hard chat at first glance like it should be achievable an example this is taken from a Consulting Project let's go is it going to Whiston now you can give you something about my come from play the 90s music stop this is taken from our Consulting project that I worked on when I was in grade school and I was working with a start-up that one it to build a natural language interface to a travel reservation system to understand and respond to natural language descriptions of travel needs like this year's you'll be able to an email to the service and they would automatically figure out and travel plans free challenges to somatic interpretation that you see here we're going to be here could you send the evening all those real like banking Oakland but it doesn't actually make it clear whether Oakland is an alternative to sf0 Baston can you know that it's an alternative to sf0 to book a flight from SFO to Oakland but an automated system of course world knowledge which might not be available to an automated this heart see you later on first back to the husband a bit different return flights Google you have to resolve them and Africa and figure out with the pronoun refer his back other weather kinds of reference resolution here in the evening or Monday morning you need to figure out that that refers to the Sunday that presumably that's refers to the Sunday that follows not the Sunday that proceeds that Friday right so I guess we're talking about cheap or Monday the 15th beard hair which is that a Friday is the 12 play let's medically that doesn't work the human that the Friday and Wednesday or Friday the part of the 12th and 18th of 6 days apart the human made a mistake it's a deal with it somehow like ring will be for the system to say did you mean Wednesday the 17th engineer that into the system it's not gonna be able to do it what other challenges do to her extraneous information like turn the bedroom light off fan United about why what matters and what doesn't matter idiom give you a flavour of somebody kind of things that you run into when you try to do imprecise semantic interpretation on real-world problem complete and precise natural I'm gonna singer goes way back to the beginning last time Chris mention the shirt blue system almost 50 years ago by Terry winograd when he was grads doing a PhD professor here and he doesn't work on natural language understanding anymore but this was arguably the birthplace of natural language understanding blocks it and use it and it's grounded in a black swirl it passes the user's input that's it to logical form and try to interpret that logical 4 minutes world and answer a question or taking appropriate LinkedIn YouTube video here and it's worth checking out later that the slide by the way link from the website and from there you can find your way to this YouTube video and have fun video to watch say you don't quite complicated things like kinda black which is taller than the one you are holding and put it into the box understand it just does the right thing and at the time people were totally wowed by the even half a century later this kind of elicit-r wow but certainly at that time it really have people thinking that human-level and I'll you was just around the corner excessive optimism trembled when later systems systems try to deal with more realistic situations and with real-world ambiguity and complexity part of the reason this was achievable was it was this very constrained blacks world that training with him Parcelforce minute parsing was the chatty system this was developed in 1980 by Fernando Pereira who's now a research director at Google play natural language interface to a database of geographical facts so you could it could enter geographical areas friends or how many countries are in Europe or things like that implemented in Prologue and it was user hand built the grammar Lexicon so there was no machine learning insight it was just lots and lots of Rules this semantic interpretation astonishing it is still possible to run chatty this code is older than most of you you can still run it on a stand for dry ice machine try it yourself here's a recipe started and it's actually really fun kick the tires in tried and see what works and what doesn't misuse of actually taking time in class to do this live in class which is kind of fun it's a little bit cumbersome and I feel like maybe the educational value of it isn't worth the cost can you do that this time I wish you some examples of some queries that that it can handle so you can ask it like is there more than one country in each continent let's see how good you guys are geography Australia is this system does think that Australia is a continent it thinks that australasia is a continent it says it does say no but the reason it says no it's because of the Antarctica and countries border Denmark set remember West Germany California 27 of the time machine back to the play cold war Andover skip the next two but this one is really impressive which country bordering the Mediterranean what Mediterranean port is a country that is bordered by the country's population exceeds the population of India he knows how to get this question right even though it's incredibly syntactically complex this one I think about that play music I'll let you go try chatty on the race machines you can ask play 10 things that today when it shows like when I think that I get out of it is trying out small variation so if in 13 what countries border Denmark if I said 1 countries what countries are joined Denmark play just says I don't understand you change one word I don't understand because that other word of that other formulation another way of asking the question just wasn't in the handle Kramer the last example car falls in the same bucket how far is London from Paris if you like a question that has very similar flavour or the other questions just another geographical question no matter how you ask this question just says I don't understand about distances between the news about like areas in Italy how many square miles is the total area of country south of the Equator and not in australasia tell you about distances between limitations what is it only knows about certain things that certainly restricted to geography even within geography it only knows about certain kinds of Geographic facts Amazon hitting the allowable freezing of questions even questions that it knows the answer to Express your question in a certain specific way beautiful ways that I can head over there are lots of other ways to ask just can't handle systems like sure blue and chatty or pretty astonishing because for most of you guys were even born which demonstrated precise and complete understanding of even quite complex sentences well running on hardware that's powerful than you know my rice cooker you have a really dirty rice cooker it's what they were able to do Bridge with extremely narrow it was limited to one specific domain and even within that domain they were varied riddle if you ask a question this way you'll get an answer if you use just slightly different words by contrast we have systems that exhibit vastly greater robustness and with very broad coverage play fuzzy and partial understand Google about just about anything never says I don't understand it might not exactly ask your might not exactly answer your question if you ask Google this question about boring Mediterranean border country.in motorbike country is population increasing population of India can you tell me the answer probably will do or is it a link you to my slides from last year which maybe had the but it doesn't have that complete precise understanding that's needed he is ok with C-3PO and the Pot of Gold we want to build systems that can understand precisely and completely show me Stratfield super inspired by examples like that that one what's the population of the largest that one what's beyond ability to enter these highway structure database type of queries has lots of application spelling if your policy analyst working on global warming you want to be able to answer questions Like These easy to imagine that you have a database that contains all kind of statistics about carbon emissions you probably don't know how to write SQL queries to pull out the information that you need so it will be great to have a system where you could just type in questions like if you grew up in America there's a good chance that you have an uncle Barney who is a total freak about baseball trivia would love to be able to ask questions Like These he doesn't know how to write database query invent nursing system make a website and sell subscriptions in October he will pay £10 a month application to head become very important at Google and a apple and an Amazon voice commands for getting things done these queries are not well served by conventional keyboard if you ask Google how do I get to the ferry building by bike play by showing web pages containing those terms play disappointment you don't like web pages containing those times you want a map with a blue line on it and only we can draw that nap is if we know where you are and we computer around from where you are to the third Building and doing that requires understanding your intent in a complete and precise way it's like these requires nothing into structured machine-readable representations and meaning that we can pass to a downstream can't to take action on it requires semantic parsing parsing is to do precise complete interpretation of linguistic input into map doesn't Richard machine-readable representations of meaning open question is what kind of semantic representation what are target output a lot of different possibilities kind of depending on what the problem is and what the domain is the goal is the facility data exploration and analysis the right to medical presentation might be a database query language like sequel robot control application you might want a custom-designed procedural voice commands you can ask them get away with a relatively simple meaning representation which is based around a fixed set of high-level intent prize bispecific Oregon full if the query is directions to SF by train might be first indicate that this is a travel query that indicating the intent for the type of intent which apprentice what is a destination for Emmet free base ID which is the free base ID for San Francisco the second parameter describes the transportation mode you is I guess any number value there's maybe like 5 different transportation modes and trans this little machine-readable everything that you need to pass to a back-end system which can actually like directions in drama give it to the relief for other kinds of other County Kerry to illustrate the central ideas of some embarrassing we've created a simple semitic parcel called sippy Cup Lewis designed for simplicity and readability it uses methods that are commonly used at Google and apple for semantic interpretation of user queries new series of four code books that introduce sippy Cup unit 0 is a high-level overview of semantic parser meaning units demonstrate the application of sippy cup to three different problem domain Focuses on the domain of natural image arithmetic this is curious skip + 4 this words rather than mathematical symbols Unit 2 Focuses on the domain of travel query so this is quite like driving directions to Williamsburg Virginia play obvious applications for assistant products like Google assistant in Alexa and Siri spinal unit 34 cases on geographical query so things like how many states border the largest a and it has a very similar flavour to the kinds of queries that chat unit 3 caterpillar Straits a modern machine learning-based approach to the same problem that the charity approach that the sippy Cup illustrates was pioneered first by Luke's Armagh and his group at UDO play Purcell young and his group here at Stanford elements of this approach text Free Grammar which defines the possible syntactic structures somatic switch enabled bottom-up semantic interpretation linear scoring model for which is learnt from training data freezing taters for recognising names of people locations dates times and so on finally grammar induction inducing ramus from training data in order to quickly scale into new domains and new languages the Fireflies on I'll talk about each of these five elements the grammar the grammar is the core of this meant person system and it has two parts it has a certificate of syntactic and semantic part tactic Park is a fairly conventional context-free grammar terminals like Google and my Amy and and then we have none terminals which we indicate with a dollar sign so a dollar of luck is a non-thermal and we also use we have a designated start symbol which new work by conventional cooling delarue derivation stop with dollar route singer notational convenience this question mark indicate what's the notational convenience I'm using this I could instead I have to Rules here download this staff without the parentheses in the question mark which has diarrhoea stuff except that saying this could be here or could not be here it's particular grammar fragment could be used to Paris just a handful of course it could be used to parse lights New York possible thing that can match the screen match spell location Google in New York bike a handful of other queries there's not very many queries which match this very limited slaver what kinds of things 4 g of natural language these cameras are usually non-deterministic so if you've looked cfgs in the context of it is like The Kooks es143 there you have seen deterministic premises where there's only one possible first one possible interpretation for any given input typically you would non-deterministic Grandma's where there are multiple possible parcel that's important because natural language utterances are very often usually ambiguous label of a parcel for the input need to Google in New York by car recognise me as an optional word we recognise Google as a fellow local location recognise New York we then recognise Lookin outlook we recognise bike as a dynamo destination and mode who's that last role to put the whole thing together route the final product tactic parts one possible synthetic parts is a synthetic part of the grammar given a grammar and an input query all possible courses of the Queen progressing into Louise in adaptation of the well-known cyk chart parsing syntactic parsing before you may have seen you may have seen this us healthworks who is we rewrite the grammar what are the rules in the grandma are binary Harry means daily have two things on the right hand side what we don't want to have is three or four five things on the right-hand side we won't have the most two things who is reconsider every span of tokens of the whole query and we do that bottom play up from the smallest pencil expensive one enlarger stands for every spin we consider alway always of splitting expand into two parts we consider every grammar rule inside can match those two parts 14 to make the grammar binary so that we only have to consider splitting things into things into 3 or 4 or 5 have a grammar rule that matches the competition match that right hand side is that we have a way of making the category which is on the left-hand side of the rule record that possible can a parcel parts for that Spain helpful as we work our way up to larger and larger Spain because it can help us build bigger things about that scattergories in trying to interpret large syntax what about the somatic part of the grammar in a CFG can come with known as a somatic attached Smith attachment here in the square brackets in green please see attachment as we'll programs that are run when the parser applies the corresponding syntax rule outputs are Fragments of our meaning representation basically if the attachments basically specify how to construct the schematics for the thing on the left hand side somatics for the things on the right hand side this one is very straightforward the semantics for Google is just going to be a free base ID means Google is The Entity New York pile of indelible who says the way you can stop the semantics for Vista milk semantics for this and this song what's the matter the first one dollar one and the second one dollar to and this makes perfect and stuff it into one of these s expressions with an in other meaning representation in this thing tells us how to build pieces into larger and larger for the semantics of Aberdour Moses just want to be the number the last thing that interesting is that this one tells us how to build a cement request actions request and get that comes from the fact it's a rat destination what are the somatic siz for this non-terminal is going to be whatever the semantics is for this non-turbo romantic attachment exactly how to construct representation are pieces building up to larger and larger play double part again the percent of added in green disamatic yield associated with each node in the past linguist this is basically Montague cymatics it bottom-up syntax driven semantic construct contractors metrics for Google and for New York as free base ideas A4 bike as an enum value can you combine these two to get the somatics for the compound location of there finally we combine everything to get the semantics for the get directions request pause there because that was a lot to absorb so far the next question is how do we recognise names and dates in numbers and things like that could anagrammer with Google any York here I have directly in my grammar associate the string Google what's the matter JD do that for long long list of entities and I could also do it for where is and things like that if I do that it's going to mean adding lots and lots like I've rules to my grammar which is going to be really messy and cumbersome and brittle and difficult to maintain big coverage anyways I mean I can't possibly put every possible phone number in my grammar who is leverage special-purpose senators phrases that describe entities locations names numbers dates times and things like that example query is reserved Gary danko with Tom next Friday imagine that we have three different annotators bus this query and it basically functions like black boxes on first before their synthetic machinery gets to work change the results to the syntactic Machine when did I have a free bass imitator in this thing it's job is to look for entity mentions set free base no Isabelle and it recognises this phrase the string Gary danko on it so I know that this thing tell me this is a dollar figured out from it is also able to generate lots of medication because it has all of free base and it's disposal already knows about 3 this thing I can figure out the restaurant in Nando's to generate category generate this helpful metadata that can be passed so for example has a confidence score how how should is that it made the right interpretation classroom end 2 scoring interpretation later imagine that we have a contact the editor which recognises this the string Tom it is ok I know who that is user ID and he has this email back in work is it a contact annotator who issued the request request is from a specific user and that user has a friend named Tom and this is his user ID so it needs access to some personal information ground the semantics with a specific user ID DNA data in recognise Industries next Friday and it says oh I don't have a generator matrix for that next Friday means May 10th 2019 able to do that because it knows when the query was issued and therefore is able to interpret next Friday appropriately so next Friday is an indexical expression but to ground it correctly I need to know something about where the when is Aquarius editors that can run situated blackbox modules run over the input query fantasies about how to interpret small span what does hypothesis as in machinery will will get rolling afterwards a preventative problem yes yes the problem I have to affect the Education itself so you could save location goes Tuesday and then look up on step reason why we should what you're suggesting is that the person could happen to top down instead of bottom up boundary actually that's definitely possible and like when you for example if you've taken cs140 three different approaches to passing and some of them are bottom-up and top-down and all of them are conceptually possible big differences as far as the efficient particularly for natural language which is so highly ambiguous down passing Transat expensive parsing is fishing because you can limit unlikely possibility when I was working in this problem Google the G that were working with it was Commonplace for even very pedestrian input released thousands of possible passes in the next point I'm going to charge you with a dually and when there are thousands of possible purses need to do pruning fairly early in the in the process we would use in search seen as Asda the processor person is happy a finite length list of the most promising possibility Crewe that was the only way to to keep the search for a Glasgow park problem of ambiguity is this is a problem that possessive language in certainly big Challenge for semantic parsing end can include instamatic a the input is mission bicycle directions to imagine the user issues this query to Syria to the Google assist interpretation for that is that we want to get to the mission mission bicycle I want to I want to ride my bike to the mission that certainly impossible where is a bike shop called mission bicycle which application is change to mission bicycle specifying transportation when I just want directions driving directions but where I'm going to go with Mission by sea this two different passes for this query in this one mission is a location bicycle is a mode mission bicycle is the location in a not specifying a mode in this case in Essex only two possible person a moment ago in complex domains it's with Rich grammars it common music 100 in thousands of possible purses grammar support multiple interpretations which one to choose with a scoring function we need a way to score the different candidates which one is the most Plaza you doing that is with a log linear model to score alternative derivation little bit of terminology here I'm going to call the input that is the the query the natural language query x derivation or purse syntactic parse tree with all of its semantic attachment lyrical meaning is why to designate the semantic field so that means final semantics it's basically the semantics that you get at the root node in the past he completely determined why any given x you may have lots of Kennedy disease and correspondingly lots of candy wise how to build a scoring function what we're going to do is first feature representation the key characteristics of the input x Paris somatics why play room for variation in the future representation but just to give you a flavour of like some common commonly used features presentation model include an indicator function just in case contains the word to the Kennet semantics contains a destination parameter like in totally makes sense those two things are likely to go together close together weak evidence is hardly proof but my can is weak ever play a good purse or you might have features which Captur again indicator features 2 billion features which capture the occurrence CFG rules for specific categories the dollar the dollar thing in the candy parts nutrition here is rules in your grammar might be much more likely or unlikely than others participe in good passes in Vaillant Paris anticipate in a vent but if you have features Like These opportunity to learn from data you can learn from your data that some rules work much better than other include is features which pass through the confidence score that you got from an editor presumably if the Editors really confident that makes it more plausible that includes that annotation is a good parts variation in the feature representation but once you have a future representation for a division score for Paris just the product of the feature vector and weight vector data from training data what score you can turn the score into a probability in the usual way by using the socket Max question where do we go where do we get the weight vector data this is basically the parameters of our scoring model we're gonna get it from that we're going to estimate those model parameters from training data and we can do it using a variant of the em algorithm using the em algorithm is that our training data inputs x navigate to st. doesn't include the correct pastriez Z there could be multiple parts trees that yield the same cement does Zizzi's as latent variables and that's what the em algorithm play me in application to the specific application alternate between Easter and M step Jobcentre expectation-maximization in Easter what we do here is use the current model parameters data parse the input in r training data in for each input x in best way possible person using our current model in step we're going to change the way we change the weights to put more probability mass elements of the embassy regenerated the correct semantic in the M&S lifts have good sex which match the target some of them to the probability mass towards the ones that match the targets and then we go back to the Easter Uri parts everything using I've updated weights that could cause the endless to shift around we keep doing that back and forth between East Evans and the M60 updates to the models are basically SGD updates this is kind of like us to catch the gradient descent adjust gradually over time 4 ways to learn more and more successful generating a target parsers for a training day grammar syntax rules it's got Smith attachment the scoring function with learner scoring function from data question is where the grammar Rules come from really depends on what domain you working in if it's a small simple domain with a few dozen or a few hundred rules are coming in that's good that's kale practical to write raemers manual you just like rain in Wicca past example quiz regenerate set will capture the the the the the properly not the common situation situation is in a large complex domain I need thousands of Rules tomorrow the domain well well it's just not feasible to write down all the rules menu we want to learn the rules automatically from from training day switch out of grammar induction and it's worth of the most interesting research work wise One strategy for Graham reduction which recent plastic illustrated in unit 1 of the City Cup Chromebooks the idea is simply to generate all possible rules send to your grammar weeks for each role to learn which roles are most likely to contribute to a successful Park what is what is all possible rosemeen basically mean all ways of combining the symbols of your somatic representation your sort of somatic when your language of normal cement combining the symbols every possible rain outside every possible webcam inside the cross add all those rules for grammar then we can generate a huge variety of different purposes is the weight learning that I talked about on the previous life which ones are good in which ones are bad but heavy cost because generating all possible rules leads to an exponential blower number of possible passes adequately in the time that's required for person work your way through City captain and wine you'll see this illustrated very vividly more sophisticated approaches to grammar induction look for ways to be far more selective about introducing your grammar rule ways to prune my aggressively contributing to successful Paris to keep the size of the Grimm and manageable Lincoln still run in a feasible manner two different ways of using training data in this process is to induce rules of the grammar to figure out which synthetic Productions should be part of the grammar play I've using data is to estimate the parameters of our linear scoring model please 22-28 of using data are both really important in Canada work hand in this underscores the importance of days out for making this whole thing work and catering auction and you can't learn the primary server score model without training day what's the little bit of data to really make this work you want massive amounts of data kitchens that are doing it still like Google and apple invest a lot of money in data annotation proprietor crowdsourcing platforms similar to mechanical Turk inhuman entertainers to look at example in book where is in write down that the target somatics for the square machine learning systems 222 expensive labelling examples with target target semantics is slow and laborious and Castle lot of money reproductive Direction has been to enable learning from indirect supervision play Pioneer by Percy Liang is the idea of learning from denotation denotation basically means if your semantic representation something that can be evaluated in some way execution or evaluation something that's all and one and two EN8 8uh to produce call here I should have put an example on the side zip on my way what's the capital of largest state please for that might be some kind of complex tell me a letter that literally says except in logical language instead of English capital of the largest stay occasion event semantics would be the answer to the question like yes talking about large in terms of area what is the capital of the largest juneau Alaska so that's the dino toy play consists of imports by what is the capital of the largest a hair with logical forms very effectively from there but it's hard to produce that training data because it's hard to get human annotators to produce those logical forms will an Apple we've figured out all kinds of tricks to make that a little bit easier for Ordinary People to produce logical forms but it's intrinsically hard easier play radio antena taters to produce like Junior training day that's what's the capital of Georgia state if we can figure out a way to learn from that kind of data how to get lots more data and of learning from learning from denotations powerful idea and it's illustrated again in unit 1 of sippy Cup domain is natural language arithmetic so they're the donations are the answer to a simple arithmetical comp go to recap the ingredients of this approach to semantic parsing rtsg with so many attachments touring models lyrics Direction lots and lots of trains I hope that's enough of a high-level overview traffic interests you you'll be able to dive into the city cupcake box stop meeting some papers how much for concrete sense of how is surface together and how old for today 