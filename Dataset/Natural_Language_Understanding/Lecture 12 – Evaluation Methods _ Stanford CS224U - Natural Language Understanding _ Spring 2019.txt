what's the weather going to be on to pick up a report set alarm for this Baker We Are to do where level natural language inference with binary classification play Wannabe predicting on the wedding Tom and even two words he had a disjoint trainer train test which reflects their expectation that we want to be able to generalise on unseen vertigo recent data set consisted of 17 67 labels and orange A46 positive labour prearm start evolution m w m F1 score read their microwave one or they waited at 1 but we only look dead to you on the micro turn the heating up to 1 m is arable matric because this histogram of the submission scores the Bassline is right around 0.7 are people able to suppress the WSM results for Top Model Sandy what are the top submissions so on is a can see the club in bedding was being from the Bible names for chestnuts that answer are we played around with your own network framework you don't have to do all out with the design but they had the arm the variable main customers primadollar Amazon printer euro lotto numbers which probably means that people haven't picked hyperparameters the variable name word is 20 experiments of Colombo in the first place goes to 26 4 of 0.75 what day did won't be used for combination of birth sequence classification model we always there turning on Tuesday pretrained Bert from end-to-end sequence classification model and then they trained on our training they said instead the day that they use over simply they randomly sample with replacement on the cannibals to balance out the on the label District please go to goodnight play song 0.75 4 was the EU Preston model retrain in Britain model on the sly that don't pretend wheat and then the earlier TN12 anarchist let him go as problem they use we did week 21 and B point B more emphasis to class one then the booty for submissions that top first in the top second on The use transfer learning the model song the purple underwear are some things that you really quite placation to combine does Tesco sell on networks regressions recipe neural classifiers Swift dental will be closed down this handcraft streaming I guess if you're coming to bed that's a first the same team that won this time one big off to as far as I know that unprecedented that the same group 12 Bake Off remember that 1 separate teams last year finished in the top 2 or 3 and 2 of the are the same team won and I believe that I won with probably the same approach which is birth with some task-specific fine-tuning other tricks that are specific to the dataset that's pretty cool can be talking about bird and Elmo in the transformer and that might give us some insight into why those techniques are working so well say it's just I mentioned it last week call now that a bunch of you have done really serious work on four separate NW10 custom models for them some of the things that you did are incredibly ambitious and I just think that's cool in terms of you have report an hour experiences to draw on when you go forth and do your own Projects so congratulations to you all this week is to try to do things that will be relevant to your projects whatever they are it's kind of awkward to be introducing new topic on material now when we know that you are probably focused on whenever special thing that you're doing for your own projects nice moment in which to start talking openly about methods and metrics and things like that because the next major assignment that you have to do is this new document that were called protocol play sing the kind of milestones document that we have used in years last Tudor lit review and then the Milestone was a kind of generic check turn the temp to make sure that you were working steadily on the project trying to do with this new approach to this interim report save you from yourselves to make sure that your incrementally working on these kind of Cardiff I are believe that it's valuable to be doing a particular kind of quantitative due to start talking in a detailed way with your teaching team mentor about exactly how the project should be structure you can be running what kind of additional data you could bring in and sulphur walkthrough this to make sure it's clear kind of what we expect ME7 highly structured report in the sense that you could just copy out this prompts if you wanted to infill them catering Baston riding a nicely structured paper be valuable for the team if you can have stuck to the intern this is the framework they were going to use as a kind of rubric to evaluate the about we want you to state hypothesis or hypotheses this is often not done or not done enough in nlu and NLP and one reason that I can make that influences that in recent years if you review for ACL they have started car Defying exactly this requirement in their referee form so as a reviewer your mental state back to the authors and to the area edit hypothesis everyone thinking about whether or not the paper is doing something beyond the parts together and running a quantitative value state your hypothesis can take many forms in this will depend a lot and work on a project you do could be something as simple as my hypothesis is that for my dataset transformer is a better model than in Ireland functional kind of Engineering oriented but it is nonetheless a hypothesis done why you believe in your head pastor systemiq lead you to even deeper inside start which is structured in a particular way has the right kind of structure to reflect function what kind of experiments you want to do but it could be something as interesting as I believe that in processing Project Gutenberg files I can tell whether the author is Amanda a woman based on portrayal of female character which is a really interesting hypothesis from a past year that are the ones we picked be more social sciences oriented and more a removed from the particular modelling tools that are using cos they're just saying but I can detect using a lots of stuff in between as you can think about it with I'm here a description of the dataset something as simple as I'm using multi and a lie ok today's euros my procedure for crowdsourcing a web script custom dataset that I develop for this it's probably ok to just mention it and give the relevant citation for the second if you give us some details on the nature of the data set because for example do you reckon a very depending on whether you're not you have 5000 instances 500 or 5 also and turn like the internal structure of the data show me a picture of that that's going to be one of our top Picks for this week you the thing that forms the basis for your quantitative evaluation if you doing a classification problem with inbound Stata easy to justify that you using macro F1 justify micro F1 to justify something version of the F1 score because you want a favour precision or recall turn on something like dialogue see today or next time tricky question exactly what metrics to you it seems to me that all the available ones have their own fault I want to say I'm going to use perplexity and has these known downsides but then again for my because actually or you can just be upfront and say it's another kind of extrinsic value turn up front is your making you're responsible ok in I really feel you can go beyond we want a description of the model that you'll be using and I think the thing to do here is remember that we don't just want a description of your favourite model the one that you're kind of advocating for but also a statement of which models you'll be using as based as we're going to talk about a little bit later today play Erasure metrics really have any meaning in isolation if you report reporter pt-76 for your mum number there no your dataset maybe we can fill in some background but then clear dataset you can do the provide that context the setup Samba over a bunch of ways that you can do this where the base lines maybe get more and more MV all of them will help us understand certainly at this stage Argos numbers you can provide the more context will have we might encourage you also to fit some of your own bass Lines especially if the public numbers are astronomically high want to say like ok Google have achieved because of their vast resources I'm running is between two more modest models and mine still wins within that baseline is serving as a and time constraints in goal phone numbers like top numbers in literature a grade if you know entertainer agreement also valuable contacts for tomatoes what will be looking for the son of leads into the general reasoning brief the data in the models come together to inform your head off this is a separate item because we don't want you to take it for granted we would like to hear your version of why your data in models and bass lines are going to come together to I don't work because if you do this well it's probably going to be the basis for the abstract for you that's what you want from an ad maybe maybe the abstract or something to show make a summary of your progress so far so what have you done if you've run experiments and they're all in and you have confidence that's great preliminary numbers you obstacles we hope to raise at this point is that we should know that might be blockers for you and don't forget this cuz will bring you for it what's the expected size of the no I don't really think of the length as having any kind of inheritance or vices here take me there what does a couple of pages read for some projects where it's like is snli this is that the parts model is better than metric is going to be macro F1 sketchout clear like the parts model is key internal either way no other is a why called when you know here very long if you're doing something quite custom open questions like I've talked with a few people who unclear what metric still be able to use I think you might develop what we're gonna do for this weekend get the website and I've kind of riding riding the code for a lot of this presentation but it's there for you if you want to get more hands-on with the material what is go beyond method metrics one I might not do too much lecturing about it's not the stuff of really compelling letter broadcast to you my framework for thinking about these metrics and how you think about your own kind of use that no Picasso Direct sight a short presentation about what you might think of is full version of the traditional metrics and methods for NLP where we go beyond show you today and start thinking in a more serious way to truly general we have like what we could call understanding for a phenomenon or it or a domain or test fruit for because you are might want to push beyond 1 scores and think about what opportunity free to do that brings me to this overview here so our primary goal is to talk about projects message or metrics in a comprehensive way but I hope I can give you a kind of foundation for thinking about all these issues and partly what I want to do is just bring some of the salient into the opening cos I have a feeling stop it you're supposed to just kind of pick up on the street kind of presupposes that you get a feel for what the issues are and how they navigate we have an open discussion about it bass that brings me to the project so I want to make a few things clear about how we think about never evaluate a project based on how good the result if not if it's hard to read you can follow along at the website all the stuff is drawn from that mean downsizing evaluate the project based on how good the results are the publication who is secondary but they do it and they do it because they space they only able to publish a few things all science has a bias for positive ever play you might get your paper rejected because you don't have state-of-the-art results even if you have about the state of pa for you we are not subject to that first constraint which means that we can do the right and good thing positive results negative results and everything you got what looks like a state-of-the-art number on your dataset does not mean that you're getting a triple Plus finding if your hypothesis totally false wet a really Stella evaluation of it and like maybe some insight into why everything fell apart that could be an A+ evaluate you based on the appropriateness of your metric method to which you are open and honest and clear-eyed about can you push this on people in office hours like first dynamic about publisher opportunity for you to try something really wild and crazy something that would just be too risky as a conference submission because you never know and surely you know the really earth-shaking ideas began is things that sounded two wild and crazy and back Mairead on the history that it was two wild and crazy for 30 years before it was walking through these issues I think some of them seem obvious but all of them are worth reflection I would love to have your questions and comments I would love for this to be a bit of a dialogue basics adored innovation you guys heard about this the idea of a train Dev test this is a common thing to see in large publicly available datasets for diving into the details of how this works is community you should reflect on this is an obvious choice right because in the background here is an assumption that you will system using the training and Dev data contest draw those from the same body of example Theo that's in Samsung's affair evaluation there system has learnt it's parameters on the training data to be able to generalise today that is in some sense from the same that is a kind of fear of evaluation the only conceivable approach that you could have to evaluate in fact you might think it's kind of stacking the Oliver results because even before you look at the test date that you'll be looking at similar exam imagine that the whole field took an alternative route of saying the ones that will allow into our prestigious publishing human subjects evaluations where humans have to deal with the system and give their ratings about whether the systems are good or not didn't value the numbers that we value in this course this is an obvious choice and the other parts that might be kind of non-obvious about stay the does come from the same body examples the train that's one way I can feel the community kind of starting to push knowledge more and more Stadium for example what's the multi in a lie dataset is one that has the mismatched test condition where you train on some John genre is that you've never see learning to push back on the core assumption that you people aspire to do even more radical things to give up on understated is training in what it would mean to test on a different one because there is an infinity of different datasets that you could steriliser system train on Sly and testing it on chairs and tables in the real-world write completely different David that's going to fail screen 1 chest many datasets that you study will have this kind of split sample size before you are you're on the Honor system to not use your test paradigm free with your publishing is system to do all your work never looking at test even if you possess just before publication with all the systems done single test evaluation into your document and submit Mercia what happened often people really in here to that it's so scary to think about people compromising that position and I think that's one reason why you're seeing an increase use of truly held out in the sense that there aren't even keep track of how many times different teams are values Munich knew you wouldn't be able to like how you did on the string and immediately or you can't actually run my results be there when you do error analysis you do it on death last point here this test said people starting hill climbing on it so if you publish a wonderful paper with detailed detailed error analysis on your test what about what works and what does technically I've never picked up the data set my community French would be that these are just kind of start numbers that you see Alice angel climbing cuz we can get a community-wide centre what's working and what isn't bud set straight like different distribution in Tiree useful to kind of talk about Lisa you just do it only works on this new dataset like that you would want to talk about me my model B failed to capture this yeah that's a fair point tell you you might like if you were I mean if you're doing nothing of just saying like a this system that was trained on Twitter get to the Wall Street Journal down because the Styles are very not even sure whether your thinking of that is your final test depend on the circumstances but if you're in this hypothetical situation if your releasing that Wall Street use it it probably benefits you to be kind of type of tuba yes very important issue yeah I agree with the car is it we want similar distributions here this can get quite intricate as you can imagine problems that have different structures where I was thinking at the really scary version of this set a pencil and knowledge graph do this anyway that and these issues arising were set off I'm not show you some tools that help you manage some of these problems about the kind of chordify I want to make yours that even doing this presupposes are pretty large they just said like Healthcare something wherever example is kind of hard one reluctant to hold out a test set because it really means that you're giving up on a whole bunch of data that could be used instead take me to the snacks me here should be like a nose fix split to have this property should be good does pose A Challenge for assess robust comparison turn all the models use emu pet experiments on one of these datasets and he's splitting his way we know the details about if I want to compare our numbers now do it right comparison we should be looking at exactly the same what happen when we did this because I want to buy answer the ideals that somehow I get let's or his model and everyone everything in a super careful way and then my results table can be reported with no cab we don't live maybe in the ideal World I have room to say exactly the same find that song reliable and I think the important thing is that you report them caviar different like if you think about reviewers being your forces numbers without giving context the Riviera might say Thumbs Down does comparisons workfare wasn't the same splits caviar then because you've already contextualized that would be the goal all text ever written good to just over fitted that you have someone stayed over if you feel it well then you just kind of so large that it might as well be as largest other and sister will ever be produced by is there a place in such that no data set is too large everyday you said it doesn't happen as you still need some something that has been told that properly I stand by that just so complicated that you'll never see the four b recreated with princess Alberta always room for test send things about the Dome baby if you're in selling when you're just trying to create like word factors that include like that you could think that next gen all relationship I need to use training on the same day the general so you don't mean literally the same data but rather just the same kind of dataset I think you're out the dictionary of English this is the entirety large data set define split as part of your project you could impose split here's what I'm going to use the testator be nice if you have the lottery of doing this I would do it because it simplifies your experimental setup in many ways as your particular the second point is important set for very careful hyperparameter optimisation you end up doing a lot of evaluate wets as opposed to doing something like cross-validation reduce the number of simplify eneloop from your experiment code you do hi there is just for small data set brings me to this other motocross validation having fixed place in Cross validation we take a set of examples the number to amor train test split over the results can a classes of ways you could do this the first one I called random please shuffle your data 10% of a data for training rest for testing make another split listen to that as many times as inside into how much variance there is so that question in the back your name but this is a great insight do this I probably want to make sure that I have the same distribution of labels across the split it makes this very easy the relevant code has a key work of stratified you've told the code District just have them equal across all bad of random splits ok so the good does many as you want without having this impact the ratio of training to test examples that will become clear when we look at true you can just write as many of these experiments as you want cos probably with a large dataset never might as well be an infinite number of ways to that there is actually no guarantee probably use the same number of times for training ring on the what's that you do that kind of distorted evaluate really nice flexibility to model is pretty fast to train and test you can do lots of splits this way acting the ratio of train to test example floor to doing some testing of references Castaway thumbs on here some you know some code that you can use cycle as usual wonderful for helping you do this k-fold cross-validation from the random train test with answer in K4 data into whether called k Folds they're so different sets you like full 123 cupcakes pyramid old I each of the experiments full dyes use process Oldham merge together for 3 Folds here these are subset of the example when I hold out for one 3 and 10 experiment to hold out to train on one and three and I get a number the third one here test on 43 and train on one and two every combination of these 34 if I do more fall confidence intervals the bad here so the good and the bad in the last year appears on a train set exactly k - 1 times and it in the test head exactly let's have some guarantees about bad of this is that the size of Kay determines the size of the train test split cross-validation new train and 67% of the data and test on 30th called you train on 90 intestines likely to be different experiments in areas and I feel like two things have gotten model together when you're doing this on the one here runs because he wanted a realsense for system performance across different settings and you end up size of the training and test data the consequence of the good might steal you back to random what have this relationship and are you giving up on this this absolute as I said cycling is great for this k-fold and stratified k fold and then it has these helpful methods to your crossvalidator crossfell score YouTube do Aldi's in code which is harder to tips I want to make there is fun please guys the explicit about white your CV argument is so that you have absolute certainty that you were doing if you want to or just k-fold if you dislike more important is that when you do this scoring battle instead of scoring value for these functions it's going to default to whatever the model skorys for a classifier that will be accurate I think time and again that that's probably not the metric that we want to be using for our evaluation take the step of explicitly changing that to macro f free your choice a few variants hotel some special cases against I could is good about this so in leave one leave one out cross validation an edge case LK Folds where I hold up just a single example for test you know forever examples in my dataset in do that if you have a very small data two ways it could be small first you might just have a tiny number of exam you can tell the do this the other situation would be that maybe you have a very large dataset but you would like to conduct evaluation evaluation that is user every user is associated with a 100000 tax and you have 12 uses so you actually have a quite large data set this user level evaluation suddenly potato sink white spots Tracy might do the leaves function called leave p-groups out that's actually quite sophisticated you to do splits that are not just the stand is it you can do to make sure that you split along a temporal dimension which would be important for anything with the time-series Aspect to it also manage things that needed to be / like and demographic group the things like structure in your data immediately evident from your labour labour find optimal splits anything out in talking about all the stuff put in Forest by train tomorrow would you ultimately training on all the data location is approximate pasta what do what you just described I think I think you spell addition as part of can you find me a valuation on the test dad has just trained test in no-dev play Someday you might do a lot of cross-validation runs inside the train exactly how your system generalise have these is your final evaluation don't have a test benefits of not having Justice test the test that was chosen very strange play the ways to know that we're reporting the average of a month tomorrow buses on the other hand pristine kind of held out 4 ratio between friends play radio I feel like ucat 20 25 33 behind stepping back and thinking like where are large enough number that the testing isn't going to be kind of wild me too much training Santa I can give get more interesting if you think about holding out much more test assistance get if I show them only have to do that for exam The dataset Creator you might be shooting Yourself in the Foot by doing that because people want lots of data to train that game to you read it back and forth to get us to E20 turn on the bedroom light regarding my first pass at this would oscillating thing that might happen to you sentence are switched so in academia we were kind of like where to buy ace for having them look better than they are when you think about the planning a sister on the line for that diploid cyst what's harder look at the economy valuations you're doing to essentially get a conservative number about much more kind of like truly holdout test parsley unrelated when you do the ply will working industry wanna comment on exactly right in the eye mean play question in sentences is on the end navigate to project outdoor on the other hand do you also want to keep your job play Radio 1 switch the true strength splitting in using the same kind of it's a little bit any point in doing leg and study in a given domain and seeing what the best ratio changes with ours I mean it will get interesting very far which one offers the best validation distance offered in Talbot to the community so that we make Better Decisions going forward we would all show me the see how we get the ideas right you staying what your ideals remind me that there's another interesting thing you could do what sizes which is the same ok have my fixed test office to use all the training you're advocating for assistant with very little I think it would be smart for you to where you reduce the amount of training doing a showing there the training data your competitor will get by on 10% of the day much better at that point than the other competitor credibly value nonsense favourite systems that can thrive describe I think you've retired into like the general feeling that we should make efficient use of all the data about bass baby stuff we've been taken for granted but it's none of us were thinking about these things I said before understand evaluation numbers in NLP and isolation and I feel like you get both sides here because I'm the one here publish a number like my system gets .9 5.99 you had you just declare Victory by default right because it's all the time play happen is that your reviewers will save obvious related to that phenomena sing about NLP performer share Brothers awkward so even those really good numbers require maybe human-level perform firebase Stan Getz pt60 you might the spare but maybe it turns out that humans are not a lot better baseline is way below that in which case that pt60 make real progress on the the clear case where we need contacts or you do not want people defaulting to what they think means in the find a really important for building good argument they shouldn't be an afterthought but rather pretty Central to how you set up your hypotheses and you can hear us encouraging that with this protocol doc play Sounds of really important to building a good case that you can do is use them to illumina aspects of the problem that you're stuck do you have your proposed we've seen versions of this before we didn't do well on it but think like have a model it's just something love Victor being sensitive to World Order pair that to a model Leica and r&m independent Ireland is better than I've got a kind of intuitive indirect argument that word think of lots of other examples that will have that quality maybe you added some attention mechanisms to unli Mars difference you know the game that you get from that is kind of illuminating the fact that there are import but important to mention here random basslines you might want to include them I mean often the random baseline will be easy to construct like you could just say I'm going to guess proportional to the class label or I'm gonna just pick the majority classed as a two common baseline the dummy classifier in dummy regressor here this about psychic model easy for you to build the dummy classifier into your standard experimental pipe testing like 5 different model separate codebase for doing the dummy classifier like you might introduce dogs there or change their value make it just very hard to make sure all your models are seeing the same data past progressive interface as all the other psychic model familiar faces all the models that we've use for pytorch and so forth in this class it's just that they don't make intelligent use of the features that you nice in terms of automating your work Staedtler issue this is kind of already pushing us beyond standard thinking about evaluation of cold this task specific baselines weather in Europe problems in Jess of baseline that will reveal something about the way the problem is posed Armada one example of this July we saw that hypothesis only baseline much stronger than random guess I give you an argument for wireless I think there's a scientific reason at least in Part 4 why these can be so strong a story that has some prominent a few years ago in there's a story clothes dataset and this is where you are given a pass between a coherent and incoherent ending for that passage reported a lot of progress on this you know numbers that look pretty good compared to human performance our kind of got qualified by the fact that if you turn the classifier loose just on the endings giving it the story really close to those Tottenham have such a hopeful story about that one as I do about the NI baseline this might indicate that is a real problem with the data play from now given if there is an issue partly overcome that by just saying my baseline is this classifier that 20% how much further we can get from that last evening A22 please time including some prior knowledge about in doing that I'm too hard for yourself you mean so do you think it holds for either of these is there a danger only joking in the centre it's probably because you're giving a positive argument that you feel it's not fair baseline for Stuart that certainly what you were doing Hannah discouraging about and seeing if bass lines that are really strong what I'm trying to do here is say The Fall the world is going to throw later will be like just be thoughtful the really exceptional papers in our field about older often auto into conceptually not just granite another big Tom parameter optimisation never going to start using those so I guess argument before my rehearsal and amplifier will be person terminology here learning the parameters of your model values are learnt this part of our optimizing the model itself we often covers the weights for the coffee when is given as a parameters of your model settings are set by a process that is outside Scribe under boundary between these two can get blurred if you think about define for hyperprime including including like what kind of computer you're running examples of the regularization term for a classifier that a clear hyperparameter we all have an intuition that those value medically effective the dimensionality of your network turning the activation functions that you you is there that you could use can have an impact especially for models that don't converge in a reliable way because I don't hear what about like the way you normal eyes feature value did you apply tf-idf say something like that can of drifting further and farther from the core part of the Mallow but I think you can think of all these things is Hyper pretty Broad national for doing hyperparameter optimisation a lot of ways you could go out this but you are pick one kind of back-and-forth narrative Duran experiments with models A B and C to use the default hyperparameters as given by the implementation initialising with no R&D the Murcia what psychic b that you found that your mother's see perform best in your report that in your paper anything you know victory I was reading for seeing it came out you were is going to ask for certainly wonder about a few things the first did you actually try any other values for the hypromellose without reporting about your prat shadowhunter perform the other show me question right you maybe you didn't have to confront had one your competition with your flat will wait a sec about whether or not position correctly fiddle around with some values a little bit running record your favourite might conclude here is that all we've learned at this stage I'm setting up the hyperprime play modern jazz favourites the other play silico hard-bitten reviews going to say why are the new that because I know that I can just craft a we pick these settings in a way that will make some model with Tara giving this is kind of behind-the-scenes fiddling I don't have a TV France every model it's best chance your model of Explorer really wide range of hyperparameters bottle tube the best settings according to performance on your development data what are the models do on those settings on all your test breathe at home this course there reviewer might say well you didn't pick the rights based approach confirm there you should have tried these new very I'm kind of rationale the other would be just that you are open-minded we're trying to figure out what's best like you've been placed in a situation where you have to do a bake off for a class and you want to win all the same Dynamics kicking it's just that you're not arguing with someone but rather just trying to figure out I think the same rationale applies because you want to pick A B and C and really see each one in the best parcel the Spirit of this that we get a fair evaluation dressing them back to the hype ideal each hyperparameter tier mod really large set of values for it list of all the combinations of all the hyper pregabalin cross product of all the sets of value features you identified as well the settings update on the available training data the settings that did Beth Medina your model play by the logic I gave you before however this could be very demanding suppose that hyperparameter one has 5 value is 10 ok now there 50 settings third with two values now your number of settings jump to 100 you want to do 5-fold cross-validation 500 runs do 10 random train test split because you want like test whether or not you models statistical different or like it confidence in 5000 runs of your one of these models take GPU compute time very expensive yeah car from the Sadie little bit this is untenable Forest they said that the only people who could publish were people had made good on this entire thing here who was vast amount of money will be able to publish it starting bias in favour of simple models because those will be the mod afford to do all these run alarm where you see this kind of things but this is from a nice paper and doing an LP for Healthcare is there a method section in an appendix all above neural networks were 2-0 automatically using Google visio with a total of over 200000 GPU hours ok the dollar an hour whatever Google is going to take from me this could cause a billion-dollar is very easy Amazon credit but not close to that that be there we can have it as an ideal allow others to expect it from us nursery were Choir of other people we need to introduce being pragmatic about his choices so here is my view alleviate this problem is very impressionistic but in descending order of attractive as a reviewer or two my imagine reverse out there if I think about them being and tiger random sampling and guidance Centre liar really large space budget of run the random sampling part would be just that I set up my great as I did before 507 slide and I'm going to look at 50 of the random sample will do with pick 50 random settings who is kind of having expectation that that's a pretty good sample of the full space that I probability Mr setting that was going to be really Transformed what kind of variation on that that is quite a card guided sampling here which would be that separate model intelligently travel around in the hyperparameter space that's better than me both of these strategies as you can just say look my budget is 50 interested in doing your hyperparameter space artificially because it could be why September 5th is pretty power the butter reviews especialista evidence that random sampling strategy included as links in The Notebook on the website sampling is another proven method in a show you a link to some software hyperparameters is really nonlinear I think it really is going to depend on your model and your settings anything vol baby bedding company play YouTube We Want To set something like random sampling yeah that's you keep focusing on this morning as you are advancing on start explain yourself this morning see that about the second strategy before I'm sorry that these are all ones search based on a few packs of trade especially relevant thing in the context of learning models wear your model mate take 1000 very expensive to do all those runs and then check for form could do is have a background hypothesis that picture is going to be like the past that a setting that was really bad at the start it's just going to remain bad settings that are good in the start are going to remain good that you might not be able to prove lotto support it with some kind of learning curves that show what's happening in those early things who runs the project out further and show that your assumption is basically career do that kind of build your case but you know if you don't have the capacity to do that even just ignore laging that this is the strategy that you based on budget rather consider Longworth Road acknowledges that work done on summit the minute everything you know that I qualify this is kind of nice because you can control exploring a large an subset of the data that another strategy that we explored off and on throughout this course and that would be that you know my actual data set contains 500000 examples but I'll do my touring on 5000 lots of flexibility to do lots of check that out to the lord the real pub with dad is that I think it's obvious that some of these parameters it's a Premier Pennant on dataset size the case will be that regularization be affected by the number of features you have and the amount of ever tracking badly with this approach do some heuristic search for getting kind of low down in this list but this is still quite respectable right so some experiments that you run just on a free farm where you figure out that some hyperparameters never change performer is really change it a lot just fix the ones that seem not to me a case for absolutely you would want to justify this in the paper you'd want to say that's a problem nationality of the hidden layer the Range 50 to 300 so we said it all the regular no you review I could say in my experience the hidden dimension Aldi really matter caravan a tickly that you didn't see it you've got a pretty good idea find the optimal hyper premier for all subsequent split Justified on the idea that the splits place of saying like I'm in a run one experiment really carefully credit cards for all the other settings that are Bob with the large data set turn up emoji at the bottom here you still do c adopting choices that other people have made any word actors on words from the web know how this model is going to behave in one pick the parameters that were in the originally published paper the data is different in my goals are different so far can you get some complaints don't translate that was after more for that paper isn't necessarily of them offer yours setting we're trying to do webscale training a word representation you can afford and again just arbitrarily expected people are on multiple runs across the whole and there's lots of papers that what's on Sunday to justify what's the best this is not sufficient evidence that urine play the paper is really one of these pure paper that just says sete and my evidence is there Wi-Fi Republic hope that you're trying to do more with this paper trying to show us something more about this model than just that you won because I don't address in a little bit call Alba what's the different type of pram you have a good night after sis and the results are promising necessary turn the juice first I want to describe that your narrative might be perfectly fine I found a setting where my mother was the best it's a really good model for the task exploration I be something that you explore more openly to say for example house market my network what are my performance drop off or how much does it matter how is regularised or doesn't matter even help with initial I was just injecting my values there that I find all that really are doing it because after all really care that you want and congratulations in this case that you want want to know is call Father is it to vary a setting perform an expectation about how old optimisation tool scikit has grid search and randomized search stand by Sia and render my search will be the one where you can fix your budget get optimizer pack guided search through the space nice cos it plays well with the existing scikit model some additional game one more thing here classifier comparison this before the nice way to round out this first lecture so just to classify man to classify as cos I think a lot of you were doing classified operations extend other kinds of models and we can talk about how that would work pacifiers different some way what could we do to establish that there if you think about publishing I trust that difference is I'm seeing a really and truly volume eight practical differences this is the ultimate your test said has 1000cc if your test has a million example find a 10000 case it shouldn't this is live saved disastrous consequences of then you shouldn't in the same much more than that you're making a meaningful impact on the world is that one case place of the test that has 1000 examples then a 1% difference in accuracy F1 I'll be like 10 exam don't know whether that matters at all bring in the background here is the Southwark West creation is there a crossroads in the case where was 10 examples for reported number change does it go in the oven like it's just on the Edge well that could be true even of these models here because maybe runs you save 10000 lives put another the amount of variation there is between runs what things mean that we probably want to go beyond just take me a few ways you could do report confidence intervals around the means that you were giving your kind of traditional way to do this from LP systems is that this traditional way here conservative in the sense that your sister might actually show less variation * this confidence intervals that show value find anything you've ever seen happens you can think about bootstrapping the conference library for do make much more use of your actual data to decide really nice way to do the confidence interval and what your reviews are likely to do is say ok those two intervals don't overlap 30 systems lot of overlap you might say are there different but in practical terms this might be the same powerful step but presupposes that you can do a lot of raw you like 10 will be like 20 different runs to get a bunch of values for them picture of what the car has been living lady kind of thing that you could do than and again I'm just kind of driving on best practices as I've learnt them from the Limerick wilcoxon signed rank a variant of a t-test doesn't make any assumptions about the under presuppose in particular that your scores are normally a lot of different runs is 10 ideally 20 or something like that you want to pick the wilcoxon signed rank test over this very closely related 1 the rank sum test which is often called the mann-whitney U test wilcoxon 1 breathe in situations in which eurosystems are trained and evaluated on the same underlying date best practices that are laid out before you want the same date of release compared you violated a lot of independent tax and signed rank test is the rank test caring about is the relative ordering of the scores that your system reports good stuff to take go along way to people look for people who are evaluating your occasions are in order this form sing about the practical importance difference is that small p-value is not a larger effect size large p-value doesn't mean result it just means that you like medication to be kept in mind this is had a very weak evidence for some kind of detectable difference would ideally bounce to get something establishing that your systems are different in practice great you that this could help you instead of advocating for a sister situations in which the numerical final test so we can wrap this up here mcnemar's test that you can run to compare two classifiers that depends only on One run of the data operates on this kind of cool variant of the chi-square test this can be pretty valid report different values under different conditions resort to this just in situations in which you absolutely cannot afford to do the number of runs that it would take to run a responsible Wilcox this is a kind of fall back it's better than nothing for you reviews if you can come if you can tear it with practical difference pretty good other big issues that I want us to confront that are kind of particular to the deep learning era out of time I'm going to see those for next time so next time it'll be these juicy issues that we are facing today beyond 