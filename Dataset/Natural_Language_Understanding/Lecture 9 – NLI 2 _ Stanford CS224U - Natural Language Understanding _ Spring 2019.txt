suppose we get started today is to finish up in a while this year before and I think the consensus is that if we finish that early I'm going to give you a glimpse of the top Picks for next yeah basically my favourite topics and I wanna get people to do projects on them and I know that were introducing that content kind of weight and in terms of people selling on project topics but maybe this is my last chance to speak some things in I've got lots of good links to dataset and so forth that would be about pretty close to the true dream of Nou I think which is like reducing grounded situations in the real just comes late in the course because we have to build to get to the relevant models certainly by the end of the day today you're well prepared to get your hands dirty with is it for underground and 100 say few things about that if there's time and just by the way rash next week we have just one day foregrounded understand for semantic parsing is there is that we have a huge amount of material for semantic parsing is a challenging topic it's very interdisciplinary basically after Noah bunch of linguistic semantics and a bunch of machine learn that sounds exciting to you area to work this whole codebase sippy Cup created lots of notebooks for it and then we have to supporting screencasts that kind of get to the essence of the technical idea wonderful papers so ring around for a project idea play Talk some antics with me before than this is like the Dream The Marriage of 130a and the score check it what's an introduction to her but it's not only so much you can do in an hour to do that kind of self study that's why I'm talking about the schedule so do you ring the last homework in the last Bake Off that we kind of settle into a rhythm of project oriented assignments you do a lit review and a PowerPoint should be paired with a mentor from the project team will be with you through the whole process so like somebody you can count on for advice and will be giving you a lot of feedback can you first and then we have the sink of an experimental protocol and I'll tell you much more about 10 closer to the time but kind of hovering in the background here is that we're going to push you to make sure that your project has some kind of quantitative evaluation associated with do you think that has to be the way I work in NL you were because we think it's healthy too kind of push you in that direction and that's why you'll find that when you interact with people from the teaching team about your project they want to know what date are you going to use because I think that we all and the teaching team think if we can figure out what date are you going to figure out what your metrics will be and get you to the point of a quantitative evaluation play mind as well as you do project planning and know that the project assignment are going to be pushing you and not Direct it's just a short YouTube video does it typically fun final paper at correspondingly most of the material that we do in class the course it's kind of oriented toward you Texas kind of it gets a little bit better we talk about methods and metrics then presenting your work and so forth should be relevant to you no matter what kind of project you're working on that's the right nothing that I wanted to just clarify about Bake Off for no what's the point of freedom arceaux number homework for and bake off for all about Word level and tournament it's kind of small version of Vienna like Prague us datasets that you can work with goes for this in terms of you bringing an outside vectors for example you can do it with Elmo or bird batteries you build yourself glove write your name that you can go onto the web and in fact the task is pushing you in the direction of having really rich inputs to these models otherwise you're just not going to do very well modelling and outside data that will initialize your vector you know cast The Net wide turn restriction that I feel like I didn't emphasize enough last time though which is onto it when you develop your original system and when you enter the Bake Off will be working only with the word destroyed prob keeping my net for the Bake Off evaluation the edge destroying portions of the day that are off limits any combination of word disjoint training day destroying part and the reason for that is that this problem your performance just goes away up if you're allowed to look at edges joint and the reason for that is that that means that in training words that you see at test time contrary to the direction and I'm trying to push us is there all new words that test time and say what we're doing is kind of stress testing your system on its ability what it knows about its training words into a new part of the vocabulary volume portant the allowed to win the Bake Off if we see that you've brought in the Editors the same word time assuming that whenever embedding space you have last time r&m bearings have the point cos it's like space has interesting structure helps you generalize into the internment Prague my sister is going to be really poorly in fact if none of the test words are in your own bedding to disguise where I got the internment date just letting you know you guys can search widely and stuff as long as you don't make a deliberate effort to figure out where my data come traits of a spirit of old can I go out and see you now word embeddings that people have beliefs that are especially good for this task before we go back into the account every capsule last time I introduce the problem of analyse and I think the summary of that discussion is kind of like an Ally at this point is a commonsense reasoning task of some kind kind of hard to define exactly what the criteria Arthur in town and contradiction in neutral but we have these natural stick datasets that reflects some aspects about humans actually reason about language and that's what we hope our systems do as well because the whole idea of analyze that it's kind of keyed into fundamental information about how we use Roman centuries information and if you think it extended senceive question and so forth SM airline multi analyte which are exciting large status will test District different aspects of your system so snli is just image caption Ally is drawn from a bunch of different genres and a nice twist about muppet analyse that in addition to the diversity of the training data there 2 deaths snatched which means that you evaluate on the same genre as that you trained search Twitter it gives you a chance to see how well your system can generalize out of its comfort zone another exciting about multi analyse that the team play bunch of annotation can use for kind of hair analysis and I'm going to give you a bunch of illustrations of that later I think it's a nice opportunity running a good error analysis section for your paper is often very difficult and requires a lot of creativity and I feel like they've given you this gift of the structure for a very rich error analysis by providing is annotation about how that works as far as talking about handle features word overlap word cross product and what you're saying here is a bunch of stuff that you can derive from wordnet and then the hero bunch of other ideas I had one side here which is narrative of my talking about cambelt features and win your models and then talking about deep learning model into a trap of thinking these ideas are in Direct competition that's not so I think it could be very interesting and maybe some of you have done this in your work so far combining like a word embedding of something short and dense feature representation that comes from homebuilt future the characteristic that we will have very high dimensionality and be very sparse contrast with what you would get from Bergamo Glover whatever which tend to be very short low-dimensional and very dense represented that you're short and dance long and spots arises then like what's an effective way to combine these two representations in a unified model but because it's not like I have to choose just long and Sparta short and dance the advantages of both now you seem to do a bit just concatenate these two representation looks like the Green has 20000 dimensions which is pretty realistic and the short and dense one has 50 2050 dementia that way of posing it suggest that there might be an issue here right which is that there's a kind of wild in Bali in balance on the one hand you have August sparsity dominating the representation and it's kind of into phases like it was very different kinds of information might not be effective to simply concatenate the two representations I have found effective in various work that I've done recently little bit of preprocessing on the long and Spark side Jordan dance undercut of two classes of approaching could take their so you could just apply something the model your developing like you not apply LSA or PSA or something like into a dense representation that's what I called this model external should be to actually have your model learn some weights that perform that transformation that in bed this really long thing into a smaller space introduce some difficulties of optimisation but I think if you give it separate parameter I'm kind of manageable at that point you would concatenate short and dents in short and density and you have kind of two comparable and I think that there is good evidence that presentation would have a lot of the advantages of both sides of the incoming timer 2 you know about that attention would normally be defined over entire representations not over the individual dimensions so added some upgrades creatively does that ring any bells for you Indian sapphire lightning transformation than this thing went once concatenated if you have subsequent wait up here modular chance to lots of and I think that's where this becomes potentially so powerful play introduced in our experiment is not required for the homework but if you want to do a project and Analyser think this could give you a real head start are these ideas concrete and introduced these two ways of doing hyperparameter select really large dataset where you can't afford to sit around through 50 iterations because resume are your timer resources introduce the side of a hypothesis only baseline what's a virus trust this can be surprisingly good baseline for any give you a rash now that was a kind of linguistic scientific rationale for why this work there are some intrinsic bias tell me problems about what's likely to appear on the left or the right that wasn't my kind of justification for why hypothesis only might always be pretty strong send and compared to random 33% in you really want to think about the strength of our path this is only bassline about premises only experiment enough about that Roman I think I said something like well set the other be pretty good as well biases that are telling you for example that if I give you a general word it's likely to be a hypothesis I give you very specific word it's likely to be a premise in an internment pair very specific things are likely to be is it raining in contradictions in the centre of this problem because exclude other event all that leads to think that a premise only baseline are to be pretty good experiment on sly aghast so 33.3 which is the very definition of ran must be some kind of bug in the code or something cos it there's just no way it could be this round and I couldn't find any Bargains I start to do some real soul searching like that I give you guys a misleading assessment of this problem send a kind of bad sleep for a while I guess does anybody actually see already when I was missing sli what's after took me a while to realise turn down and I am going to bring in the local task but I don't think that's the answer why is it random the weather dataset was constructed premise to crowd workers and exactly one of internment contradiction and neutral was constructed on the basis of that premise constructive sentences different while that of course and they do contain biasing information about the label but if I give you only the premise this was like what the crowd workers got before they started their work to see is exactly what you get from the data collection which is perfect the system has this unbiased information just to reassure myself where did this for the word intimate model cos your homework ask you to do hypothesis on strong baselines again it's the same lesson but here my insights are least consistent Close results what is well with the world concerns by construction in any of the premises could have led to any kind of you could have contracted something that wasn't a neutral even more about design it's like I was given the premise and constructed through centre element 101 intro this is really by Design and I should have known so that's a lovely cap with a little bit of new information and then the final things I wanted to do was move into the ground because I think it's exciting models that are kind of specific to and Ally the question previously song logistic regression differentiated from this morning have a higher baseline progression for the experiment down here model to implement premise and hypothesis only based it's just that both of them have a lot of information that makes them much better than RAM tomorrow this one was constructed to have no bias light comes to the that one was not Lancia word strong from somebody play two classes of models hear that and this is a by-product of us having a premise and hypothesis the first class I'm going to win call sentence including mod it's models you get a separate summary representation of the premise and the hypothesis and then you do something with this is the kind of model at your pushchair if you have in mind that you do dog and dream that your learning representations from an l i d l transfer to lots of other tasks because for many other tasks you need an individual sentence representation offers exactly that play hear play slime for this kind of model which is just time of word doctors approach that you've explored in various places before has the sub got every dog danced as my premise who does the hypothesis bedding look up to get this layer that would come from some free train space presumably 6 dimensional representation for each side here because the word the number of words could differ nothing that's a kind of hard summary does vectors like the Sun or the average cancel whatever I put some operation that's going to take you from this collection of vectors into a single fixed dimensional do XP and xh through that again this is familiar you might just concatenate what's with the input to your classified what's a very want to a simple linear one or a hold deep learning model whatever you baseline in the sense that this is a pretty primitive way to combine all those words into a fixed dimensional representation actually these models turn out often to be pretty information from the words end up in coded in for the premise fancy that simple approach you can take to the Bake Off would be one word premise in a one word hypothesis so there's nothing to summer average I get XP and xh directly questions about it code for doing that I know your code in the sense that like goldglove leaves 5 A75 these are functions that you've actually worked with before twist here is just that as you process examples you need to process two parts the premise and the hypothesis where as before when you've done just Stanford sentiment treebank what is the same it does ok sly here training train death 1.6% actually below the hypothesis only baseline which can't make the same kind of Monster give you intuitively some information about a model that you build on top of adding if you do something more sophisticated than the sun take to be the Rational this before sofa premise on hypothesis separately might give them our chance to find Rich abstract that's a kind of a human way of doing this like I have an understanding of the prayer hypothesis and then there's a subsequent step for figuring out with them a list of Requiem in addition this is the Dragon dream sense level and coding good for Celtic transfer to other if you wanted to do like the the glue benchmark for exam natural to choose a sentence in coding is the premature hypothesis in one of the sentiment tasks or in one of the grammatical the judge that's received a lot of attention in the literature that is a kind of step-up and complexity from that simple averaging of something a vector I kind of have to RNS the premise hypothesis I use the same example every dog danced every put on move in what I presume is a single in betting you can have different in bearing spacers for premise and hypothesis if you want assumption that word senses are different depending on where they appear I am assuming here is that you would have different parameters for this recurrent neural network so I put them in green and purple hair at that level you might want to learn something very what the environment for the can't four-seater you can have type parameters that can be a single or a here is that the way this Mallow subsequently works is that I get this Final representation H3 mihir HP combine somehow turn to the classifier depending on his that like the final state in this our nan is a good summary representation of that example hypothesis makeup happier once you get to combo this is exactly like that first baseline that I show this is primitive like song or average to combine my work better really complicated functions and r&m but I've learnt as part of the task instead of imposing right choice for that know your network is going to discover that the sun was exactly the right function conceptual talk to think about that variant I like even our codebase will let you explore a very well this is a bi-directional ironing and in that case you might be using a summary representation drive like sort of the H1 01403 cat representation of the did all of that into this combo funky wonderful to see you guys exploring different variants of these I've only provided strategy for excited implement this for one in the know but I thought you're here instead of just looking at the code which you can do on your own what to talk about the conceptual strategy that I took for sentence including our own and how would I implement this model here it's nice and clean give you good code if you define a dataset class turn the repository by and large for these models it deals with just a single example so a single sequence of words length and a single label what you would do if you just use the iron and class with force you into that more than having a single dataset you could subclass the existing one and just have it you objects that are pairs like every dog downstairs tokens every put on with this token phalanx and then finally bookkeeping and that's just imposed on you by the structure of the model that you're examples need to look like for the classifier model itself the subclass of NN module in Python torch you need it to conceptually be a premise rnn and hypothesis are in who is implement its forward method those two are orange no the premise and hypothesis get processed in exactly the same way Ireland coldest processing regular exempt gives you the two final state representations just need to buy hand as part of the form f in return fed into the class of find me the way the code is ri can you do anything to the base class this special one just needs a slightly different predict problem f get me the different predictive f video with examples like that's kind of just back to the fact that your day that are a certain way anything else about the logic of optimising the model is the same is there a local ice to doodle data handling through the structure of the score what is the computation graph love pytorch because it makes all the so easy and with actually if you look at The Notebook you'll see that in relatively Wilko you get one is there inheriting from a similar class weather in spy Direction activation function is in the dimension Aldi's in Salford that kind of order nulc entering into this class of sentence including model a small variant of the one that we just saw instead of having to Orange triage three structural networks if you look at just the premise or just the hypothesis this is exactly the model that we used or that I showed you for the SS all the same variant also if you wanted like more more complex combination functions than the one that I've seen on here introduced with all be kind and nicely modular Rise because dealing with a tree or in a tree of recursive neural network for the premise and hypothesis the class of it was like exact this will be pretty straightforward to free RN that's it for the sentence in Connemara stands for all the ways that you could impose new Innovations and self worth I said I wanted to introduce a cold change mod them changes that basically you just run together the premise and what version of that is the one that I've given here dog danced every put on me write your own boundary symbol if you wanted if you thought it was important for the model to learn that transition point it can be like a unique token that you learn are unique in bedding for you can just run them together like this reading space that's all these Grace l Ireland for a process the entire thing premise and hypothesis together on the basis of the final state year make a classifier bi-directional in which case it would be using h6n H1 as the basis for the classifier in all the same variants applied I mean this is literally just using that you might have used already I just feed in the Blurred together premise and hypothesis in process I guess the rationale for this would be like the premise establishes the contacts for the hypothesis because it is it as though a human reader has gone through and just read a decision about the Blurred together exam play me that yeah as I said it may be that corresponds to some real process this is the throwaway here like really do anything except define a new cat makes the leaves of the 23 just called that early in the course reefer experiment simple barrier where hi Ben I'm going to run together the premise and hypothesis give them separate prayer I can Ireland for the premises further hypothesis the Lincoln point is just at the initial hidden state officers RN state of the premise different parameters intermediate between the sentence and coding thing which was keeping them completely set together one that I just showed you because they do get better together but with different parameters reflecting different parts turn obviously this has lots of area fill around with different views on each release different time station functions different cell curious presented tuitions about which model is likely to be better is the kind of clear winner if I vent rock performance at the nli questions about banana 2 imagine your bed is this wonderful turn the settings on accommodation speaking cat call better than have any of the ideas to represent to contribute to the classifier that nation is a good choice because then it would private above that for the classifier you have access to all the different what part is Cher like this certainly has it it doesn't have it but if it was by directional then you probably can catch do you reckon it was alphabet I'm here to Ireland this would be the best one I intend to lose some information over time listen in on my ones and then combining it's interesting just in keeping it shorter here thanks for presentation here and saving for the classifier and let me doing better and I also like even if you've got a sophisticated sell-by CD inevitable actually it really look at these major boards because explain that inside attention which I'm going to show you next is highly relevant to this because to the extent that have mechanisms that help me remember distant still I like that WhatsApp data play have enough to me to be able to differentiate between your hypothesis and everything so just having good night with more frankly I want to take advantage of both I can see you're having two different ones would give a little bit more turn on the green inside think about data versus the number of predators evil I might do best with this one which is the kind of most minimal model turn on by think that the best performer now that you can have a tense encoding model by enlarge by people who are doing multitask learning so they want a representation of the individual sentence transfer to other tasks or representations from other two if you just want to say hello that's like Powerful turn on the Bake Off turn the light on what's the weather created this was by reading a premise and then saying I'm going out riding Tillman model exactly that process time set an alarm smart network handcrafted stuff activist model that's because sentence including ones score there is currently 0.4 on the test set I'm here I think these before we get to the ensembles I think these are buying large letter tree these are the but separate premise and hypothesis lstm tend to be a bit higher right exid on song obvious just had a glance exactly what these models are doing Freddy Shazam ring ideas for attention mechanism more connections between the Promise and hope offer showed you are colourful limited in what they allowed so the sentence including ones just use the final two representations in catnip information is it has to be summarised by the to that I use for the classification as I must have this in processing the hypothesis read some reminders play Prince hidden representation isn't enough to get those kind of room another guiding in Sofia might be there it's useful to have a kind of soft online meant between the premise and hypothesis insight from protester nli where people use algorithms to construct and of high-fidelity alignments between premise and hypothesis news that is the base decisions about internment can imagine that attention mechanisms especially the word by word ones that I shown software to explore that kind of alignment by establishing some connect what's the on in the Spirit of kind of making more dance interconnections between the two parts of your exam what I think is the simplest attention already kind of involved but I think I can convey to you the intuitions about how this no you're example every dog dance some poodle dance will be the target stay here hc use hc score vector that combines HD with H1 H2 and H3 the simplest very in the sentence all I'm doing is Farming the dark product it's kind of like unnormalized cosine similarity give me a vector of scores are scores getting normalized buy a softmax function into what a property called the attention weight vector of length 3 in r example here wait a kind of waiting on you in this case the association or the similarity between what's called a context vector version of that would be just that I take each one of these reply it by its attention weight down here play Ave is a vector that has the same dimensionality and each one of these hidden States waited by attention tension way is measuring similarity with this final stage some kind of attention combination function so here over concatenate patient with the final representation here through a kind of dense layer activation I have called the attention combination variant where I have separate parameters for the context vector actor here and just when are you fit a classifier companies you're not into Italy what's happening is is it form here I'm getting a measure of similarity with the final vector averaged into this can't extract context vector and the usual thing that we use for prediction is felt through into this classifier to make a final the usual thing and then k would be what I called the reminder generated view of what happened previously in the Prem fast-tracked to classify example just in case you the sort of person who likes to see numbers here so what is two dimensional vectors scores intuition which you could probably read off of the vector values is that the first word every very similar to dance time magnitude so you get a really large and normal similarity get normalized by the softmax function Latin singers mean don't expect X Factor is Dimension to right it's an average of Aldi's wage turn over here concatenated with this represent 4-dimensional vector Justin Slayer to get the attention play that gets better lightbulb larger protector can you get another one just had water number well it's not only larger numbers but also larger numbers that are very similar to this Final representation that sort of the way I cut things up so that you would see what the vectors are but you're right where I am to this Final representation and the larger my magnitude the greater man can I get free clothes I made up the van benefit from the attention vs seen all the hidden Fates which word of the day a question so how would I see all the other clothes how would that model deal with varying 56 if I didn't know the link summarise them into something that was fixed dimensional so that I could feed it traduction just literally reusing them tension mechanism is giving a second look back in a way that this mechanism is hardcoded to be waiting things that are similar to the spinal we're going for the same situation she's everyone and bring them back Aldi's model turn off light as they move through data hazy view of their own past and then we want to give them these remind reminders come through this very thin transition points scoring functions of people have explored I just showed you the dark product learn parameters in here like this one that's called general by the longer because it's giving us this signing HC the final representation with each one of these facts lyrics for that kind of by linear combination give us a chance to go way beyond the hardcoded ideal of the dot product into learning just in general what what relationships are valuable I'm trying to solve that is very similar here I think the crucial innovation from both of wait as part of the attend promise request yes and if you look at the literature especially the early attention which was focused on machine translation the weather define rights another state also have it for this one the hypothesis so what you know the translation the reason that meaning for it's because in Translation each one of these has an hour predicting a word for example I know why you haven't helping only in this last 8 this presentation computing for all these other steps the information would just disappear anatta limitation of the places where we getting super for this global attention the answer is kind of no it's not worth doing for every step in the hypothesis transition to word by word detention is going to try to make good on exactly your intuition I would like to get information at all of these premise involved to here let me see how I can do this dance some put on move attention on this Beast this has a kind of recursive property so you're feel little bit at sea at the start presentation to build eating it's looking at be here so big it's copied over read it by the previous attention weights k a representation I didn't show you those but just imagine you had Wait by those attention things Adhan vectors from everything in the get copy Deezer all the same repeat Ada whatever dimensionality you had for the Prem not ready but in this case I'm going to have to make the simplifying assumption that I have EastEnders w you can see that the fundamental thingy in all of the premise using the current state with everything from the Premier get through the snow have the right dimensionality for us going forward weights for this are phentra softmax combination of m with some other Lord Waits what is can of like the context vector that we saw before it's all those promised it again by alpha b bringing also the previous for that play it's the same as before this classifier layer is just I would use this one calculation that I got define austere I do it for a and Furby and when I get to see pacifier depending on KC which is depending on k B&B which is dependent on k and a lots of info influences from all these attention weights that I kept word by word through the entire process I think it makes good on the intuitions that both of you had behind if your representation States have similar values rights their product return professional B Simone kiss right maybe it's really useful if you're trying to say that one thing is not related to the other that your hypothesis is not liking your promise attention attention of that at what point does it make sense that you're always assuming that similar values for your features is what you want it is a felling only of the dot product hardcoding the idea because I tried to highlight them in orange drive three chances to learn parameters reminisce could reflect all of these different association so not only strong things will also you and you see this in visualisations of the learnt weights that you can have learnt to amplify important instead are not informative for the final test like again the data are speaking very loudly I work for the dart just mention if you had a girl I mean what I've done there with those two views I mean I just picked to that seems kind of Representatives of strategies people use my personal opinion is that the literature on it so many very open up a newspaper and see that people have tried a different view of attention it's hard to know 4 differences are incidental implementation details alarm to think Like There's kind of these two classes global wish I could pretty hardcoded values about what attention we should be which one which is like very free learn what's the attention so long it also played in local attention instead of looking at the entire premise when forming whenever effective attention when someone a form I look only at a control the window cobalt in situations like machine translation where are you have lots of output States and so you're kind of moving through the hypothesis different local when windows in the Prem source and target Lang language modelling a dial RNLI kind of seems like state that really matters and then you might as well tend to the entire turn that down here buying has become a professor primary source of connect before getting to that so word by word attention 30 here can be set up in many ways and you can have many more learnt stream case of that this road test foundational in showing and attention is powerful for nli play seem to have taken the view where are the matrices that you have here you have associated in lot more we use that would be an here the same into do the hypothesis in fuse every state that you look narrated version of the entire you couldn't as you trip through the hypothesis append attention weights from the premise at each time step that's another variant the one get all Explorer things like the transformer so a recent development which I think will talk about in a couple of framing like the one I've taken for this lecture memory connections are the Ireland dislike additional layer connect paper I think this is the one that's called the attention Is All You Need there is that drop the basic recurrent network connection WhatsApp attention Mac that's doing it kind of just saying point that I'm at an example the Explorer connections to my neighbourhood throughout the whole exam is there any influences anywhere around resolving my problems with a very open-ended wave exploring the data it doesn't even in code a structure that you get from an orange does it have to be really Powerful find alternative more indirect connection but still interesting the memory network got a few years ago similar to attention mechanisms in the sense that they're trying with additional augmented representation the fact that your network might be sorted forget mechanism there is quite different to hear you have this kind of memory store represent traditional 10 it's very technical stuff hope you feel if you weren't already prepare to dive into the literature it will be rewarding I think it's a very clear result of this point that for lots of problems in an hour final face here I wanted to show you somewhere analyses to get some new possibilities for doing this so Cortana light annotations here I've given a few examples the annotations are on the left 16 categories for them doing is just giving you know that humans important semantic properties of the examples of this one is modo incorrect because in here and it's sort of garden ennafoura connection between students of human misery study tense difference conditional active passive vs active passive new programs to increase efficiency and programs to increase efficiency were consolidated surprising that's a common strategy for people who are constructing example really interesting to ask of your sister is learn the kind of human linguistic thing that active-passive variance preserve some aspects of meaning example of something you might do use some of my own Amazon cry a bunch of experiments on the cloud just using the course code Direction with cross product features and I explored like the regularization strength and the penalty type of 1LT St Helens I did a chained one play version we have shared premises across premise and hypothesis sentence encoding call that of that notebook that I showed for the embedding dimension the hidden dimension Aldi's in the learning rate and the activation hope you have given the performance on the thousand or so edited examples regression with best and then sentence encoder side here I noted I wanted to store all the model printers so that I could test them on your cases alpha logistics regression is over 600 MB has 16 million feet feature space is really large with the lstm where the model files each atom MB difference if I wanted to run these models on a phone I just can't even just aggression right but these mammals down here nice things to keep in mind I should have checked concerned with the absolute perform to bring into the mix for an air analysis but I was really interested in it's just absolute performance where maybe logistic regression is best a different according to these different annotation category 16 of them side group in a few ways categories where are all the models are more often correct in what's the colour of high-level summary in that active passive belief reports conditionals long sentences modals and again a lot of them where they do incorrect kiss for all the models were more incorrect ankle no matter what your model the hardest in this quantity time doesn't surprise me because I think models and also the future spaces are just not sensitive to this low-level kind of stuff it's very saddle this is a classic case of setup symantec morphemes having very large consequences for what the overall sentences we're only the logistic regression with more often correct and incorrect the other models were the reversing that's coref intense next to me in the sense that the word product model festivals keeping track of all of these pair how many standing rather normalization it also knows about differences in tents has this enormous space of features kind of pretty well attuned to the fact that you might have like was that directly represented whereas struggling because they probably end up with pretty similar representations for the tents very turn a lot of the same in Byron what's similar story for the correct only the chain lstm was more correct and incorrect and this is word overlap that's kind of surprising to me I don't have any if I had added some attention mechanism I would have thought that the logistic regression will be really good at about finally here the cases were only the sentence including lstm was more incorrect incorrect the other two models were better right this is our worst model remember and this is paraphrasing quantifier initial input embeddings work random or something no outside infirm since the logistic regression model can benefit from glad vectors at least not in the way that I implemented it I would also not give the other models that is that if I brought in rich initialization than the St Annes would just jump up more information at play spirits by the industry players the day mentioned this problem would be comparing lstm with and without petitions for 8 words cancel it's going to get eliminate like you probably even have some expectations there better at things like that it overlap antonyms baby believe in again long sentence my sing so another thing that you can do to some linguistic expectations that you might have you could ask for example I don't know that location is downward man right consistently that final rain until the final move didn't remove entails photo didn't run I mean diamond 10-minute Direction tell me the net I mean for models courses that any example that you plugged in play where it got this right if I do just had an occasion it should get this experience these models disappoint be disappointed if you try something a little bit more sophisticated I know that every is down with monotone on its first argument and upper Don it's x romantic factory stick is very clear probably one that have the systematicity that leads you to think that yes this logic off examples where leaving in Accord with the generalization and others were does something quite crazy if it is performing well assessment like I think anything just by way of wrapping up dimension before that bill as part of his thesis did lots of rich work on natural analyte task logic has the characteristic that it has very sophisticated little algebras of semantic relationships 5 summarise one here this is a kind of navigation can I have P and Q are disjoint like vindication of those two are new this whole table here strut artificial datasets where you just insist that the lexical item so just semantic relationships like internment consistency overlap contradiction define the algebra on top many examples as you want in a algorithmic sash years of the class we've done this is a homework prob Frankby semantics MOTs could ask if you train your model negated dataset an artificial one that you create like not not not not q generalize to the Tripoli negated cases it should right it should have all the information that you need at that point because it seemed the best cases navigation and it should now know that double negation just kind of flips you back another systematic space in this little algebra but again your mother has fully there in the shower to brand new will see you there ask it to make predictions about longer and longer this is a nice way to stress test your model and I've given a few examples of papers that I think do this quite well including knock knock knock you not you not you not you or just on not he not knock you quite like the on the Gator cases the single e and the double that's giving your model a fair chance to have seen all the relevant kinds of pads everything at needs now to generalize to triple quadruple models completely non-christmas bottom one language to another infinite were part of the same family English English language probing artificial data separate from any intuitions you might have about the gate more like testing the sentence which doing something sister cognitive science like has it really learn to abstract from your data iPad as embodied in this algebra up nice because since you control all of the model in the day love it come in good sense for whether you post this an affair are you inside a 2 watt you're models actually capable of learning general an interesting linguistic question of whether turn on one language would transfer to others remnants of people trying to do that and then for an Ally because that Facebook group released a version of multi Analyser that is Multi pursuing the hypothesis that there might be some abstract layers of represent multiple seeing things and saying it's better than you how to say everyone what's the main way of asking this question is just right things in Simon answer is yes using because it's like a Samsung has become instead of having humans come into the lab where you probe them behaviorally to see what they know black box mod play papers that are trying to push this pass you have another model that does local look how as they pass through the network activate different parts of sailing to what they actually have encoded it still comes down to this kind of behaviour really bad just means that you may not want to get rid of human subject agents play some optimism that attention is not only can a performance enhancer but maybe also some can explanatory what is my model of you paying attention to and maybe get something I just saying that attention actually is not any good for this it does not explain things I think there are different opinions would be let me give you two beautiful first four nli at the list of the different attention weights that has learn on word by word what kind of reflect what you would expect that certain pairs of words across premise and hypothesis are really informed classification looks good you like I was really excited when I stop people adding attention mechanisms into these what like it was a soft part of the example like independency Paris would be really cool because maybe you'll be able to read off of the attention weights Paris was or maybe with the top most likely purses for this other and swear That Dream real one way what you're going stop for about attention when we talk about big party material will try and his last few minutes to do the ambitious thing I've shown you too much about these grounding slides stop grounding and what I did is I'll give you a bunch of linguistic insights about why grounded language understanding is import speakers listening another mind just say that speakers that would be like what I've done for a landslide this focusing describing colours because version of a very large class of interesting problems and colours are great because they have like cognitive complexity and linguistic complex you could think of models that are learning to describe colours for that and this is a knife to confront comes in and then you need to produce a review is the converse right so it's somebody gives you a colour description and you need to make a gas about what colour their described tingly different the inputs are different and the output space is a little bit harder to deal with chatbots you probably heard about chatbots are often kind of etsy because they're just using language and producing language and like that like actually connected to any problems or any social situation is Facebook paper On-The-Go shh where you have chatbot play the gold it's very exciting inside just tried that present their model look for that's a really cold day other Minds ice between pragmatics machine learning the interesting thing about that is that again you have a bunch of really interest Barbados really well prepared to think about all of these models you find if you look through you'll see that because all I can align models wear instead of having to Swanley for every stage in your height colder part of the problem list of what you guys have already please call dataset how much more detail about that on Monday but if you're still casting about for Project through this and just maybe folder links to various status for now that's been enough can't 