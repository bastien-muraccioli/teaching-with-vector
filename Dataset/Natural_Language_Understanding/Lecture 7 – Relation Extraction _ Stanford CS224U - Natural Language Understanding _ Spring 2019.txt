tighten up the Bake Off happened last week what you doing distributed distributed representations using words summary data sets what's the word pairs with the social human irritated somewhere is pollution measure tears in your chosen BSM first round scores two bumps on Lower End when on the higher end higher Ed what Kristin was calculate observed over expected scorers after separating all the big of entries into the top scoring teams into the bottom screen on the last year we have I like more likely to appear for top scoring teams and another carry on the other hand will likely appear for the ones in Battle frontier that helps you get an idea of like what distinguishes channels from tomatoes names like Richard fitting on wordnet and using LSA or comments to build better models what's this first place team a play basically start gpmi tutor Halloween colour score 80 you place bedwars but it's plus retrofitting with a Subway so they are so proud on a bunch of things pmit pasteles do not work as well this model for last you interesting how they do it like sorry did 2000 Ella say together but I think the thing that made them Jakarta is a disc teams are had the default which I think is closed so that's like an interesting detail there good to see you like we swap out your car still interesting results I like if you're not doing too well on your models for these big office still interesting to see like how something that could be still feel another category polite talk the most Apatow real-time rate go hard to like can I buy things with a ring a lot of tests that they had like 10 cells and Mikey still have a look at different experiment and I did like a lot of hyperparameter tuning but it could be are some low scoring teams that play in effort special consideration quantified yourself in terms of computer embassy to the real professor Shay I have a couple of announcements before we get to our main events Bake Off two has began so there was there was a post on piata within the last hour or so house on how to do that so make sure you take a look at their post big off to closes on Wednesday so you definitely want that's a diet tomorrow Friday special session go to numpy and pytorch tell that they are in this session is going to be recorded Wednesday Wigan have some guests so young news from Romania in murderer Katherine's who's from Google research looking for a panel discussion on nlu in industry due to attend in person if you can't am I good good show of support from from Stamford and to have I think the value of that session what's the interest in interaction and if you can come to the session specific questions that you what is it really like use nlu techniques in industry in an applied that the stuff that we've been talking about in real world specific questions or topics in mind there's a Piazza post encouraging the to think about this then you can add your this question as a comment on a PR visible panel discussion probably won't take the the full hour and 15 minutes next time so will use the extra time for coding and class focusing on homework 3 and Bake-Off 3 place an extraction that's what we can look at today but having some time in class for that on Wednesday will be particularly valuable last time to prepare before that homework is doing please result play sure exactly today I want to turn to a new topic which is relation extraction two big ideas that I want to introduce you to what is the Task of relation extraction itself this idea of distance supervision relation extraction is in interesting topic for a bunch of reasons that I'll say more about in a moment interesting interesting term for US in Canada the nerve that were developing is it it's the first time that were looking at an nlu task the output of the task is a discrete object a numeric value add sentiment analysis in the output a scalar value it might be like 125 veritas that were doing for the homework in a big off it's just - 10 or plus 1 sentiment analysis is just 0 or 1 it's basically binary classification but in any case it's a scalar value for that we looking better space model the meaning where the output is a vector a real god traction for the first time we can look at it test with the output is an object in this case it's a relational trip a row in a database essential something new and couple of weeks from now with the task of semantic parsing where we go even further in that direction and were looking at outputs which are complete logical form so that have real complex structure to them relation extraction is one big idea the other day is distant supervision and this is interesting because it's a strategy for data bottleneck that has been one of the biggest what's the goes to progress in applying machine learning methods to to NLP and nlu of distant supervision enabled people working on relation extraction suddenly to be able to use 100 times as much data as had ever been used before for relation extraction and that enabled vastly greater statistical power in the in the models that were trained explain how about works in a moment but that was a big innovation that applicable not only to relation extraction but other kinds of problems as well is it the innovation here is not so much a new kind of model it's not a new neural architecture or a new loss function or a new optimisation algorithm instead the innovation is in the kind of supervision to provide a signal on which to hillclimb Christine because listen to revision work develops right here at Stanford so the idea first came about in this paper by Ryan snow eggs are in 2005 Ryan's no was a PhD student was advised by dangerously home some of you may know incubated here in 2005 tended by hello think I had a reference to Minster all right here distant supervision for relation extraction without labelled data that was also in danger askies research group in Stanford NLP group same Strand of work lead eventually to a generalisation of the idea of vision which what is the idea of data programming developed in Chris rays group some of you might know Chris Rea who's also professor in the computer science department here snorkel which implement some of the ideas of data program a generalisation of this basic idea or strategies breakthrough the limit imposed by needing supervise fully supervised data for training a little bit about the task a relation extraction is it is it big enough release extraction is to extract relational triples from natural language texts I mean things like these examples here so founders turn on mask killer Riley workout you and musk Tesla dresses a fact about the real world is three components there's a relation do entities have these underscores in here that suggest that there unique identifier of these entities and I'll say more about that in a mo the relation is one of a small number find relation that were interested in why is this interesting why do we care about this world we can accumulate a large collection of these relational triple baby's of facts about the real world building a knowledge base slow and expensive and time consuming web or other large that collections of documents are filled with these kinds of facts already so we can figure out a way to extract these facts free from natural language to the Accelerate the creation of knowledge bases why would you care about that why do we want to have these large knowledge bases it turns out that incredibly useful first because portion of human knowledge can be expressed in this form because there are abundant commercial applications for an hour's basis like this flooring depot my current boss John giannandrea earlier in his career founded a company called metaweb co-creator of freebies is a very large doesn't actually exist anymore which is a terrible shame was large community author which was essentially a vast collection of relational triples like this required by Google in 2010 it became the Foundation of Google's knowledge graph ^ variety of different knowledge applications including questionnaires ring and also the knowledge panels that you sometimes see on the on the right hand side Google Search results it was a really powerful idea a couple of years later in 2014 introduced its own version of an autograph there's told satori play the same thing also matters for apple in 2017 apple acquired a startup called ladders founded by Chris Rea from the same computer science department and lattices business was also to do relation extraction from documents from the web in order to build and extend knowledge bases that in this case are helped to use to help Siri better answer things about the Wirral so there's abundant commercial interest in building knowledge bases like leaves other applications that might be a little bit less obvious extending wordnet wordnet is basically a knowledgebase of lexical semantic relations the role of entities is played by words or actress in sets relations between them are things like the hypernym relation with the synonym relation or the 18m relation you can task of adding new stuff to word that has this phone B should be in wordnet but is that doesn't know categories and subcategories in subcategories of video games and wordnet doesn't know about others different categories video games and the fact that this one is a subcategory of that great to be able to automatically extend word not to have that kind of knowledge change that this technique of distant supervision was introduced by Ryan snow in 2005 that was actually the application that he applied it to was automatically relationships between word senses by extracting and from natural language text other applications for a double in biology every year there are thousands and thousands of biology research papers published do with a gene expression and gene regulation networks we can make these if we can apply these techniques to papers maybe we can run a relation extraction focused on jeans Gene activation over the thousands of research papers and extract format lots of relationships between different teams whether they have activating or suppressing effects on other genes big graph of relationships between all these different genes and then I'm not biologist so I don't know what you do with that from that point but they can be really useful to biologist for trying to understand these complex networks of relationships between different genes sent locations in why you might be interested in this task so how do we actually do l turn examples here we want to learn some facts about Elon Musk active from natural language actually do that techniques that may come to mind first is to counting patterns that Express that were interested in so for example a pattern like X is the founder of y lyrics to found a relation so maybe we can just search a large corpus for occurrences of that pattern the XY pairs we might find Elon Musk is the founder of SpaceX a new triple to my knowledge base that seems very straightforward in fact approach was the dominant approach this for the dominant paradigm for relation extraction for many years like during the 60s and 60s and 70s early days in natural language understanding try to be slightly more sophisticated by instead of defining an exact string and looking for exact matches on at string defining patterns that could be like a regular expressions to a comedy some minor variation of things like verb tense in someone never let it with the basic approach it doesn't get very far and it doesn't get it very far is it language is incredibly very there's just a million different ways to say the same thing It's Like These interesting that you must co-founder of PayPal when on to establish basic space exploration currently SpaceX founded by PayPal Pioneer Elon Musk city is blah blah blah is sentences expresses the fact that you are mask is the founder of is match the pattern that we proposed x is the founder of y nor any other pattern that would be obvious at the outset right you wouldn't have anticipate is exact patterns in even if you imagine That You Lie study by making it a regular expression instead of an exact match there's no way you would have been participated patterns once you seen examples you could say ok what I'm going to create new patterns I'm going to add to my parents by having a pattern that captures parentheses founder of y Apple maps to more examples somewhere else in my corpus and then I can add another pattern which is I'm a co-founder of PayPal went on to establish why actually replace the PayPal with a star so that I get some more generalization these things are really going to help at all these patterns are still really specific that many if any other examples in your corpus they're not gonna generalize well adapting this approach is even if you manually engineer even if you write down patterns any try to make them general using regular expressions skip with a system which might have high precision inescapably going to have very low recall there going to be lots and lots of is which Express a relation match any of your patterns and you're just gonna miss those you're not going to have the opportunity to do those extract strongly suggest embracing machine-learning and this is what happened in the in the 90s and 2000s statistical Revolution that came to NLP and people said that used data and machine learning you learn the things that we want to learn in addition feature representation that will allow much better generalisation what's that we have to do this we're going to have lots and lots of supervised data to trainer model we do is go to a Corpus in every time there's a sentence that has two entities in the Entertainer relation between or if there's no particular relation we just said no true and we find the settings and we say you are musk and SpaceX and we do anything that was something that says founder second sentence in the third tense with something that is found might also be sentences in a Corpus like this one blah blah blah about Elon Musk and SpaceX actually Express the founder relation is not saying that Yvonne is the founder and so we've mainly in this example with a label that says no relation big corpus all pairs of entity mentions are annotated with a relational between them or no relation we can train a class we can define a future representation each other's Paris going to be mentioned maybe it's just the bag of words in a sentence we can train a classifier using techniques very much like what we been exploring for sentiment analysis it's only works and it was a huge step forward for relation extraction it allowed us to build relational traction system better performance than the handle patterns that came before and particularly much better recall 4 is the Big Show coming as well which is that it relies on having this manually annotated data 4% manually label everything and that manual annotation is slow and expensive and it doesn't scale when this was the dominant paradigm in relation extraction during the 90s and 2000s data sets that were constructed for relation extraction had on the order of example a dataset with only thousands of examples seems kinda small not enough to do modern methods to innovation that I I wanna gets excited about is this idea of distance supervision this is the big conceptual breakthrough to manually edit 8 entity mentions in a sentence an external resource source of Truth particular working to use relational triples from an existing knowledge base automatically identify extraction examples in a big corpus we already have a knowledge base that contains this relational triple founders SpaceX Elon Musk who is we take that pair of entities space x and Elon Musk large corpus for any sentence that contains those two entities unreliable assumption expresses the founder relation we effectively label each and entity pair each example in a sentence relation label 8 of using this external data source the knowledge base labels in the corpus Addington manually label all of the examples in the corpus have sexy one last time more than one label to and will come back to the question the moment because it goes to hell you formulate the classification problem is it a multi-class classification problem we have to choose just one relation out of a bunch of relation multi label classification problem where you can assign as many labels as you want can you do this on something limitations of this approach is that because it starts with examples of the relation that you care about you can use it to build a knowledge base for a completely new relation or add a new knowledge base turn to do extractions for the founders relation I need to start with some examples of the founders relation don't have any tables yet or need to use a different there will be more bootstrapping is an overloaded term turn on please basically the same idea if you read you might have might be referring to the Reading from djerassi and Martin bootstrapping so it's basically wave play idea and making a little bit more rigorous stuffing ideas presented interesting Martin the seed examples the same two balls are just chosen by hand you just got to think about it where is an example of a relation and you come up with a very small number like maybe a dozen from there is that shopping is presented by Jurassic Martin it can be a repeated cycle where you use the sea to bowls actions from your to identify some examples in your purpose do you identify new patterns intern can help you identify new two balls the process can repeat provision we're not repeating a process we start with a much later much larger set of C2 poles hopefully if I this is already kind of large in Argos extend the knowledge base to see two balls where we're not sort of going around that psychotherapy please before I go on put Radio fashion like that sentence setup every pair of entities Kennedy extraction the entities in a sentence there are three pairs and you can consider each one of those three pairs and you know if there are more interviews there you have even more potential pears and each one can be considered external knowledge base as a source of supervision that is the central idea of distance supervision and it's a really powerful idea what are the reasons it was such a game changer for relation extraction about 10 years ago to suddenly use 100 times as much data as it ever been used before the largest data sets that were fully supervised had thousands of examples stroke this idea allowed us to allowed Minter of the authors of the 2009 paper to use 100 times as much data 100 times as much data to define a feature representation that had much more precise to a specific pin Point features add enough data to overcome the problem of sparsity for those very specific call dem to get much better power from the from the model that they were built set time they wrabel to achieve 100 x gain I mean there's no there's nothing in principle that limits even larger scaling-up bad temptation resources available deadtime today we can probably easily go to 1000 times or 10000 times as much data who were using for relation extraction in the 2000 successful idea is on making this assumption sentence I'm starting from this tripling my knowledge base founded SpaceX Elon Musk I go to the corpus I can't every sentence where those two interviews co-occur every sentence with those two energies co-occur expresses the founder relation it's natural we know that an unreliable assumption assumption in a fact I just showed an example of a sentence press the founder relation provisional is going to label that as an example that does Express the founder relation play Doing it I'm essentially injecting noise into my dataset the creature A Challenge for the learning algorithm it harder to learn when you have noisy labels advantage is more than outrage having vastly more data and so even though your learner has this additional challenge it's still able to learn much better models limitation of this idea the one that was already mentioned it can only be used to extend an existing knowledge base or an existing because it relies on existing triples from the database and the knowledge base it can be used to create a new one from whole class so that's the the biggest idea if you only got that from today's session I think you would have already gotten something that eyeball what I want to do with the rest of the time is to walk you through the rest of this cold to help prepare you for looking at homework three and Bake Off three which are going to be due set up stuff we're going to import some useful is it in libraries so the data directory for the class including subdirectory called relaxed data things in there there's a Corpus little bit more about each other cancel the default setup then this kosher work for you at the box if you put the data directory somewhere else you may need to fill with the line that has it where to look for the data who did Athens to talk about the corpus and the knowledge base as usual whenever doing nlu we start with a Corpus of large collection of examples of language kiss kiss there are some specific things that we need from our corpus examples that contain two entities that's the whole idea and it's going to be really useful if those entities have we have what are called entity resolution solution-based retakes and Anthony mention in a sentence that refers to an entity a unique unambiguous identifier it's going to be really useful because we're gonna want to connect the corpus to switch is going to use the same unambiguous identifiers making the identifiers unique and unambiguous kind of souls to problem a policy which means how is amuse when one word or one phrase can mean two different thing here is New York New York Maine New York State or New York City resolution correctly does a map into two different unique identifiers New York City will have one identifier Newark state will have a different identifier and hopefully something in the context around tell me which one of those it should map to show me the other problem the tsunami when you can have two different expressions that refer to the same entity city in the Big Apple both preferred in York City hopefully those two different expressions or both get Matt to the same unique identifier the resolute the Revolution is a whole topic unto itself people near work on developing models for doing entity resolution properly print unpack in his class but we're not going to do that we're going to cut a treat it as a black box is already been done correctly this project what were going to use for a Corpus examples of derived from this wiki links Cork close the result of collaboration in 2013 between researchers at umass it's drawn from really large snapshot of the internet 10 million webpages altogether Google there is a Google has a terrific entity resolution module in house render centre resolution system the web pages basically recorded the resolutions that appear in the WikiLeaks are not the result of human imitation of a model that's just running over webpage if you wanted to you could the resolution component from scratch and then you don't need any annotations on your data at all you just start from text from the it's a little bit easier for herself without leverage the output of the sensor resolution system from Google WikiLeaks we want to have text around a 2nd mentions so would like to have the text before the first mention the text between the first mention and second mention any text after the second mention text is potentially relation is if you look back plus that I gave the first example Elon Musk founder of SpaceX Neil here the word founder that strongly predictive relation an example you get this word established that's in the middle which is strongly predictive an example where surely the most predictive thing is not in the middle but is outside maybe the example comes later I want to have that context around the around the 2nd dimensions and umass version of the wicked link-status it actually has that context help settings up to be convenient for for the purposes of this class we massage the data we filtered the data to make it small enough to easily loaded into memory in and to work with find up with is a compact corpus that's going to work really well for for April corpus class that provides an easy access method to the contents of the corpus so I want to give you a peek of some of that and at the same time do some data exploration opposite to memory just use this call here we can start to poke around and see what were working with so the corpus has examples every example is 20 mentions in the context of a sentence 332000 example 20 examples but still small enough to easily work with a memory take a look at what an example looks like so just look at the first example here play scrolling little hard to read who is basically a 2 bore is if I remember correctly 12 two Fields are called entity 1 and entity 2 call St one is New Mexico Institute Arizona the unique identifiers for entities that I mention New Mexico has an underscore in a kind of suggest that this is not just an English expression this is actually a unique identifier in some kind of energy space and in particular use our so-called wiki ID the last part URL for the Wikipedia page corresponding to this thing and serves as a unique identifier for New Mexico and Arizona in this case the next 5 Fields or Fragments of the text from this exam Leftwich is all the text to the left of the first entity mention which is the English tax so here is New Mexico without the underscore cos this is the actual fragment of text sing between the two men West End listen to Arizona and I have a right which is everything to the right of the second mention Pistols together you can kind of get the the whole settings where is still used in parts of New Mexico Arizona show the Fire Inside the corner and sofa in the last five Fields similar to the previous five fuels accepted Boris speech has a slash after it and following the slash are cabs to DT jj&s NN in self-worth that indicates the part of speech and again the part of speech of been generated by an automated system by Armada too much about the meaning of the different parts of speech they come from the Penn treebank annotation scheme 40 or 50 is you can sort a figure out what they mean down plural noun things like that but you probably don't need to worry too much about what the specific things mean here is that it could be useful in building feature presentations for your model and you may find homework questions asked you to try that whether the part of speech information better generalization capability musical instrument which actually pastes the left mention one middle mentioned to Andrea all together to sort of reconstruct the original settings data exploration just to get a better sense corpus and by the way this is good metal or metal logical practice whenever you start working with the new do some parking around and figure out what's in their trying to try to look at some summary statistics of what the data looks like the little bit code to extract all The Entity mentions and then count them up ent by frequency geography things this list you might think it was only geography play the case but it certainly looks that way right here the benefits of having a corporate class we can do some indexing and make it easy to retrieve examples having specific a method called show examples for pair easy to look the container particular pair events examples that contain Elon Musk and Tesla Motors list five examples like that and a princess list of those 5 examples Direction matters here so this isn't necessarily all of the examples that contain Elon Musk and Tesla it's just a vehicle to have a must internal identity too it could be the other way around so it's also look at show examples repair Tesla Motors and alarm there's two more examples that have be conscious of Direction looking for examples to have Elon Musk and Tesla Motors we need to check in both directions for making us calls does have some flaws one floor that you'll discover as you get further into things inside parking around more is it there a lot of examples that are near duplicates of each other so not exactly the same but with minor variations like only a few words different or something like that this is a consequence of this this is this comes from the Winx Davis consequence of settling strategy that was used to choose the 10 million web pages the dataset was constructed I don't know or I don't know exactly how they did that lots of near duplicate documents on the web pages that different from each other only buy a little bit and I think they didn't try to Windows out to those near duplicates anyway working results that will run into whether it's a floor but it's not a show stopper for this dataset Jamaica is a discord doesn't contain any information unlike the supervised paradigm explicit indication enniscorthy hold between The Entity in the Scorpions that tells us nothing in the English word Elvis Elon Musk is the founder so we get that we're going to have to connect the corpus to the knowledge listen to KB now before I go on any every single game turn the lights on would you get a lift to work it was that were where does Eddie's in the Wicked interviews that were recognised by the energy resolution system that Google that's a possible variation in fact earlier I mentioned to work by Snow that was used to extend word now notion of like what were the units of extraction in a set of entities they were they were looking much more general nouns and try or noun phrases relationships between noun phrase the knowledge base now the knowledge that we're gonna use is a fragment of freebase I mentioned freebase really likes this sort of community constructed knowledge base of relationships between entities very sadly shut down in 2016 but you can see if you pop around in the internet you can still find data dump and the knowledge base that include here from a free base stage play a relational triples that we've already looked at each one consist of a relation a subject and object an object that connection to subject and object in a synthetic sense in the sense of English Centre military designation for the first argument of the relation and a second argument to the relation relation the thing in the first position is one of a small number of predefined relations like place of birth or has valves entities that appear in the second and third position again or with the idea so it's like the last segment of a Wikipedia URL and a constitutes unique identifier that we can connect to the identifiers used in the cork just like we did for the corpus we should do some data exploration and see data actually looks code to load the KP into memory we can start poking around there is 46000 KB permit to work with team relations space Wikipedia freebies the original freak like a fool free base has way more than it's a free base has relations millions of entities and literally billions of relational triples what were working with here is a is a fairly small slice of that whole knowledge base I want to know how big is he formation how many triples does each relation have to leave counter them up results here it looks like the contains relation is the biggest one it's got 19 thousand and some of them are quite small so the capital relation has only 500 very quite a bit in size prince one example from each relationship so you got a sense of waiting here France Spain pretty into Silas Sheridan le Fanu capital of Panama Panama City I think he's a pretty intuitive for the most part to point out is that some of these relations are intuitively symmetric relation so I presume that a joint relation is symmetric a joint Fred Spain is in the knowledge base then probably hopefully Spain France is also in the knowledge base no guarantees in theory it symmetric but nothing actually guarantees that the inverse relation is actually in analogy relations are intuitively asymmetric so author Uncle Silas Sheridan le Fanu retrieve that the relation was defined in that direction there's no reason why I couldn't have been the other way around so I could have been instead fine the representation of choice that was made here was to put the the author first in the war and by the way that's also point out that relations frequently or typically have a relation will have a type for a first argument type for a second it's like in the corpus there is a convenient better that makes it easy to look pause the contain a particular pair of entity so we can look in France and Germany it looks like this one there is a relation a joint France Germany and we can also look up Germany and France it turns out relation is in there that's great we can look up the relation relations between Tesla Motors and Elon Musk we get the results of the founders relation holds between Tesco and my mosque I could have said some of this stuff already we can look up things the other way around we can look at the relation between Elon Musk and Tesla Motors there is a triple workday Tesla Motors so that can make sense this point address is a question like an Aprilia there can be more than one release given pair of entities so Arthur and Tommy Lee blah blah blah has sibling relation in usual but I just things were different the distribution of entities in the kbc here we go through all the triples entities and list them in order of frequency can you see the it's dominated by entities that are related to geography exactis look like this look like looks like it's only geography but that's not actually probation there's no guarantee that the KB is complete this knowledge base has founded SpaceX Elon Musk it has founders Tesla Elon Musk it has worked at Tesla Motors does not have work at 11 musk SpaceX he did and given the other three you would think that it should just so happens it doesn't in fact that's the whole reason that were doing this the whole reason that were doing this is that we want to an existing knowledge base with facts that should be in there Audi complete with wouldn't have any work to do so the fact that incomplete is no surprise and in fact it's the whole motivation for doing next question to look at is how we gonna formulate the prediction problem that were going to undertake and there's only two questions here one is with the input to the production Prague play output input means what is the that we're going to try to make a prediction about sick formulation of the relation extraction problem with to say the input is a pair of entity mentions the context in a specific sentence dimensions next working and try to predict the relation between them which is to say the unit of production the input to the prediction problem just a pair of entities he mentions but just a pair of entities abstracted away from any specific can't is Whitby output of the prediction in his toys back to the question that came up earlier are we trying to predict a single relation that holds between the two entities can we put it in Multiple relay example of has sibling has spouse both holding between 11 what's nothing approach to relation extraction first answer to both of these questions today the past it were going to pursue chooses the second answer to both of these questions and I'll spell it out a little bit more as we go the talk about the first one how do we formulate the input prediction problem and it comes to the question of how we're going to connect the corpus in a KB possibilities private earlier it kind of sounded like use the knowledge base as a way to generate labels on entity mentions in context and it's similar to the classical approach to relation extraction we were going to do something else instead where find our problem as the problem of classifying not a pervert dimensions but a pair of entities we're gonna use all of the examples from the corpus 2eddies co-occur feature representation classification Prague classify mention of Elon and tesla in a specific sentence all of the senses were Elon and Tesla co-occur to generate a feature representation plus make a prediction about whether Elon and Tesla apart from any specific context are related to me Horton that sinking let's the input the prediction problem will be a pair of entities considered apart from any specific context feature representation use all of the senses in the corpus with those two entities co-occur for those features so that means that we need to the the wiki ideas as the waiter join mkb continuing idea data exploration we can look at how many examples we have what's the we can go through the triples in our knowledge base we look up all the examples in the corpus that contain those two energies we can look at in aggregate or on average over the over the knowledge base how many examples we have to support each trip prizes that look for a joins there are 60000 examples altogether 1708 means there's a lot of examples for every triple about an average about 34 examples for every trip lots of evidence lots of examples to draw features from an extraction model for this reasons for the number of examples is much smaller have a number of examples for triple of like 1 and a half you have a lot less evidence in the corpus to draw on everything we need Chinese models and that is negative examples the connecting the the cable triples to examples in the corpus is going to give us positive examples for training a model get training model from positive examples alone need negative examples as well negative examples for each relation use pairs here together in the corpus evidence for the features for them where's identities that are unrelated according to the knowledge appear together in any Kirby triple pairs of entities unrelated have sentences that they co-occur written code walkthrough and find all of those unrelated pairs boss William Randolph Hearst the cat's meow Les McCann Bobby Timmons Chateau al-khattab I don't even know what most things are but they appear together in a certain somewhere and yet according to the knowledge base is ASDA negative examples for a classifier is all related to the input side of the prediction problem on the output side we have this choice to make about whether we're doing multiclass classification or multi-label classification Inigo is multi-label classification important motivation for that is actually quite common in an average pace a particular pair of entities to belong to more than one relation where is out one example the one with Cleopatra and has has spelt and has similar Brighton Coach countdown song over lads that are really common in economy Intuit it's really common for a pair of integers to belong both to is that makes sense that's like Einstein is a physicist and his profession is visit apple in containers that makes sense as well place of birth and place of death there's no necessary relationship there but in practice it's quite common for people to die in the same place they were born so that's not too surprising further some debt are Southend Paris at least one has sibling and his spouse it turns out Cleopatra's not alone cases overlap is one case where we have patience nationality place of birth and place of death looks like noise parents in work day two examples of those overlapping and I think that's in jail play The Air that's enough formulate a problem as multi-label classification this means that we can assign racing steward giving St-Pierre me different ways to do multi label classification but the simplest way to do what's called the binary relevance method in the Bible relevant method basically mean kitchen problem into binary classification problems so if you 16 relay what we're going to do is just factory into 16 separate binary classification relation Jason Orange binary classification for each of the 16 relations you're smarter ways to do multi-label classification because this way of doing account or in this case the relations can be correlated with each other it just assumes everything is independent simple way to set things up and it will work well enough for our purposes think about a prediction problem is a Kennedy Kirby triple terrific relation in entity 1 and entity 2 or is it or is it not a vowel KB if it's not already there that's the binary classification problem that were reducing this problem nearest encode to create datasets and not gonna dwell on this basically is going to combined I did he said he's going to be a bunch of KB triples labels that say what binary classification for that KV triple is thing to be aware of is that because were factoring or prediction problem by relation play structures are going to be organised into maps stop Beeches by rail a collection of KB triples that have been grouped by release is a dictionary where the key is a relation name who is a list of Katie's still be a list of cavities for this relation and a list of DVDs for that relation first thing here the second thing here is exactly parallel data structure labels for each kbt it's going to say whether that thing should have a true or false ring made for it I said here build Davis going to come by negative data it's going to derive the positive data from the KB Drive the negative data in the the way described by looking for pairs of interest weather in a Corpus but don't Coke positive and negative data there's a parameter that lets you control the sampling is 0.1 because we have about 10 times as much data as we do positive data so using the same sampling rate will give you roughly bound status it also let's you specify arrange MC2 that you can have reproduced for results what are you doing playing the guitar for some point not sure if I understand the question are you talking about words that you might use in the future representation as predictors like you play music for yoga yeah and so have you I think you're asking about the words that you would use to the way you construct a feature representation try to make a prediction in do you need to account for do you need to resolve ambiguities like that one in constructing the future representation interesting you could play with I think the typical answer is no we don't typically Friday December duration before constructing a feature representation I could use some some benefits do you have a we would hope that with something are you missing that a relation that appears in your I miss that the the this should if it's a well constructed no trace it should ambiguity aiming for with the knowledge base is that it's completely unadventurous that how to speak about this a little bit earlier in connection with the entities that only have an entity ID that shows up in an orange face we definitely want that to be unambiguous we won't have City we won't have an onion big us entity ID in our knowledge base so there's no question about which New York were talking about who is truth for the relations that we don't want the relations to be anyway going a little bit about the approach to evaluation into a really good idea to define the way you going to evaluate your systems before you start Building Systems is kind of the same idea urban development in software engineering so first you set up a test harness that allows you to measure performance and then you can start building your system in iterating on performance having a test harness set up an event means that you can sort and measure progress as you go rate for step a random classifier that only takes 5-minutes to write but gives you sort of a baseline on which you can begin to measure progress two evaluation typically starts with splitting the data into different pieces play some advocate play split into a tiny split a train split and a dab split the tiny split is only 1% of your data it's also going to be really fast to train and tear in America having a tiny split is that you can do your early development on this tiny split and I can help you like just for a flush out bugs in your coat I just make sure your code is running before you scale up to training on the big test training and testing on the DAB split which can take a lot longer to run get a Greek mythological practice that I encourage you to use another place about three quarters of a date at the devil is going to be about a quarter of a day that's a pretty typical set typical train on the train split and test on the Doves held la test reviews for the bake off so you don't have access to that yet but it exists you know keeping it in a in a vote Underground for now I'm going to skip this person discussion but we want to split both the knowledge base in the corpus and some discussion of how did try to do that split so it does align well with each other as I'm going to skip that for now we want to talk about evaluation metrics factoring your problem into 16 binary classification tasks binary classification matrix of three straight for it this is an arena where it's all is well understood with the best evaluation metrics are typically precision and recall or what you look at particularly when there is an unbalanced label distribution as there is for this problem precision and recall really helpful when developing a system to have a single summary statistic on which to hillclimb combine precision-recall into one statistic solution is to use f-measure which is just a harmonic mean recon people use F1 which gives equal weight to precision and recall location though I think you can make an argument that precision matters more than recall the goal here is to identify new KB triples that we can add to our knowledge base I want to be putting garbage into an orange face it's better to miss something that put garbage into an allergy want to give more weight to precision then we due to recall is using the F 0.5 score which gives twice as much way to proceed Escolar makes it easy to to calculate that statistic that averages across labels call n f 0.5 for each relation separate combine it all into one summary my only question is whether to do micro average or a macro average average which gives equal weight to each relation because it's not really significant it doesn't really matter listen to happened in this data set to fall into each relation cute relation as equally important solution code that helps you actually runs evaluations here on a random classifier and what you see when running on a rainy and Pacifier is most everywhere call is red around 50% this raining pasta is just flipping a coin crew half the time it kind of makes sense that if there actually is a true Labour when the label is actually true Fifty Shades of getting recall is measured through all the time and most of the time label which is why precision is quite low macro average precision is 8% the makarewicz recall is 51% test score percent is really low it's much closer to the precision than it is to the recall which is a characteristic of f-measure it tends to be to the lower of recon and the next thing I do is create a simple baseline model I'll let you look at it on your own for the basic idea it's not even a learnt motto it's not it's not a machine model instead look through the examples tied to each relation and we find the most common phrase where is between the 2nd dimensions what are the three most common phrases for each relation fuze exact matches to one of those three phrases for example debt for the was it look at the parents station the most common phrases are in the forward Direction action and camera and and his son just sort of makes sense certainly seen the word son in there makes sense and stop words are really Common probably a bad idea listen to mother in LP tasks people advocate dropping stop words and punctuation you probably don't want to do that here your sincerely comma bear karma shows up almost everywhere almost every one of these relations has, as one of its most frequent medals so comma is significant but it's extremely invidious it could indicate almost any relation how this dose we better stick is 11.5 so that the macro averaged f score play The Rain we get sir I'm still really terrible right the rain gets always like nine percent 1/2 percent help really terrible modest Expectations for how were going to be able to do on this task on this dataset getting decision the KB is incomplete could be some relational triples that actually are true in real life is in a Corpus that tell us that are true true because they're not in the cage play BB King complete trouble getting recall there could be some relations that are true and are in the KB but there's no evidence of Vienna corpus or kind of small and that's going to on precision and recall the next step in this journey is to apply a real machine learning model to this notebook which is linked from the from the course webpage doing that to using a bag of words feature representation logistic regression classifier the most vanilla setup possible play vanilla setup gets to a summary statistic of if I remember fifty five percent play better than the simple bassline you down the road that vanilla setup leaves lots of room for things to be working at in homework 3 and in a bit we do much better on thank you 