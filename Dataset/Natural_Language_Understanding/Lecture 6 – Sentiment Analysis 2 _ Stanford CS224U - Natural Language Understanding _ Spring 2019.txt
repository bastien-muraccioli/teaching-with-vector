announcements and then have a quick gas lecture from or sweetheart it was irresistible in terms of Maurice just having that a project that kind of Bridge is that from the vsm unit into the big themes that I tried to introduce last time around sentiment analysis and why you would want to do it and tasks that are adjacent to it a couple of quick announcements exciting Cathedral Bake-Off submissions coming in I've been checking them periodically throughout the day anything about the scores because we only reproduce everything but I will say that I myself I was pretty proud of my system I thought I was doing pretty well and it's pretty clear actually that I would have got in about the 30th place some people have really done better than I did have a follower report after we've done some reproductions next week I want to make is like my bias for this course is definitely that you are come up with your own projects I think that's more exciting but when people riding with project ideas that you might be enough I'm going things that you might want to join I can't say no because some of those things are really exciting as well and I wanted to just mention two today so there's first one this is from the people that gridspace which is an area startup that does a lot of speech to text work time for connections as well so you might have encountered them think some new data sets that are going to be kind of Quasi public not protected data so it's pretty free use around dialogue and at least part of those are people interacting with artificial agents Justin at project they have this Google form that you can fill out and then I think I'll get in touch with you about denso first and actually this is exciting because they're still in the process of Akram people potentially you about what kind of labels they should get and how they should design the dataset so this is a chance to Canada get in early say is that I attached a bunch of sample files which they sent me automated dialler where is is hilarious as advertised here Chinese from Childrey who is done this in the past ongoing that some students work with successfully in the winter for the cs224n nicely time because the centerpiece of this is relation extraction using distant supervision which is our topic for next week so you potentially get a nice confluence of the work will be doing and a project that you might develop with me again I provided his contact info and I think you should feel free to write to him and then before we dive back into this St Moritz to come up said this was just a great opportunity because it right now and the topics are nicely aligned with both things that we've done so far crossword diving into the next unit many starting on your projects or at least thinking about it 5-minutes to just fashion and politics and also make a make my case that you should not forget about anything that we were in the last 2-weeks because it's extraordinary useful for whenever you're going to be doing and to prove that point blow-up powertex aggression politics specifically political polling so I do some work with a democratic political polling well and about the space which is kind of interesting critical poem is actually yeah really bad right now most of it still happens on the phone and you get these like really kind of structure questions like do you care more about balancing the budget or income tax Tuesday or be in so with the the actual politicians politicians get back is very stereo in it's like highly issues based really want to know if you really want to know why free violin for my what's driving your decision how are you thinking about candidate how are you thinking about the election like what messages are resonating with you what's top of mine for you they want to get into this lot more about attitudes perceptions opinions how was the cinema not clear work if we know that better or cos you know has 32% of the vote in a bit Warren has 18 it's knowing that one product has a 5.5 star rating of 4.5 question is why what's the difference between us to why do some to resonate more than others while some more support than others so system and it's needs to answer these questions looking for an extended why are they so why are they supporting their stated choice for the election for the status were going to be looking at trains from a survey they were asked what qualities you look for in a president candidate for us Congress or senate or for somebody for state or local government and we also have free to responding the party identification so 2000 responses of people basically talking about what is how they framing the selection committee supporting and why and we want to start understand how can how can we give politicians real feedback on what's driving is decisions so I wanted to show you a no but I can get my computer connected but only reason initially nobody's because this is literally the only code in that note wonderful vsm code I took a dump my own like whereby word matrix the bed every night so do they call these two ions in suddenly all the rest of the expiration basically is based off of these two hands so I think it doesn't taste Robert Rinder how quickly you can get started with a totally new and unknown dataset so we wanted to just do very quickly as look at some of these teeth implants so Christian understanding at do Democrats and Republicans make decisions different for for the election so this is obviously you can kind of see the same colours colouring the terms based off of what are more associated with Democrat or Republican greeners Independent Traders Republican it's kind of up here blue is more down here in the future zoom in do it there's a there's a couple of nice things that happened every innit it just for clarity on the Republican side about issues so the words that you can't really read up there or words like defence physco words like constitution about conservative values immigration policies and values but if you look at two where the clusters are more dominated by blue things like collaborative co-operative composed articulate charismatic well spoke descriptions of personal characteristics descriptions of somebody mad have you people are thinking about Trump so you can see already kind of a clear split between thinking about candidates in the election and how Republicans are we can also look at this again taking advantage of that structure we had the dataset which was we also know what office they're talking about same thing this time were colouring the colours of the words based on whether it's more likely to be in a review or in a comment about presidential local or congressional elections and act there's time here the personal qualities this blue this dark blue that all presidential so personal qualities of temperament being quick being unifying being a ghost Herobrine classy as all super important for more at the the area of Here words the other colours for local elections set a lot more about issues we have added kids accessible at Healthcare affordable housing is are much more the bread-and-butter of like I have local policies that a care about that need to get done great work mods thanks Chris spaniel thing I want to shows ok so this is been this was like how long did it take I need to take the original data and then call two lines of this code in another visualisation line feel like returning to get a sense for how we will tackle this problem for building a new system to answer the question what are voters looking for calling Canada text over Wi-Fi then one thing we know we have to be sensitive to whether the talking about issues or personal characteristics that seems to be kind of a cleaver dinner date we might want to understand as ok we want understand personal qualities more and how does which colours are resin more other people so maybe we dig deeper and try to find subclasses what is a bad decency humanity in fairness but this is about being well spoken this is it will be a first step you're diving into a new unknown dataset you want to know what is in it how do I actually solve this high-level and all your problems I want to solve that we've iceborne so find the first unit is your best first stop and even for I get paid to do this by the way for like in industry snap just started kodi can give you a handle on on a dataset on a problem and so as you embark on this next unit and your projects I encourage you never don't forget that the humble vector space model also that domains like politics understanding and experience I can play on your experience all of these domains need another you people and they do not have a figured out so it's up to all of us to to help them into exciting new things for April any questions somebody answer the survey to even care got the data that comes back to we trust it means anything to us we actually have in in in the actual application of this analysis we have data on turnout what kind of things that we can verify about like daddy's people vote brother questions telling you about their experience should you care does it matter I'm going to say I'm going to make the claim that whenever people are being in Motion nursing opinions you should care because they have taken play 92 Philippine surveys or offers reviews in Southern my first bias is always there's something about you here they had some reason for telling us this in an aggregate of a million people tell you something you can learn something from question I think I mean that is true overall is in politics in for Presidential elections in America stylematters some of that entering but I think that maybe two kind of in the age of trump things are things are different word orange soda lot of things to say in a presidential survey otherwise so I think like that also speaks to how like when I apply glug to the same day that it failed like it didn't because the the words people were using in this context just means something a lot more it's also play coming back to the rescue when Blofield let's dive back into the centre Minster recap what we did last time set the stage I talked about CL general problem in nlu when I try to make the case that it's a very interesting problem even if at first blush general tips evidence for this word thinking about pre-processing your data how you do that thoughtfully and software Mr Stanford sentiment treebank that's up highlighted some of its unique properties as a dataset in general and certainly for centamin the bunch of time walking through the basics of SST depay and I'm not sure you want screen from that kind of summarise is that whole unit the reason that's important if that's what you want to work with productively as you're doing your homework too and bake off too in fact I'm going to try to get through this material Al Lee pace but will keep our keep moving forward can have some class time today to make sure that you guys are all set up and working productively with it and a bunch of the team is available in the classroom did a post on Piazza in case you remote and 1 and just log in and chat with someone about king or maybe take a step back from where we have and think about provides learning the kind of stuff that I'm sort of taking for granted in this course but but if the team is happy to fill in any gaps that you might have but the point is since the time when they was tight we want you to get good with the code today today I'm available for that hear kind of summarises the entire frame here is kind linear methods which group were going for first but exactly the same framework works for the different learning models that will discuss flexible and modular in a way that will let you run a lot of experiments without making a lot of mistakes the long and short of it is set yourself up to point to the data distribution function to record at 5 countries and return dictionaries account dictionary Aldi's motto rappers with taking a supervised dataset in X Y pair return that fit in model do you have to do to run an experiment is called SST experiment data distribution feature function in that model wrapper from here you can see that there's lots of space to explore if I wanted to try naive bayes or a support vector machine I would just write a different model wrapper 4 different feature functions I would just write new functions of trees and that would be very quick in terms of evaluate Again by default this is going to evaluate on random train test split that are drawn from the training data you could periodically test against the dead Set to see how well you're doing in reality imagining a quick peek under the hood at how this code is designed I would encourage you to look yourselves and find out even more about how this is working but it's kind of all based around these dictvectorizer is which I think help us avoid a lot of common coding mistake patience of data free cab are there any questions or comments about it before I dive into this new stuff is it might of emerged over the last couple of days what's diving budget sometime later in the term when you guys are in The Thick of your projects to talk a lot about methods in metric going to return to the two themes that I'm going to introduce now in that context and have talked about them even with a little bit the now because I think they're both of these can make you a better experimenter right from the start can have hyperparameter exploration comparison type aprender haircut that hyperparameter search and I want to give you the rest now for it so just walk through this argument the parameters of a model are those whose values are returned as part of optimising the model itself does it sometimes called in weights not mainly what we think I've only think of machine learning is that like front data Armada has a capacity to set all these parameters in a way that are effective for new data important or water called the hyperparameters of the model Deezer any settings of the mod side of the core optimisation process samples are liking Glover LSA you have the dimensionality of the representation that you're gonna learn Fallout 4 glove you have the learning rate for glove you have x-max as part of the weighting function and alpha which is also a preprocessing step essentially that does something to your account mate it is learnt that part of glove but they're super important in terms of what you learn what's a regularization terms hidden dimension Aldi's learning rates activation functions you can even go so far as to say like the optimisation method itself the algorithm uses the hyperparameter your model these things essentially if you go onto scikit-learn and just look like in the linear model package at things like logistic regression or if you look at the support vector machines in their or than I have these models they have dozens of things that are hyperparameters and you can see them cardify them cold because it says arguments they have some default one of the many values that you could explore for each one of those things crucial to build a persuasive argument you need to be thoughtful about your hyperparameters national there is what I said here in the best possible light to run out otherwise agonistic situation you could appear to have evidence that your model your favourite model is better than some other one strategically picking hyperparameters that favoured yours over another and really good settings for your mod no account of the general for the other one and then you say look my model is better come on The Searchers all about kind of making sure that you don't have the opportunity to do that opportunistic selection think about this is that science there is a kind of antagonistic dynamic that happens and I think it happens primarily in the service of making sure we make proper you submit your work somewhere careers now evaluating mode for that referees to thing don't really have this person is saying and from their perspective they might think well find me to settings they didn't pick those settings in a way that would ring the game in their favour free is really looking for is evidence from you well I'm described in three here which is put every model that you evaluate in the best possible given the data that you have implies then evaluate these two models extensive hyperparameter search not only space that you explore what's a report and statistics about how does models perform certainly the best performing model but maybe other information about average performance and sulphur done that if your mother win confidence and your referee is going to feel less antagonistic feel like police models for giving a chance antagonistic you yourself could be that critic right you're a very waiting a bunch of models you want into the Bake Off you have every incentive to really and truly pick the best model in that case you should explore wide ranges of hyperparameters so that you can give the best entry giving the things you're exploring I'm encouraging you to do this think about this code to do this search set up a great of hyperparameters that you want to explore go to the movies or take a hi that entire time you are working hard exploring finding the best possible model that's a reason for automating the stuff talk with you about later especially in the context of your project some perspective today talking about infinite spaces of hypothermia infinite dollars doing his explorations someone has to impose a budget on you and I think we can have a discussion about how best to think about those budgetary constraints were you know at 3 or you can straining your search review that you haven't done any party I think it's ok I think we just wanted anywhere you to do is some exploration of pretty reasonable spaces are parameters for your models this is all about again this is a complete experimental setup leave fire alone it's just a bag of words classifier representation softmax with cross validation I see the value of having put wrappers around all these models is a poster just calling fit they do in this case is first set up a logistic regression model that I want to cross validate over 5 phones that CV equals 5 the interesting thing is this in cycle is called this parameter for the intercept bias I'm going to try the model with and without ruc which is the inverse regularization stay Terraria station strength explore this range of values from .4 to 3 something to explore which kind of regularization I do if I pick L1 and probably going to get a model that favorites release part feature representations a lot of the values for the weights will go to zero who will give me more than 0/8 kind of evened out that is well cluded in the software for the course in utils fire with cross validation you give it your dataset x y Anya parampil value fine for you via cross validation so should be pretty responsible by default here is exploring the full grade of value grow very quickly that's why you might want to pick a long movie or a long time large perambra is that your mother will turn away at this turn what in a digital age has decided is the best mod enter that into the Bake Off or into a subsequent evaluation every set what does trupoint 4l 13.6 L1 like that for all of the value Todd Grimshaw hard-wired grid search in reasonable to think about things like random sampling from the space typically performed basically as well and another more sophisticated packages like scikit optimise do more data driven model based exploration of the premises come down to the same intuition which is exploring wide constrain function here best mod I think it actually print out so when you call that it prints out the best primers you'll get some feedback but you also get the model itself which you can use subsequent terms of SSD experiment nothing changes in the reason nothing changes his because all of that cross-validation stuff was packed into the smallest rapper so go forth and explore widely let your computer run overnight The Logical thing that I thought I would introduce now is classifier comparison let me set this up again thinking about interacting in the community so I suppose you've assess the baseline Model B Matalan assessment metric Pavers looks like you want but is your Madeline question if the difference between B&M is clearly a practical significance can I save lives and you can show it's going to save 1000 lives don't need to do a subsequent round of statistical comparison or anything like that maybe it's just clear that we should pick your model still especially in this age of deep learning you have to ask the subsequent question of whether or not there is variation in how either of those two models perform because maybe on one ran you save the Thousand lives and another actually your baseline model is worth saving more in the case of practical significance you want to do it might want to do something further two methods for doing something further is Tehran a wilcoxon signed rank test that I got from the literature that say kind electric version of the t-test that's not assuming anything about the distribution of your scores bring you to a SAS seeded runs of Armada play to be better we can talk later about the Rational precisely for choosing that test but I guess the bottom line for here is that it's going to be a pretty conservative especially if you can afford to run them Allah robust picture of whether not your model is truly better than another one good the only downside areas you have to be in a situation in which you can repeat the assess B&M that has to support that like a lot of random train test split and also your budget has to support that right if it takes you a month optimiser B&M not going to be able to do this kind of on the afford to do one or two I've done is like 10 to 20 runs situations my offer to you is mcnemar test applicable in a situation in which you can just run the models one confusion matrix from the model operating just on Netflix set of values hypothesis put your testing there is basically have the same error rate noisia and you might have less confidence in it but I think it's better than nothing when it comes to comparing classifiers then just looking at the wrong numerical values in deciding that the larger one is clearly superior listen again if you think about reviewers reviewers when they see just two numbers xt10 difference real practical significance is the best answer to give things were further substantiated in situations in which the values looks small still have a good argument in favour of your model if the two models are very consistent in their behaviour and then you can do these tests to substantiate that even in the space of what might look like a small deer Let Me Down Easy set up dine is fixed bag-of-words feature representation file like before tomorrow rappers one I called fits off maxi my college fit logistic regression fit naive bayes comparison especially for sentiment which is better but just a progression or live dates what does tomorrow wrappers and then the two ways you can do this or first again this is a kind of Swiss Army knife SST compare models play Future Funk rewind it was soon that it should use both for both experiments on train function here you can see how fit till that was fit softmax and fit nowadays the things are just a thought that is that you can probably leave alone run all the comparisons that it needs to the signed-rank test which is giving hero stats doubt is the two meanings for the model this is just printer appear you can say I'm also returning the full vector of scores in case you want to like them to get further information beyond just the means about how these two systems compared one might have chaotic performance that you see when you look at the score Mrs test kind of similar to run run26 experiments softmax and I vs then you can run them mcnemar's test you just have to make sure that you run them on the same data interface the actual values from one one of the other these cos they're meant to be the same and then look at the this will take awhile to run because you have to run a lot of is running just one WhatsApp open things that I wanted to future representation Stockton I'm just doing us by way of example and the idea is that you see having seen a few examples start to get creative a battery represent features far we've done is a bag of words small but when I've done here it's just two bag of by g trigrams and so forth stand for trying out thing that you can do because we have three structures is actually use that structure and so I've called that down here does is represent the data as words that occur in isolation in all the pairs of words that occur as siblings of the same parent sing that is amazing for my phrase linguistically but nlu in is do not you can see the difference is here so I know you and is for example is part of the background what part of the phrase of representation because there is no single pair nlu in is play that out over the very rich tree structures that are in the SST getting very different representations then you get from a simple linear pass like 5 g to me see down here I just said the height is less than 3 to get Justice local tree tyre right and it will be a version of light getting higher level and Graham's because you'll be finding larger and larger chunks of these trees I want to highlight here is negation so stick method that I think is very powerful navigation words better in a lexicon all the following tokens with underscore the day are in the scope in some sense of that negation word that is that can be pretty imprecise you end up depending on things like punctuation tracing isn't there than the snake Martin just runs on in on way past what you would think of any kind of semantic scope for the negation I have three structures we can be much more precise about this example the dialogue wasn't very good but the acting was amazing negation is meant to target only very good for us to keep track of in the William F with a tree structure basically what you want to do is read a feature function that when it finds an occasion parents and then marks everything that's that's below that parent very close to what language think of ASDA this is an all this clearly only gated general idea there are lots of things in language will it take scoped in this way that have semantic influences over the things what's the time in the tree actually touched on this last time they said it was a great commitment to it being great whenever it is this method of saying imagine writing feature functions that are also marketing things within the scope of verbs like say and claim out and deny Martin melanson special treated differently by your model it might be successful right that kind of head very different from it is successful capture that by doing some of the scope marking from might down into the sea play say something it's really possible only because we have these the weather ideas so obviously I features you gonna do some of those for homework 1 about modal adverbs marking nerscylla it is quite possibly a masterpiece or just totally amazing those adverts are doing something special to commitment what in the literatures called sorted expectations this is the case where I build up one kind of a valuation only to offer really another one right many consider the movie bewildering boring slow-moving annoying building up to a positive endorsement because of this shift in perspective they're performing in one signal that is that they're kind of laying it on thick with all of this negative like in both directions how does a brilliant unprecedented artistic achievement worthy of multiple Oscars best kind of review is one that your model is going to struggle with because of all the indicators of positive one word that was negative balance actually might be a signal to your model that somebody is King into this sordid expectation thing count all these words essentially and look at their ratios play harder of course I love like the Holy Grail for work in sentiment analysis is getting a really good grip on non literal language used it's not exactly a masterpiece terrible say it's 50 hours long for a movie that I probably they probably don't literally me special emotionally when they pick that hyperbolic expression smoothie in the history of the universe and you feel like it could go either way and turns off sarcastic what's the weather idea just a sample on them and that Lexicon I do I showed you some lexicons before that the pretty rich resource system mind for doing synonym one other methodological note that I thought I would insert here it might be that for the Bake Off for example you end up writing a lot of Future Funk you could make early on for example in this Bake Off is that you should be using when you're models not deep learning models because you only have till Monday and it can take a long time for deep learning models to work set you on the path of Reading a lot of interesting feature functions of the sunlight just been describing to you insert here just a methodological note about nice feature functions party Britain a lot of combining them into a single model encourage scikit-learn offers in it's feature selection package a bunch of methods for doing processing feature functions individually what you doing with those tools is assessing a century in isolation patient they tell you about the class label you are picture of how much predictive value they have I'm here with this little dataset which I constructed artificially it's just tried to send you a warning that feature functions in isolation kind of good first past your wrist very dangerous because what you end up doing in the presence of correlated features is often overstating the value of those features that's what you're really doing when you run your model is assessing them on the context a&k unified model doing something very different when you're doing with this individual test think about the example here but the point chi-squared test in isolation at least these two features what's 2 to include and if you decide to drop x 3 width of the matter is if you fit an integrated model than X1 alone is the best mod Fuji X2 actually degrade the performance of the model that's argues I think for something more like a holistic assessment where you're may be dropping her adding individual features but always in the context of the full system that you're evaluating and not so much in isolation the way of done here play Something like the boy from do like to actually liked you for 20 minutes how old is France away removing relations call some problems at test time play one of those matrix factorization methods and do that to your train features new example what to do that reliably almost knew examples as well that can be problematic because the characteristics of those in test examples might be very different from the training think about in terms of a real book if you do that reduction on your training set but so you've done a single test exam WhatsApp Claire Whitchurch bus to do with that test example in terms of turn lights I could actually try to manage list so for example it does for PCA but if you rang tf-idf get transfer feature matrix for reweighted you rang that with just transformed I think it uses it values stored from training and it applies load make a correlation between label the class what is a way to change the river telling you something about the entire mate skip firegrain in information if you can afford actually take it has these functions like ablation things that will repeatedly run your model having had a removed some of the features Tennessee there practical significance in the context of the 4 model with exactly the scaling that you're going to be using for your test about Fisher representation that's a kind of transition into the world of deep learn here is using distributed representations as features simple example he is the Rock rules Xanax this case is just look up all of these words in an inviting space maybe glove maybe it when you've got yourself right from the first unit stop to do in this space is find a way to combine them into a single fixed dimensional representation because all these methods presuppose that all the examples are coming from the same embedding space essentially simple mode what you would do is combine them you could use for example the sum of the values the mean or the product or ever some function that will take them together into a single representation function x here is just the input Gear classifier for if we both a bunch of feature functions like the bag-of-words one your feature representations will have in like 20000 dimensions play Spurs distributed representations of your data what have 50 to 300 dimensions depending on what batteries you downloaded are built will be very dense of course right all the vectors will be a different perspective on your data using how much how old is pacifiers can perform giving how the information you seem to be feeding them how hard is combination step typically because of course this is a hyperparameter cheer model it's not something that your model is morning play paragraph how many add together signal BlackBerry radio actually striking how you could do this with fairly long documents just some them up a representation of what's in there situation like intuition is that you would not want them mean play the something that sound it was including lot of information about the length of the tax which is essentially get from the magnitude of the dimensions as you add them together kind of amazing how well that can work all-purpose rough look at many experiments with this again a climates really easy question in the back I think this is a nice bass line for a sequential ma argue for an rnn that used glove input this is my bassline you doing mirrors saying constant the amount of lexical information that I'm introducing is the value of modelling A46 assessing your iron and about how well it did over and above combination function cuz it's kind of like this song combine these vectors in the complicated version is your fancy lstm I can accommodate these representations I've given you the full recipe here and most of this is just building up the embedding space glove look up purpose feature function that has too many arguments for the frame special-purpose one that builds my glove representations exercise before no change there play experiment and the only thing you have to remember kind of data coming in that is vectors and not dictionaries is to say vectorise = false is it soon that does vectors coming in are actually dictionaries and it will do it all it will be something like crazy doing his saying hey where's my data for my using a dick factoriser just take it as it comes works out when's about that approach two more things into Deep learning so the first one will be iron and classifiers centred in a few ways in the package if you want all the details go to NPR and classifier forward algorithm and also the backprop for learner recommend that you use a torch classifier rnn Tramore Road are drying on the same basic structure which have depicted here rock Ross is my example mapping something bedding selfie glow or it can be a random in betting switch off thing is the action of this iron and so it has the thin layer here donations to get hidden state each time step the simplest version of final X classification decision by the time I got into each three I'm looking at some fixed dimensional dance representation to a classifier this little part here leave this is the final hidden representation from the ra I think these are kind of a natural pair if you think about assassination just the label for the sstc series models before sing one thing about how you do preprocessing a bit fiddly so you're examples here are presumably list of string tiny vocabulary so imagine these are just different words to examples preprocessing flow you map those into indices indices r into an embedding space that you've built all the code will randomly initialise one of these embedding spaces simple use that as the inventor is indices you do all the Look actually practices of course a list of vectors final version of these examples is this list of vectors play this is a kind of standard set up for this problem obligatory of course then we look later in the course at contextual word representations we will skip all these look up in tyre secret different initial representations depending on the context disembedding of cutting of course Kent turn on itself is an exciting development to my mind because fixed representations down here sterling a full sequence like this I'm modelling all of these words in the context that they occur in example one thing I really like about this as a linguist is patient education is important for cinema do a messy heuristic method based on the tokens a very precise version that depend on trees the trees might be wrong or I might just be confused about how scope works in my dataset what's have drawbacks because neither of them is really responding information from my labelled dataset and here of course representations you know one in the same token it 0.02 depending on whether or not each one origin for renegation sensing which needs models just out of the bath you do have the power to see those semantic influences play out at a level the sequence thing for mites in for saying for claim in for no one in for everyone all the things that are affecting the semantic context expecting the hidden representations associated with the words in the Seeker play holistic severe example play the potential to capture lots of interesting symmetric in very successful colours for lots of different here on lstm I'm not going to dive into this too much suffice it to say that dance like that nt1 down here consequences surprised that they do because the signal then get from the training label appear becomes very diffuse and problematic as the sequence is getting longer another mechanisms like there managing the flow of information what's the better results the torch and tensorflow lstm cells by default can I give you an explanation of them here we don't have that much time and also frankly can improve on this to both blog posts especially this first one lstm lovely kind of Visual language for helping you think about what's happening inside the cell work so well use a code snippet to round this out again here I'm building a glove space from iron bedding future functions of course they get kind of trivialise what I want to do is just return the sequence of words that are the ly position of that point because the real action is inside fit or NN turn on the train vocabulary Daimler keep just the top 10000 Words by frequency and all other words will be mapped to UNC set parameter however you want embedding space I'm here other parameters that you can fool around with to the XY pair that come in let's just SSD experiment as usual and again remember to say vectorise equals 4 it doesn't assume it's a bunch of that would even working this can't terms of hyperparameter respiration your singer going to this year the values that you could explore play the dark side of deep learning 8:30 things will matter a lot one more thing let me just do some coding in here in the room tree structure network it's a very similar idea to the rnn instead of having simple sequences were going to process the sst3 as they come same example the Rock rules hidden formulas in here hidden representation Syren Orange instead of processing them in put to him child no together to form the hidden representation and you do that part recursively reach the root of the tree which is a hidden representation and that's the basis for your classifier softmax classifier is used BlackRock is a process of taking the errors and feeding them down and splitting the gradients apart and then feeding them down NP18 3NN exactly how that process turn off the lights how much is a great question my code is just concatenating the two child nose through this kind of simple classifier here we have learnt weights in a bias good baseline for the space it's the first one that was explored and the first one that ever and kind of setup three alternatives all these words for by Richard socher actor where you represent each word as both of Vector animator combination to form the parent you can have cross the Matrix and vector so that you get lots and multiplicative interaction to powerfulness instead it had too many parameters relative to the data so true it all motivated this tensa combination here this combination function is it with method for every conceivable way the concatenation of the two child nodes detentionaire kind of sandwich in the middle allowing for alters interact and actor function looks like this unsuccessful in like when I was showing you examples from the SSD project before that was a model that was using this combination funk rounded out of the more recent development in tired all Harvey to the nodes that are doing combination functions of the nodes are lstm racing there is that there state other kind of getting information from the child to child separately so they can be created separately and then combine them into a single representation tell me recommend all these papers if you're interested in this face this is a nice progression of ideas final thing that you could say that unfortunately we don't get to export too much in here is that record course that in the SST you actually get supervision not just at the tree note the way this model is employed all of the Sub-Zero model implementation in r that will do that singing tuition here is there point of supervision parameters for the scratch of Fire them all together information for all of them and then pass that down and each one of the centuries in terms of implementation once you get your head around how old is information is to be passed around in the tree it's pretty straightforward you can get really powerful models coming from the fact that you're getting so much more information about what are the sub constituents mean in terms of send down the sound a picture of how you use SST experiment with these three models and I think the only special thing about this is that the three models are unusual get the label is part of the training instance itself the default the regulatory and then it looks to the root node to find its label as opposed to having it in a separate bag little bit funny down here that my dad's it takes only x and not y play for assessment the reason for that is that santry supervision they are all of those points of supervision for my training instance and not worry about how they align with some other the only got your other than that you can use this framework for the full set of models that I've introduced you questions because I sometime imagining for the homework of course that we just ran through it quickly and get you oriented and then you can start coding on your own the words alone is just asking you to use a lexicon a filter bag of words feature function how much information about the class label is included in this off-the-shelf lexicon asking you to do is designer feature function any more test to compare now due to their inner thigh king about classifier comparison pretty straightforward and more powerful vector something Bassline is asking you to explore in a little bit more detail give me the representation approached I showed you as the final phase of thinking about Michael linear classifiers start of seeing weather for model and just a linear classifier one that made in cover lots of interactions among the future dimensions you better at the problem no one is your custom system here just means basically anything goes turn on other people's code of course I have a huge bias for you taking cold that we provided and doing cool modifications to it to see whether they pay off we don't have to place too many rules here original system in that's what you're enter into the Bake Off evaluating on the ternary class prob off so you should kind of hill climb on that who is probably turn off testing just within your training data quickly test against your dad's let's look at hyeres actually generalising to new data never look at the test and of course the test that although it's distributed it's completely out-of-bounds until the Bake Off start then will actually run the evaluation and I'll give you some code to guide you exactly how to do that but haven't mine does well on the Turner request problem on the test the other thing that one knows that I added Deliverance at the bottom here asking you to give an informal description of your system help the teaching team kind of get oriented on what your code actually does and what your solution is we can classify the Solutions and come to some kind of understanding I've got work in what give you really rich reports on me the first one on Monday I can tell from the submissions that have come in that there will be interesting lessons that we can reflect that to you about working continue stop talking about you or just get cold 