started play we're going to talk about is atopic for coreference resolution and I'll explain in the just a minute what that is before getting onto that just had a couple of words on the announcement feverishly working on getting homework 5 grades without so we hope that we can deliver those to you and tomorrow just in case you're interested to know them before you make your final decisions about things and then the other thing that you should be remembering is that the Milestone for the final project is this Tuesday now I will confess that even to me at seems like white boy this milestone came around really quickly so you probably feel that we I realised and then so you know apologise for that a little bit video really hope was that we can actually use this to be helpful and give you feedback on what you're doing and suggestions you seem like well the only chance and which we can kind of turnaround giving more feedback on the project goes into the final week of the quarter is if we can kind of gets to be then sort of Turn It Around Again by the end of the week so the hope is the help you not to just roadblocks in your life so today what we going to do linguistic topic for a change and learn some more stuff about what goes on in coreference resolution so first of all I'm going to talk about the time some of the kinds of models of people due for coreference resolution first of all what is the idea of coreference resolution is what we do with three have a text Barack Obama nominated Hillary road and Clinton as Secretary of State on Monday and this text like most texts are about into these way into these are commonly human beings but they can be other things like God so talking giraffe so whatever it is it's like we want to and where into these are mentioned so into this and mention the referred to as mentions the things like Barack Obama and 60 of stay he her they're mentions of entity when we talk about coreference resolution the tasks that we wanting to do is say which of these mentions refer to the same entity the same real thing in the world so well one into these as mentioned in this text is Barack Obama and then he's referred to Laser in the isn't he and Sophie noun phrases all coreference to each other offers to this real world in today and then we have these references Hillary road and Clinton secretaries stayed her she first lady they're all references different entity and so they are referred to this person and so those are examples of our coreference play this is true is obviously a human being and looking at things but the connection be kind of Tricky and had so I thought we could spend a few minutes corrective working.out coreference together so that you guys can have think about it all for a few here's part of a little story and it's a story by shruti really cold now I can fix that since this is a CS class I'm not a literature class I did a little bit of I'm hopeful editing of this so I could fit more of that what was going on onto the page everything that is the Sword of a linguistic 24th is something that comes from the original tour this text the first entity that's mentioned ok ok now where let's do it forward where else mentioned in this text right so this her not the son but this her is a reference of an idea right resign ok bar if there's another she was there another reference before that herself is also reference to the Nightjar surveillance against she skip ok ok that's it ok so then we've got a Keeler is Aquila next referred to ok there we go the references IKEA ok what's the next into the that mentioned what other references are there to produce they ok so he is a tricky one this day I mean who does that refer to refers to Padua land yes it this may refers both the pod well and this that's something that happens in human languages this is referred to a police antecedents where you have one thing that they that sort of referring to to distributed things that came before my firsts admissions of natural language processing technology none of the NLP systems that we going to talk about later today or in general that have been billed play dentist quickly lose as soon as they're sweat intercede sad but that's the state of technology so it's something we could still work to improve but ok there's this sort of a that's kind of half proud well so there's directly punctual here but was there another place early in the text that produce cures son is really another mention of pod right parsva maybe not ok we go on next entity so we have a cat here and that then again we have the first son referring to a car a car brother what other mentions of a cache are there another Kasia him broadcast ok where's the obvious a caches this son of a tricky case here but you could wonder what the right treatment are these right you know it's sort of cyst a cash was to be a tree in some sense is a cash terms of reference in this story 3 is the same as a cash you could think to treat the instances the tree what is he of the tree and later on when the nicest tree right that really is a cash as well quite be alright but this is something that comes up in coreference right so here we have a sort of a predictive construction with your be and when you said when you have sentences such as a child is the smartest kid in the class or something like that in some sense saying that the smartest kid in the class has the same reference as my child systems count it's over that kind of predication and say that is correct don't think that that's not quite reasonable so different things go on ok so those Sophia number of entities I mean so there are obviously lots of other things that I mentioned that sort of right so there's a local park right that's a mention of some into the there School so the school here and so that the school is plant with store IKEA and then there's a tricky one of how to treat child Lord Krishna Kirtan opening something representing mad I'm in there lots of other into these that I mention right as a t-shirt and his trousers things like that another tricky thing that turns up here when you get later on into the story is you can have entities that have parts not only have a tree but that tree then has a lot of part right to the tree has a truck foliage things like that and there are these red ball for the representing fruit right there's a lot of stuff together and somehow separate and that sort of that doesn't fit terribly well with the kind of models were used with coreference either because really we make out basically had a destination of entity this complexity that human beings have parts to write we have hands and faces and we can say all that the separate into the that there's somehow and involved with the other in today that sort of useful to give some idea why is coreference resolution used for all kinds of things that would like to do well in natural language processing that you really can't do well and how to do a reference resolution so anything that we want to do in terms of question answering summarisation extracting facts from text or anything like that there are places we're going to fail and West we can do coreference resolution because if we're reading a piece of tea was born in 1961 spec-delta answer question if we can work out who he was another wise another place that we're this is very useful is in machine translation lots of languages pronoun so you don't have to give us a pronoun need to be able to work out how to fill them in and this is making a reference decision arguments of verb a couple of examples I'm coming from Spanish to English on Spanish you can freely drop the subject of verbs and in these sentences in the because clause there's no over subject and so he gets Alicia like Swan because he's smart and so Google Translate is stuck in a and that is right and two sticking that he it's implicitly making a coreference decision and say ok well the subject of the smart one whose mail-in therefore I should say he light knows nothing about coreference and making these coreference decision been I'm covered quite a bit in the media now and I think I'm up earlier in the early class that Google the falster mild for what if you put around and safe one likes olesya is also says because he's smart and where is probably it should be because she's smart in that case no just the bad effects of that everywhere so many languages Turkish Indonesian don't actually have gender so that they are much less sexist languages than English French or Germany can you then translate where you just have a generic pronoun that me pronoun and that Google Translate is essentially using us language model which means that reconstr stereotypes of she is a cook and he is an engineer he has a doctor and well in a connected piece of discourse if you'd like Google Translate to be able to do better than that again what would be required is that you can actually two coreference resolution and track along the actors in the text as you go along 150 example we haven't really talked about yet but will get back to you soon now because the class is almost over is doing things with dialogue agent so chat system if you want to do anything more than a single turn dialogue that you need to start dealing with reference so if you got something like a book tickets to see James Bond you want to say something like Spectre is playing near you at 2 and 3 today how many tickets would you like showing at 3 mckellar there are various kinds of reference going on here we're things have related reference complicated here and this is something that will come back to Anna moment so inspector and obviously the same thing context bike I'm booking movies they are the same thing because one is a character in a movie series and the other is the name of a movie there's currently showing the belongs to that so that they sort of associated I'm in the sort of subtle way that is an exact identity but it's relevant to a lot of the things that we want to do a little bit when we took bit more battling with 6 of those if we want to do the task of coreference resolution essentially two step step I want to workout there are in the text that we should be doing something one is effectively pretty easy but I have just a few slides on and immediately and then put the bulk of the cost is going to be on his working out coreference between mention about this car references essentially clustering first part you have a set of mentions saying well how can I group these in the clusters that have the same reference and so that's what we going to look at more doing play on mention detection so don't mention we want to find all the span that I candidates for referring to some into the and the answer that what these turn the data is basically there all the noun phrases in the people think they're being three types of maintenance that we identify their pronouns I you he she it etc that are referring to different these names of people like that was a Barack Obama and Hillary Clinton example and then many of the tricky examples and then we have common noun phrases all the big fluffy cat stuck in the tree with the big fluffy cat stuck in the tree is a mention it's actually a complex mentioned because it also has been better than cider other mention the tree is also a mention how to read what answer is the say well we've looked at various other in old PC systems on and off and we could just use those innapeace systems as preprocessing systems to find mention sofa pronoun tigers that say what's a noun or a verb or pronoun and so we could run those and find all the pronouns with and the names of things like Barack Obama which took a couple of times that named entity recognises so we can run those and find all the names into the a common noun phrases that sort of where we need puzzles to find the structure of the sentence and find the noun phrases are and we have talked about dependency parsers and well one choices you can use a dependency parser to find the sort of normal arguments and work with them sexy little bits at 11 just thought of wanting to pick up spans that refer to common noun phrases so the other notion of the passing which we come back to next week something constituency puzzles at the simplest way to find mentions for this process is easy there so tricky cases as the what counts as a mention or not so if it's kind of it is sunny I mean is it a mention of something it's not really it's just it seems like it's on it that you stick up the style the centre anything so that's maybe not a minute student is every student dementia some kind of collective what's a very clear concrete reference goes further if I save use different quantifiers that was like every Noah caught quantifiers I mean no student definitely doesn't have reference because anything right it's asserting a claim of non-existent so there's definitely and it isn't a mention of anything doughnut in the world reference I clear this is the kind of thing with actual philosophers of language debate over right so if there was a green lantern what the best dad in the world is then maybe it has referee I can say sentences like I'm searching every way to find the best doughnut in the world and then and that sentence doesn't have any reference try that sort of intentional description of what I'm hoping to find that there's no concrete things it refers to 100 mi behaves like a noun phrase want to do that doesn't really have reference send us a question of how can you deal with this stuff whenever you want to deal with staff is real pacifiers when they pick out some things that and said that something that you could do is write a classifier that the spurious things that you want to say I really mentions and people absolutely have done their actually people skip that step and you just sort of have your mentioned to take that find all candidate mention turns out that tends to work pretty well because after we found all of our mentioned I'm going to be doing this clustering process to find PAYE reference few stray mentions like no student and we don't cost them wrongly with anything else it kind of doesn't do any harm because remind me involved in this clustering process I'm something you might be wondering is reply now we have a pipeline and saying we're gonna run a part of speech tiger and we going to run a named entity recognise and we got a runner past and we going to run send a text that and then eventually we're going to run this correct plastering system so we have a sort of a 5-step pipeline is that the only way you can do resolution and the traditional answer was yuck that's the way you did coreference resolution that essentially all systems for coreference resolution 2016 were pipeline went through about those stages and just recently another system in the neural world have started doing what's been effective in a w a places in the new networks world of saying can we just building into him for a reference system that starts with just plain text of a paragraph coreference clusters without there being any intervening pipeline steps a bit more about how that works get into systems I just want to say a little bit more about the linguistics of coref electric end extra stuff that's been thought about very much by people who build in Opie sister and I already mentioned from the shruti real story and the example of split antecedents right that's just a clear linguistic phenomenon that happens it's not even incredibly rare and that you know simple just can't deal with that is really quite a bit more structure to what happened 6 of co references the systems people build so I just wanted to show people a bit more of that and essentially to serve understanding how people see things linguistically there to concert related and commonly confused coreference things are correct to mention the same entity in the world trump in the current president right there to mention the same person in the world and so that it's a relationship of coreference and that's been contrasted with a nephra and so the idea of an F40 is x in texts independent reference countdown referee number thing in the tick if we have the sentence Barack Obama said he would sign the bill he is an NR4 and if I just say he referred to in the abstract world saying something Mail you've got no idea why cos you can't work out Rocky me it's just by now he you have to be looking at 8 relative poverty interpreting a relative to the test situation of ok I see this refers back to Barack Obama so he is another mention of Barack Obama than understand is this concept of a nephron have this sort of like this either have these independent mentions which do refer to the same thing in the world their career any cases such as when they're full mentions like President Obama vs Barack Obama they don't have any text relationship it's just a happened refer to the same thing in the world and then contrast with case there's like Barack Obama said he would do something where the key has a text relationship actor Barack Obama and that's an example of a nephron is up until now and almost meaningless distinction that something that maybe gives you more of a sense of this something useful here is and these text relationships exist even when there isn't coreference mentioned before his cases like no dancer item no dance that doesn't have reference right it refers to nothing but if you have a sentence like no Dance The Twisted her knee well we have and Anna for here and that Anna for is referring no Downside doesn't have reference have the end of Foret text or relationship did you know her knee is there in a part of her and so these sort of part relationships again but honey in a sense about just come back to is also and Anna for which is interpreted with answer so we have two and a firework relationship here even though we have no reference interesting case relationships which are the same as reference Lucifer forms of anaphoric relationship sentences like this play concert last night the tickets were really expensive mention here of the tickets interpret the ticket interpret them with respect is switching back here a concert because really what this is saying concert with really expensive also referred to as an anaphylactic relationship where the meaning of The Tick put a text away based on another reference relationship the concert and the ticket for clearly two different entities place the cases are referred to as bridging and f because she should have have to supply for yourself the bridge that connects together the antecedent that's how we didn't have these pictures we have this done what's between coref that we sort of talked about one of the note on nephra if you done any ancient Greek and ancient Greek ok so play origins of the word and that's meant to be that you're finding your text your rest and so there's actually mentary which is referred to as where you're finding your reference after you so here is a beautiful example of catastrophe so this is from Oscar Wilde The Picture of Dorian Grey order of the divan of Persian saddle bags and which he was lying as was his custom innumerable cigarettes Watton could just the honey sweet and honey coloured Blossoms of a laburnum lights here we have this mentioned Lord Henry Wotton and there pause and that referred to Maud Henry water and and his come before and so these are referred to as instances of Iman end of classical Scholar you don't know what a laburnum yeah so this is Catherine out now there are two said things to say the first said thing is in modern linguistics the term tethera is completely disused and let's use the word enough for everywhere as meaning a word that gets reference from some of the mention that what side are on so he goes downhill one stage to linguistic NLP and ouigo downhill a second stage cause what you'll see is all the systems that people are building for resolution make any distinction of Direction at all at once you find a mention you're always looking backwards first reference and you've got no idea that while maybe sometimes you could what it means that the systems end up doing a saying well there's a here there are various other things there's the he is etc and you'll eventually get to Lord Henry Wotton and will be able to be trying to find its reference by looking backward Sword of il-4 many kind of linguistic Saints where's really he and his that should have been looking for their reference forward good after there any questions try and move onto kinds of coal tell you as much as I can and they have 45 about so the kind of models people build with coreference and I hope they mentioned quickly four different ways that people have looked at coreference I want to tell you a teeny bit about classical rule-based coreference then mention mention take care time on mention writing systems Which have tended to be the easiest simple systems and then just say a little bit about plastering systems Which should be the right way to do it but in practice has been away that's been hard to get the best performance from and the sky here is Jerry Hall retirement party from university of Southern California last night Gary Hobbs waybackwhen wrote a famous paper as in 1976 difference resolution and in that paper he propose what's normally now referred to as the hobs algorithm had actually in his paper he refers to it is a naive algorithm and how come back to that distinction and just a moment but what the hobs algorithm was is if you have a sentence say this this algorithm is just for finding the reference a pronoun so one can extend out to other cases for the part I'm going to show you it's just the part for doing the reference of pronoun so when you find out find a pronoun in you want to say what is it do is run this mechanical algorithm that's looking at a part of a sentence what to do with opium dominating the pronoun drop the trees of the first in POS cold is X and the path p traversal there's more but that's only beginning of it a lot more stages and but you know I'm not don't really want to go into the details of the how to try and explain the flavour of it he is a piece of turf all paid and a snappy dresser heated hair if you can remember any of the steps that algorithm here there Brown him and then what said to do was begin at the NP noun phrase above the pronoun and then it said to go up to the first noun phrase or it's above there vs about there and then what you're meant to do you're meant to go left to right through stuff that came before that Anderson's handwritten algorithm you like space of clever hand reaching out reflecting I think you should go to the closest thing to find reference but actually if you have in the same Centre more common for the sort of highest synthetic roll coreference with so you're more likely to be coreference with a subject and an object Ludovico reference with an object and something like a noun phrase in a that's inside a prepositional phrase that follows the start from the left here and we're going to say he is a noun phrase set the first one we come to and then there's this clever bit of ticks branches Explorer to the left left to right head sealant a noun phrase and that has a noun phrase or sentence between is an egg in the air this will be a candidate if an only if there is some other noun phrase or as in between and so what that saying is heated him cannot refer back to Steven moss and that sort of pretty much a pack of English syntax nothing to do is distinguish between another thing that we could have had here was a noun phrase possessive noun phrase and sided so if we had something like Motors I hated him in the Steven Moffat Moses mother hated him then that would in that case it be perfectly ok for him to be coreference with Stephen Moss algorithm allows that because relative to this noun phrase is another noun phrase above it in between Latin work c2c then so then we go onto the next step of the algorithm and in the next step says we should proceed backwards sentences on right to live is an important touristic the proximity is actually good heuristic to find coreference pronouns is usually close by overall and so we go to the first sentence back and then in this sentence again we go there within the sentence go left or right because there's the same kind of subject prominence roll and so we going to start in the centre ok he is a noun phrase what's in a different sentence there's nothing wrong with this one so we say we have a candidate Niall Ferguson is a possible and to see them and it's the first one we found and therefore we say the Kim refers back to Niall Ferguson and this algorithm actually gives the ride answer if you could follow along all of their those that sounds like a horrible handwritten stuff what's the weather where horrible handwritten star testing this algorithm couple of reasons is you know this is actually one of the first places in natural language processing that someone produce the Bassline for final projects and elsewhere in stuff we gave you and I did see now in in Opie and other areas that anything you're doing the first thing you should do is have a baseline a simple System and see how well it works and this was his simple rule-based system for doing a reference wanted to observe that actually this baseline was pretty good in actually gave the right answer a lot of the time and so the challenge was assistant that did better than this baseline and so he was well aware of it you know it was the dumb algorithm but he proposed that as a good baseline for doing coreference resolution interested in remember that we're back in the 1970s here was how to do knowledge base coreference resolution essentially what he was noticing is well these kind of synthetic factors that as mentioning prefer subjects prefer close by etc they are all useful predictors but there are lots of cases where they don't give the right answer and to know when they give went to know what's really the coreference saying you have to actually understand what's being described in the world have this Centre what's from the picture into the carpenter it was fall what is it with shut up thank you ok so that it refers to the cub but then left look at this example she poured water from the pizza into the cab until it was empty what does it refer to for the crucial thing to notice in these two sentences is this sentences have identical synthetic structure right so Jerry hubs algorithm can't possibly work for both of these sentences it's going to work for one of them but not the other one working from left to right within a sentence it's going to say the picture about x actually right so you can't get the answer ride by Jerry Hobson's algorithm and Jerry believed and still believe only way to get these kind of examples arrived is actually if you understand the world and you actually know what's going on in the world so you can see what this is talking about and no more examples like this and this is another very famous example the City Council refuse the women permit because they feared viola they refer to counsellors here's another centre so refused the win a permit because they advocated violence they refer to ok so this time first to the women and again you know identical syntactic structure it can possibly be gone right by the hobs algorithm so this particular pair of examples comes from Terry winograd and how long time originally in in opifex and he's sort of got dissolution with nop because there was making much progress turn off into the land of hci and the cat that became his career began as early work he was interesting these to know manar and came up with this example and so this example really stuck with people and said these kind of contrasts are referred to by other people is winograd sentences or winograd schema and so this is actually something that interesting is Revived recently so ha paper just five years ago now where he was trying to advocate to doing more in the way of knowledge and world modelling and artificial intelligence ring that then what's the problems that you just can't solve by the kind of cruise statistical method learning systems are using and that you really needed to do more world understanding and so he proposed that these winograd schema would be a good alternative to the Turing test measuring intelligence actually they just coreference decisions right so so there's a claim here that if you can do a coreference ride 100-percent of the time artificial intelligence in the two so that you can come code knowledge of the world into coreference problems yeah so people have been trying to work on these winograd schemas and was you no you just couldn't do these using kind of the kind of statistical factors the people put into the machine learning system song about that be work and both neural systems and otherwise has shown that actually can get a trivial distance with these kind of problems because you know if it is the case you know you can somehow see enough examples where the City Council refuses permits fearing violence you know if you've got if you're collecting your new language model over tens of billions of word in some instances of things like that and you could sort of predicted just the request is near how far can you actually get doing that without having a bit more of a world model you know what Hobbes was interested in way back in 1978 close the met you for brooches quite good play speaking it will be a long time before asymmetrically based algorithm is sophisticated enough to perform as well and these results said of very high standard for any other post waiting for it was told me right about that that really wasn't until the 2010 managed to produce an algorithm for pronominal anaphora resolution that outperform the Hobbs algorithm even though it was for a naive algorithm we might call a crude set of linguistic rules says yet there is every reason to pursue a semantically based rhythm top not work anyone can think of examples where it fails in these cases that not only failed he gives no indication that is failed and offers no help in finding the real and to see that food for thought there but standing there I'm gonna just rushing here at this point and tell you about some of the and statistical on your algorithms and that have been used for coreference resolution form of algorithm commonly used is what is called mentioned pay model dimension pay models is we're going to take pairs of mention playing a binary classifier that says is coreference or isn't coreference and so then we going to proceed left or right through the text free time when you mention we going to them evaluate at classifier with respect it every proceeding mention and we're going to say are they coreference and it's going to say yes or no and we going to find out the some of them it says yes for no because he was my values she said if we have a good classifier in all say yes to the two blue one estevan and so then will have a training time negative examples that neither and he and negative examples 4K res positive and negative examples and we can train a model and sofa training a model we have a sort of the classifier outcome is euro based on whether to mentions are coreference we go and have a reference model that predicts the probability of them being coreference and we're going to train it with the same kind of cross entropy are the places and I'm trying learning model that predicts coreference when we get to test time have a piece of piss run this classifier in it's going to say yes or no with some probability and we pick a threshold like .5 will add certain coreference links load of works pretty good but we're going to sort of completed off by saying well if as a reference to B&B is correct in really also as a reference to see so we gonna do a transitive closure and that will give us our clustering not here that there's a sudden danger in there means if we met with the transitive closure that's always heading clustering means the danger is that we can over Costa because if we make a single mistake and we linked things that should be so for example if we wrong we said he and Maya coreference then everything of this discourse with collapsed together into one cluster and everything would be deemed coreference oversized becomes up is well there some mentioned that the coreference and nothing riding the truth the real story there was a park which was just mentioned once in the tour this form of algorithm the classified ad say is no no no no no but all of the season nothing and then it's just a single to mention thought of works but hasn't proven to be the best way of doing career a lot of the reasons why it's not the best way to do coref cos we have this phenomenon of a nephron where we have text or depend on time that we're not wanting to make this all coreference decisions with like to make the net for decisions to say that he 18-in my is dependent on I the tsar never relationship just choose one example of what is the scientific notation that's lead to people then looking at what I called I mentioned pay model problem is that if we have a long document with lots of mentions we want to not be saying trying to find all of them and say yes he saying there's a want to be saying that there's a particular one so for the key at the end here it's enough for relationship is back to nature and you don't want to be trying to say this he is also a reference back to all of these other things that earlier in the text something that's been actually this is a case again pause forever for that seems like the right way the thing is that they have one depending on where is Trooper reference when you just have various mention Ralph Nader this round made of their leader dead exley dependent and they should always been grouped together as coreference but l models sort of don't normally try and do some one way and some the other way but you choose one of the models the other one the other way you do what's mentioned ranking sofa mention ranking the idea is mention we going to fight try and find as an antecedent that comes before in the texts front weird and we going to make a 1 of indecision ECC here we going to say ok what is this and we going to pick one thing that's coreference might be other than the tics doing that problem with Singleton mentions because if we trying to for every mention we find say choose the thing that came before in the text with witches coreference the right answer might be that there's no such thing is we add one additional dummy mention right at the front here the na mention SO1 choice say there isn't anything proceeding so fictive Lee when you get to eye since this is the first I will mention that paralegal to choose antecedent NI you then go onto neither and you have two choices you can either say it's CO2 I or in a the new mention a new entity that's been mentioned in the tour the new mention in the new entity being mentioned in the tour Becky and I have three choices nice things to say that's coreference Nida time it's for training a models it's sort of the same apart from this sort of this different one of semantics previously we wanted to say that for a classifier that is going to try and classify IMC and my and she and both of them had to get a high score without sufficient that just one of them gets a high score enough for us to do so what we gonna use as a good old soft Max and so for she we're going to put a softener the antecedents and I hope it's simply that we get a high probability with one of the antecedents if it has an antecedent or a high score within a if it doesn't have any prior reference when we doing classification at runtime we going to sort of only the highest scoring coreference link we train at a slightly differently because now what we going to do what we wanting to say is we want a hi of coreference between least one of the antecedent impossible models we can maximize this probability so for the ones that are coreference in the gold standard data we want the sum of their assign probabilities to be high what that mean is what's sufficient if we have one of them play probability and they don't all have to give a high probably so providing it's giving 0.9 probability stayed at one of the correct end of seed score we're going to turn that into a loss function in the kind of standard way we do and which we take log probability and then we want to or negative log probability minimise a mention ranking model at test time it's pretty much the same but as softmax classifier is just going to assign one and to see them for each mention weekend sort of give us the kind of clusters that we want and there's no subsequent clustering phase it's a big part of the owl which was I've just said ok we had this probability of mi in MJ as the are they coreference but I've sorted the Euro as to how you can determine when they're coreference or not so briefly the classical way of doing the classical way of doing it is you had a whole bunch of features statistical classifier which gave us score and diesel a kind of features you can use so there so strong features of personal number gender agreement so if you have a masculine or feminine pronoun you want to find an appropriate antecedent for it there weaker add ability features so the mining conglomerate the company disorder similar to a company you can do something out words of text similarity and assist there Hector constraint so this is then kind of like what the Masala bad is working at how likely different synthetic configurations going to mean coreference does the k what are these feature based systems used hobs algorithm as a feature inside the system there was waited and was normally very strong feature to Psycho reference the things you put in his features and recent see so John Winchell movie Jack went as well he was not busy the most likely reference for he is the closer candidate Jack mention subjects are more likely to be the antecedent John went removed with Jack he was not busy and John seems more like like and to see them so that's the subject pressure Allison Preston jakto movie Joe went with him to a bar Reiss the modern thing that him parallelism reason dealing with the subject to their various kind of linguistic features and constraints and so on and you can throw these all interested historical past fire and that sort of 2000s decade systems as the hell they were built recently people have built-in neural system please where kind of normally using the same kind of embeddings will have a candidate antecedent 11 buildings will have a mention that has embeddings and it will be something like average word vectors or something like that for the mention and we gonna feed these into a neural network that will give us Celtic score find is the most of these systems as well as having something like word vectors they also have additional features is features some of the things that were in there testable classifiers often features that reflect things like call relation does it's a subject it's an object that's something you could put into the features of a mention play some more likely to be coreference so might have additional features here which record how far apart dimensions are and those things get thrown in as well these kind of features are still important even in your systems now and what is the kind of current state-of-the-art for coreference resolution and this was a system that was done at the University of Washington in 2017 by Kent and Lee and assorted other authors doll hair was to produce an Intu MK reference system that was texting plasters that are coreference out and so they're wanting to use the server more complex network they can do the whole thing into and so I'll go through and the steps of there so the first step is we just start off with the word which word we going to look up a word and bedding for us and there's another stuff we've seen we also been put in a character Level CNN and the 2 of those we're going to give the representation of each token that much for look familiar then after that we going to run a deep bidirectional lstm back and forth across the sentence look familiar from stuff for that we've seen before Next Steps as a bit into doing something more special what they want to do after there is have a representation for Span I Stan we mean any sub phase of the word of the sentence so this is a span this is a span this is a span electric said the postal is a span every subsequence come back to that but you know they'll in principle you're working this out for every subsequence they want to come up with the span representation and so this spend representation is going to be in the Saint one of these top sequences please forget it's own representation and so the question is what we have the spend representation and it's going to be in these three parts here his pubs are is well first of all we going to have a representation which is just looking at the first word then I'm the last word of the span according to the bios team so we look at the spend the postal service dispiace TM and using them as part of the representation of the span that's a good start but then actually do something little tricky so kind of like when we're doing dependency parsing was well phrases are going to have a kid word I'm so bad if your sister the head word of that is sister and there if it's something like corner of the field the head word of that is going to be go find a way of capturing head words out of the Tech so what they going to do for that is use a tension play we have the span the postal service and we going to use attention as a span internal mechanism to serve approximator head to do what we going to do is we going to want to learn wait what we going to do is for this fan is it going to be learning based on names of the Spain which words to pay how much attention to so we can put a tension weights on the different words cancer in the usual attention way make this weighted sum of bird pit the bidirectional lstm peers through a feedforward network and end up with this new representation of a weighted representation and the hope is that in this case most of the weight will go on this Final server word and will be sore distributed across and and so that gives them turns the news both ends and hope to find the key word of the mention that's two-thirds of the span but they still have over here this additional features and so they still have some additional features they want to be able to Mark speakers and addresses they want to Mark other things like the grammatical role in is still useful to have some additional features play this isn't representation of each span and then they're going to want to say R2 spends coreference have one score for the two spans which is essentially saying is that a good mention and then you're going to have scores of do they look coref so having calculated this representation then you running through things through a fully connected feedforward network weight vector and that's giving you a is that a good mention score and then are they coreference your taking to span pointwise hadamard product of two sperm features another neural network and that's them giving you are these two spansko reference pieces an overall loss function so you can say that your model is play lstm we're gonna take all sperm no this the gold ants for our reference system and Zoe sing things for the co reference and have a loss probability that we calculate with these scores as I mentioned ranking model using a soft next lost likely for so if you put all of this together and training them too and you've got a hooker reference system that goes from words to coreference decisions is a huge problem with their which is if you actually applied this naively while the problem is the number of spans in a piece of toe player of the length of the text in word you're making coreference decision green a of Spain algorithm O2 the fort really really computationally impractical so this point they sort of say well actually we do want to use down now so it all and we want to work out how likely different things are to be mentioned play and then they're putting in a lot of pruning decide which sperm things that they in some senses a little bit of he tried cos really is pruning step here is ok we're going to stick in a attentional system genus alburnus in terms of birth in terms of the loss function that defined the loss function is really defined into Wayne from the mansion making decisions in to win model even though I'm practice to make it practical you have to have something I can mention to take that to get it to work mention a model in mention writing model and and sofa both of those you just take it individual mentions and saying he is another mention do with that let's look at mentions and see if we can reference to each other and your concept of entities which are classed as dementia one-off decisions between peers and mention the entities as clusters just emerged as a consequence of those mentioned pay decisions list of long-standing feeling that can't really be ride the right way to do it must be really to do it as a clustering task and people often refer to this as saying we want into these as first-class citizens want to be sort of putting together two clusters that represent the entities in the obvious way to do that is to do a kind of bottom-up agglomerativeclustering so you start off by saying each mention is its own Singleton class star and then you're making decisions to me which is initially I'm saying to mention zoco reference but as you go on with that you're then making decisions that two clusters are coreference or not here is to have a piece of tape play blow blow blow blow the company announced Google plastic love of a blur the product features brother blur and so you have here some Minchin sticking to a start-up saying that OK that is for mentioned that eat their own plaster and then what we going to do is we going to make some decisions decided these two clusters and and merge them into one cluster we might decide that these two Thai restaurant into one cluster progressively clustering and so then we get a look at these two clusters plus the two and say no we don't think those ones are coreference and therefore we going to keep them apart and so you're reference algorithm stop I'm left emerge why people think this is the right thing to do is the feeling is that if we sort of Bill partial classed as like this how to do a better job because if I just saw disable here to mention Google class by regard reference or not since your smart human beings know what Google is not Google Plus no of course not let you know if you're just a computer trying to make a decision it's on hard to know the right answer because there are lots of other cases when there are shortenings where the right answer is that there coreference right because it's been Google and Google call beam write to regarding this coreference or if it was sort of Hillary Clinton and Hillary it would have been write to regarding a scary friend so I can offer me kind of hard to tell what's a rest friend but the hope is that if you've made some of the easy decisions firstsave decide Google on the company account reference and Google should be much easier to tell and say well product and Company they're definitely different things and therefore that is the goal and so doll the kind of models people build in a selection model that Kevin Clark is on the PhD students here couple of years ago the idea as well what we going to do is were initially going to consider mentioned peers and build some kind of distributed mentioned pair representation which is kind of similar to what we're doing previously with the previous model the goby on there and come up with cluster representation cluster pair representation I hope that by looking at these cluster representations will be able to make Better Decisions of what to merge of what next emerge do more slides that go through the clock rhythm but I'll save just a few minutes left I think I'll skip the details main thing that's interesting here is the idea of clustering algorithm principal it should give you extra and that's so the main useful thing to get through that's what I want to make sure we have covered in the last few minutes and I've said nothing at all about is how do you evaluate coreference resolution and how well does it work so let me skip a head to that if you look at reference resolution papers or something like that a mini metrics that people have used to evaluate progress play alphabet soup of name so there's Muck and safe Benelli A and B cubed and things like that so effectively part of it is if you look in the clustering Witcher there are lots of ways that people trying to evaluate clustering and essentially any of those m as you can to reference evaluation I mean white kind of difficult is the the situation you have is that you have a gold standard which fix out certain cluster picks out certain clusters and get some result like this you have to decide how good it is I just quickly one particular algorithm b-cubed algorithm uses precision and recall and f-measure like with thought of before so it look turn the five by the system and it says well this class star is cluster 1 so the position is 4/5 surely there 16 and gold cluster one it only has a recall of 46 plaster and then it simile on the same kind of calculation and then it's going to average across the precision and recall come up with an overall b-cubed score this from an algorithm this is actually tricky because I sort of said ok this classed as mainly gold cluster one so that means you have to do a bipartite graph alignment between system clusters and gold clusters the hidden hidden inside this evaluation and system is action in pee complete problems but in practice you can normally do it here aesthetically Welwyn method runs and work and so the kind of thing to notice is that if you under cluster you automatically get great precision but you get bad recall and if you over class that you get get great recall and everything that should be in the same Costa you get terrible precision and so what you want to be doing is balancing those two things 2 min are my dear performance so these are results from the ontonotes dataset which is about 3000 document unless labelled for coreference the scores are reporting sexy and every service three metrics one of testo due 4BQ there is some numbers 2010 was the Stanford system so there is a shared task of evaluation of coreference system believe that Jerry Hall find with rule based coreference 2010 we managed to beat all machine learning systems with a role-based coreference system and we were proud of sits performance right here in subsequent years people did start to do a bit better with machine learning systems but as you see not very much right for these 2000 system on someone better this one really wasn't better a bit of progress 1015 Amistad in to be neural systems so the first neural system I vaguely mention system and the numbers are going up into the mid 60s and it's just the Kent and Lee system that does the end when you're close loaders from what's the stuff far from a solved problem I'm so if you want to have a bit of fun and you can go and try a reference systems for yourself Stamford one on the first link or the one from huggingface as a good modern coreference system as well and if you just try this out some some pieces of text you know they still get lots of things wrong and there's still more work to do this does the hard and language understanding kind of like Jerry Hobson add earlier Observer for now yeah I shouldn't have reminder invited speaker next Tuesday and so we taking her attendance for advisors speakers 