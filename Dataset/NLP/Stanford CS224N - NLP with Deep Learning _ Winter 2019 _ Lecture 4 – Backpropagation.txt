see everyone back for lecture for the class today's lecture what I want to do for most of the time is actually get into that heart of these ideas of having the back-propagation algorithm for pneumonia construct computation graph efficiently to do backpropagation your nets the train then you'll know troll what I plan to do it today so at the end of last like try slightly ran out of time and I started mumbling and waiting my hands about the respect the white gradient so I kind of wanted to do that bit again so hopefully that actually communicate slightly better 2002a bit more about sort of just tips for doing matrix gradients a particular issue that comes up with word vectors choleric the back-propagation algorithm and how it runs over computation graph last part of the class play the this is so just a grab bag of miscellaneous stuff you should know about neural network neural network you know we dream of a future of artificial intelligence where our machines are really intelligent and you can just say to them this is the data and this is my problem go and trained me a model and it might work and then some Future will that maybe and what comes the one that's something that's only been search for the moment and of the topic of automl I guess the question is whether it turns out that automl was a scalable solution or the climate-change consequences of automl techniques is sufficiently bad that someone actually decides that these muscle lower power Systems by Lexi be better still for doing some parts of the problem either way we're not really there yet and the fact of the matter is when you're training you all networks there's just a whole bunch of stuff you have to know about initializing shin and nonlinearities and learning rates and so on and you know class last time the people would pick this up by osmosis that have code 2 people start a code we initialise matrices and reset a learning rates that by osmosis people and understand that's what you have to do Hammond in release order the practical tips and tricks and obvious that when we got the final project I'm so at least for quite a few people osmosis 10th work so this time at least one of the few minutes on there and we point out some of the things that are imported and and I mean just in general you know the reality of 2018/2019 2019-2 no of techniques of doing things that lead your net training to work successfully as opposed to your models and failing to work successfully one final announcement and I going to her so we've sort of being doing some further working on office HR placement and I guess they're sort of multiple issues which include opportunities for local icpd students with outstanding ideas be able to get to office hours for the Thursday night office hour and that's after this class if you'd like to go and talk about the second homework that Thursday night offer Sarah is going to be in Thornton Thornton one tin now I didn't know where Thornton was sense to me when I translated that has that the old termon NX that's probably just showing my age none of you remember when they used to be a building called termon that probably doesn't help you either but you know if you're heading I don't know Direction with facing if you're heading that way I guess if you know where the Poppy and Eugenie sculpture garden is and the son of open grassy area before you get to the population of guinea sculpture garden that's with him and used to be in the building at still stands in there is Thornton Thornton 110 and tonight I think it starts at 6:30 right 639 ok finish off where we were last time so remember we had this window of five words metronet layer of the = w x + V nonlinearity of 8 = 8 of I'm going to go to get a score as the weather this hasn't at Centre name did the like Paris which is sort of taking the stock product of a bit the Times the hidden layer so this was a model going to work out partial derivatives of S with respect to all variables and we give various of the cases the one we handed done is the weights of the way to all of this new net letter here ok so changeable the parcel of the sdwa is DS X HD ehdc x dcw and well if you remember last time we should have done some computation of what those first two and partial derivatives were and we could said that we could just call those delta which is the signal coming from above concept of having an error signal coming from above is something I'll get back to the main part of the lecture and the sort of Central notion that the baby haven't dealt with is this dcw and we started to look at that and I made the argument what shape convention that the shape of their should be the same shape of their w matrix so it should be and the same in times in shape as this w matrix so we want to work out the parcel of the sea which is the same as the you express bdwm so it won't work out what that derivative is that's not obvious one way to think about that is to go back to this elements of the Matrix and actually first off work it out elementwise and think about what it should be and then once you sort out what it should be and to realise it back in Matrix form to give the compact answer so what we have is we have these inputs here and a bias term and we going to do the Matrix multiply of this vector to produce the about what's happening there so we've got this in Beatrix of Wade particular wait await is first index is going to correspond to a position in the hidden layer and it's second India respond to a position in the input vector and one white in The Matrix being part of what used to compute one element of the hidden layer so the one element of the hidden layer you're taking a row of the Matrix and you're multiplying at by the component of this vector so they some together in the buyer said on but one element of the matrix is only being used in a computation between one element of the an element of the hidden vector so that means I'm if with thinking about what's the partial derivative with to the Spectre waj well it's only contributing to the I bring anything with XJ end up respect to wiha we can work that out with prospector just to his first spectre.ai and when we're going to look at this multiplication here what were ending of this sort of summer terms Wik x x k where this sort of what's the Matrix going across the positions of the Victor so the only position in which wajs used is multiplying by XJ and adept interns the sudden you know base doing a differentiation this is just like we have 3 years and be safe what's the derivative of 3 mattress Texas confusing so I can say that it's likely have free w&e what's the derivative of 3w with respect to w it's three right so that we've ever term here which is Willoughby why x x j in its derivative with respect to why is just XJ ok so an element of this matrix with just getting out XJ and at that point well cos we want to know what the jacobian is for the full matrix w well if you start thinking about this I command applies to every cell so that for every cell cobian 4w it's going to be exchanged so that means be able to make use of that in calculating out of Cody and so the derivative for a single waj is Delta I exchange a and that's true for all cells so he wanted to have a matrix Friday cobian which has delta I exchange every seller bad and the way we can create that is by using an out of products so if we have a row vector of the dildos the error signals from above and a column how long delta error signals from above and we have a row transfers vector when we multiply those together we get the out of try exchange sell and that is out akobian answer for working out the Delta is still 2w that we started off with the beginning we get this form with the multiplication of an error signal from above and I'll computer local gradient signal and that's the pattern that we're going to see over and over again and it will exploit and our conversation grass all good so he's just who's your meant to do some of this stuff here just over a couple of collected My Hope will help I mean pack of your variables and their dimensionality is really useful cos we just can work out what dimension out these things should be too often kind of halfway there I mean basically what you're doing is sort of applying the chain roll over and over again it's always looks like this but doing it in the sudden matrix calculus homework you have to do a soft Max which we haven't done in class I think you'll find useful if you want to break apart the soft the two cases won the cases to when you're working out for the correct class the Abacus the other incorrect classes yeah and derivation I did before I said well let's work out and elementwise partial derivative cos that should give me some sense of what's going on what the answer is I think that can be a really good thing to do if you're getting confused by matrix calculus skip last time I was talking I talked about it for a moment the homework I'm so however you want you can work it out and terms of you no numerator ordered jacobian said that seems best to you but would like you to give the final answer to your assignment question convention so the derivatives should be play as the variable with respect to which you're working out your derivatives the last little bit for finishing up this example from last time I understand little bit of it is what happens with words one answer is nothing different but another answer is they are a little bit of a special case here because you're really we have a matrix of word vectors right we have a vector and so then you can think of that has sold his matrix of word dexterous which roller two different word but we're not actually kind of connecting up that matrix directly to our classifier system instead of that what we connect connecting up to the classifier system is this window and the window will have in at 5 word different words that only the same word might appear in two positions in that window we can do exactly the same thing and continue our gradients down and say ok let's work out the gradient of this window vector and not to mention de will have this sort of 5D and Victor but you know can you do about it and the answer to what we do about it is we can just turn the 5 pieces and say a ha we have 5 updates to word victors we just going to go off and apply them to the word if the same word occurs twice window we literally applied both of the update so give his gets updated twice or maybe actually want to someone first and then do that plants play so spell actually means is that we are extremely sparsely updating the word vector matrix because most of the word vector matrix will be unchanged and just a few rows of that and will be being updated going to be doing stuff with pytorch around pytorch even Hudson official store likes passes gd4 meaning that you're doing a very sparse updating like that but there's one other sort of interesting thing that you should know about for a lot you do it that's what actually happens if we push down these Grady word vectors well the idea do that I'll be just like all Avenue ALLNet learning third we will sort of imprints the ball say move the word vectors around in such a way as they're more useful in helping determine named entity classification in this case because that was downloaded 18 example so you know it might for example learn that the word in is a very good indicator of named into the fault outside the place-name following so after in you up and get London Paris etc right so it's all got a special behaviour that are the prepositions down to see a good location indicator and so it could sort of I move its location around and stay here are words that are good location indicators and therefore help out classifier work even better small beds good and it's a good thing to do to update word vectors that help you perform better on a Superbike named entity recognition classification is that it doesn't always work actually and so why doesn't always work well suppose that we're training a classifier yeah it could be the one I just did or a soft NEXO logistic regression and we wanting to classify use sentiment for positive or negative if we have trained outward vectors but some word vector space word vector space play telly and television old very close together because they mean by Sookie the same thing a word text as a god suppose that was the case our training data for our classifier to this is how training data for movie sentiment review we had the word TV and telly but we didn't have the word television can I happen is well try and train our sentiment classifier if we push gradient back down into the word vectors what's likely to happen is that it will move around the word vectors of the words we saw in the training day that televisions not moving right only pushing gradient down to words are now training day that so this word goes nowhere to it just stays where it was all along about training is in world's get moved around so here a good words for indicating negative sentiment actually it when we running out model if we evaluate on a sentence with television in it is actually going to give the wrong answer haven't changed the word act as a tool and it doesn't let them wear out learning system put them then it would have said television that's a word that means about the same as TV or telly I should treat at the same in my sentiment classifier and it would actually do a better job 12-sided weather you gain by vectors what is a summary spiders and practically what you should do first choice is g is it a good idea to you the doctor's like the word addictive that used in assignment 1 or using the training methods that you're doing right now for homework to that is almost always yeah that is these words that the training methods are extremely easy to run on billions words of text Zoe you know train these models like Glover word billions and billions of words and it's easy to do that for two reasons firstly because the training algorithms are very simple right that the word defect training algorithm skip grand very simple algorithm secondly because we don't need any expensive resources all we need is a big pile of text documents and we can run it on them so really easy to run it on you know 5 or 50 billion words you know we can't do that for most of the classifiers that we want to build cos if it's something a sentiment classifier and named entity recognizer we need labelled training data the trainer and then we asked someone or how many words of labelled training day that do you have for named entity recognition and they give us back a number like 300000 words or 1 million word orders of magnitude smaller I'm so therefore amusing pre-trained word vectors because they know about all the words that aren't now super buys classifies training day there and I also know much more about the words that actually are in the trading day there but only really action for that if you have hundreds of millions of words of day that then you can start off at random words company done is for machine translation which we do later in the class it's relatively easy for large languages to get hundreds of millions of words of translated text if you want you build something like a German English or Chinese English machine Translation system not had to get 150 million words of translated text and say that sort of sufficiently matchday that that people commonly predictors and being randomly initialize and start training the Translation system second questions ok I'm using pre-trained word vectors play my supervised classifier should I push gradient down into the word vectors and up and update them which is often referred to as fine-tuning word vector try not tried Milos gradients and not push them down into the word vectors and you know the answer that is if the pins and it just depends on the size so if you only have a small training dataset play it's best to just treat the predictors is fake stop doing the updating of them at all large dataset then you can normally gain by doing and fine-tuning of the word vectors and put the answer here is what counts as large turn Lee if you're down in the regime of 100000 words couple 100000 words you're small if you're staying to be over a million words then maybe your large but you don't practice people do it both ways and see what number is higher and that's what they speak then the sun pointy that's just worth underlining is yeah don't principle we can back propagate this gradient to every variable and a model theorem that we can arbitrarily decide to throw any subset of those away improving the log likelihood of AR model not being consistent even does so pick some subset and say only train those 37 throw away all the rear animal still improve and the log likelihood of the model pets not as much as if you trained the rest of her variables as well but you can't actually do any harm not strain anything one of the reasons why often people don't know those bugs in their code as well is because if your code is kind of broken and only half of the very involved being updated you'll still seem to be training something and improving and it's just not doing as well as it could be doing if you coded correctly this point that sort of show new backpropagation writes a backpropagation is really taking the river those with a generalized chain roll with the one further trick which was sort of represented with that delta which is g you want to be ring this so you minimise complication by reusing shades on the move on is the sun look of how we can do that much more systematically which is this idea we have a computation graph and we're going to run a back-propagation algorithm through the computation graph this is kind of like an free expression tree that you might see Anna compilers class or something like that right so when we have an arithmetic expression of the kind that we're going to computer we can make this outside tree representation so we got the xnw very Volkswagen and multiply them that would be variable we can edit the previous parcel resolved we're going to stick it through online when they're ready x you and that was the computation that we're doing in the neural network snow to import terms of history at operations and then we've got these edges that pass along the results of their computation and so this is the computation graph for precise the example I've been doing for the last two things that we want to be able to do the first one is 3 one of the above start with these variables and do the computation and calculate 1 SD the Dead simple that's referred to as forward propagation so for propagation is just expression evaluation as you do in any programming language interpreter that's not hard at all the difference here is hey we want to do a learning algorithm so we going to do the opposite of that as well what we want to be able to do is also backward propagation or backpropagation or just profits company called which is we want to be able to go from the final part the final part here and then at each step we want to be calculating these partial derivatives and passing them back through the grass sort the notion before that we had an error signal right so starting from up here with calculated a partial with a spicy which is this the bed and so that's all of our calculated error signal up to here and then we want to pass that further back to start and computing gradients further back start with the partial of sfas what's the parcel this Spotify as going to be the rate at which this changes the Razer switches changes that we just start off at 1 and then we want to work out how this gradient changes as we go along so here is when we working out things for OneNote that a notice can have passed into its upstream gradient which is that error signal so that's the partial of our final salt which was out last and by and the the variable that was the output of this competition know so that the parcel this by 80 operation here he is the non-linearity but it might be something else wanted them work out is a downstream gradient which is the partial of if spyzie which was the input to this function how do we do that the chain rule of course right so we have a concept of all local gradients so he has h poured and the import for this function here this is our non-linearity right so this is whatever we using the nonlinearity like a logistica A10 age we calculate age and terms of z and we can work out the parcel hyz local Grady if we have both the upstream gradient and the local gradient we can then work out the downstream gradient because we know the partial other spicy is going to be the sdh X and the HTC will be up the pass down the downstream gradient to the next node a basic role which is rainbow written in different terms is downstream gradient = upstream gradient x local gradient is that this simplest case where we have a node with one input and one output that's a function and like a logistic function but we also want to have things work out for a general computation graph so how are we going to do that well the next case is what about if we have multiple inputs so if we calculating something like the sequel x play XYZ nx2 themselves vectors and W Matrix important we was an import MC is out portrayed with Cannock group vectors and matrices together well if you have multiple input with multiple local gradients even work out and the pastor of the respective softie with respect to WM so you essentially you take the upstream gradient * each of the local gradients and you pass it down their respective path and we calculate these different downstream gradients to pass along sense what sort of looking in example of this and then we'll see one of the case so here's a little baby example this is some kind of really looking like a new Ned but we put x y and z and X and Y get added together and YMC get Mac take the results of those two operations and we multiply them together so overall what recalculating is x + y x see we have here a general technique and we can apply it in any cases ok so if you want to have this Square from we want to run at forward well we need to know the values of x y and z so for my example x = 1 y = 2 = 0 so the values of those variables and push some on to the calculations for the Ford arrow then well the first thing we do is add and the results of those three and so we can put that onto the arrow the app for the bed next to as the app for the value of x is what part we have evaluated expression its value is 6 add ok so then the next step is within want to run backpropagation to work out gradient and so want to know work out these local gradient highest the result of some so he is the result of Sam so a = x + y so if you're taking the adx that's just one and d a d y is also one that makes sense the Maxus slightly tricky because some wipes and gradient for the max depends on which one is bigger so why is bigger than Z BBC TV wires one otherwise it's 0 and conversely and for the parcel viz that one's a little bit depend on and then we do the multiplication and and work at A&B set a B which has the values 2 and 3 if you're taking the parcel diff by a people by which is 2 and vice versa ok so that means we can work out the local gradients and each know and so then we want to use those the gradient backwards in the backpropagation pass the top the partial diff with respect to it is one because if you move the fy10 so that's her Council down as one anyone passed backwards ring that we have is this side of x nose and so we know it's Michael gradient the password if by a is 2 and the past roll of it by B is 3 and and so we get those values so formally we taking gradients multiplying by the upstream gradient and getting out 3 and 2 effectively what happened is the values on the two acts swaps and then we saw it continue back ok there's a Max note so upstream grading is now free and then we want to multiply by the local gradient and sin these tours to as a slope of 1 on this side so we get three there's no gradient on this side and we get 0 and then we do the similar calculation on the other side where we have local gradients of one and so both of them come out of 2 other thing to do is we nos there are two arcs that started from the why both of my OK did some gradient on and so what do we do about their what we do about that is we some the parcel death by X2 parcel if by The Zero the partial define why is the sum of the two and five this isn't complete voodoo is this something that should make sense in terms of what gradients are right so what we're saying is what we calculating is a few wiggle extra little bit does that have on the outcome of the whole thing we should be able to work this out so our external offers one but let's suppose we wiggle up a bit .1 well according to the what should change by about 0.2 it should be magnified by 2 and we should be able to work that out right then 1.1 so that's that 1 to hear that x 8 and 6.2 and 1 and the whole that went up I point to Simms career play the same let's see it's easy to see which had a value of 0 by .1 this is 0.1 when we met said if this is still too and so calculated value doesn't change it still see the gradient is zero wiggling this does not turn the final one is why so it's starting off that you was too so if we go with a little and make it 2.1 our claim is live results for change by about .5 should be multiplied by 5 times .1 within 2.1 Willoughby 3.1 cared also be 2.1 and so is have 2.1 x 3.1 lyrics particularly doing my hair we take do .1 time 106.5 one I'm so basically it's gone up by half we don't expect the answers please exact of course right because he knows that sort of a catalyst works right it's showing that we getting the gradient right ok so this actually work what are the techniques that we need to know so already seen them also you know we discussed when there are multiple incoming arcs how it so work out the different local and derivatives the main other case that we need to know is if in the function computation is a branch outward the result at something is used in multiple places and so this was like the case here I mean here this was an initial variable that you know it could have been computed by something for the bag does this thing is used in multiple places and you have the computation going out I'm it's just a simple roll that when you do back-propagation back the gradients that you get from the different app with branches = x + y showed you before that we doing the some operation to work out the total possible of this by why and if you think about it what's the little bit more there thought of these obvious pattern it's very simple example so if you've got a crush really the upstream gradient start of heading down every one of these Grant branches when you have multiple things being sung in this case copied unchanged that's because they are computation was Express y in it could be more complicated with passing it down down each of those branches so plus distributes upstream gradient like a racing operation because Max is going to be sending the gradient to in the direction that's the man is it going to get no gradient being passed down to them and then when you have notification this has this kind of fun effect that what you do a switch the gradient riding so this reflects the fact that when you have you X the regards via bektas the derivative of the result with respect to you is the and the derivative of a result which respect of these you and so signal is the different sides so this is so we have these competition graph workout backpropagation backwards in mum part of this to.do.im which is the same g we want to do the subway to do this which is the sale he wanted to calculate the parcel this by be and so we can calculate that partial which was essentially what I was doing on last time slide if 5 equals the partial of a spy 8 times the partial of Hz the bible we have all of those parcels we worked them all out and multiply them together and then someone says and what the parcel of spiw and what's the channel again I'll do it all again the parcel of if my age x z x the partial bye what's that you do with a long list of them and you calculator with him that's not what we want to do instead we want to say this shed stuff signal coming from above then we can work out the error signal the upstream grading for this no we can use it to calculate the upstream grading for this no we can use this to calculate the upstream gradient for this node and then using a local gradients of which there are two calculated this know we can then calculate this one and that one and then this upstream gradient we can use the vocal grade in this note to computer this one and that one and so we saw him doing this efficient computer science like computation where we don't do any repeated work that make sense ok sorry that is the whole of backprop so the slightly sketchy graph which is sort of just recapitulating this thing you have computation that you want to perform well source for the topological thought which means the things for that argue arguments sorted before variables that are results that depend on that argument you have something that is an acyclic graph you'll be able to do that I could rest you're in trouble well have you actually technique people used to roll out photographs but I'm not going to go to that now so we sorted the note which is kind of loosely represented here from bottom to top in a topological sort area ok so then for the Ford Probe we should have go through the nose and they're topological sort order and we Terry Bowie just said it's value to variable value is if it's computer from other variables their values must have been set already because there earlier on the topological sword and then we compute the value of those notes predecessors and we pass it up and work out the final output the loss function of down your network and that herself forward pass after that we do have backward pass and so for the backward pass we initialise put gradient with one the top thing as always won the pastor of the with respect to see and then we now go through the nodes in Reverse topological sword and so therefore each of them will all anything is everything that we calculated based on in terms of the fordpass will already have had calculated it's gradient of the product of upstream gradient x local gradient and then we can use that to compute the next thing down and so basically the the overall role is for any node you work out at set of successes the things that are above at that that depend on earth and then you say ok the path of the with respect to some Cecil local gradient that you calculate at the Node x the upstream gradient of that node examples that I gave before there was never upstream gradient if you're messaging a general big Grey so different upstream gradient for the being used in for the various successes apply that backwards worked out navigation and the gradient of every the final results of the with respect to every knows now graph and the thing you notice about this is if you're doing at right and efficiently the big o order of complexity of doing backpropagation is exactly the same as doing for propagation I expression evaluation so it's not some super expensive complex procedure that you could imagine doing and scaling up and you're actually in exactly the same complexity order so as I pretended here this procedure you could just think of something that you're running on an a graph collating this for passing the backwards past I mean almost without exception that the kind of Newell net to be actually I use have a regular like structure and that's been precisely what makes sense to work out these gradients and terms matrices in jacobian of the kind we were before we have this sort of a really nice out with them now this sort of means that we can do this just computationally and so we don't just had our computers do all of this with this so using this graph structure and we can go play work out how to apply and bet prop and the son of two cases of the what's the weather in each node is given as a symbolic expression we have our computer workout for that symbolic expression is so it could actually calculate the gradient of that node and it's referred to as often as automatic differentiation so this is kind of like Mathematica Wolfram Alpha you don't do your maths homework on that you just type in your expression so what to do if it and give it back to you right it's working during symbolic computation and working out the derivative for you listen to be used to work out the local radio use the graph structure and our role upstream gradient x Michael gradient of downstream gradient i.e. the chain roll to then propagate through the graph and do the whole backward pass completely automatically so that sounds disappointment learning frameworks don't quite give you there famous framework that attempted to give you that so the theano framework was developed at the university Montreal those they've now then and in the modern era of large technology corporation deep learning frameworks piano did precisely that I did the full thing of automatic differentiation reasons that we bad current deep learning frameworks like tensorflow pytorch actually do a little bit list of there so what day do is say well for an interpreter computations an individual node you have to do The Catalyst for yourself individual node you have to write the Ford propagation they you know return x + y and you have to write the backward propagation saying the local gradient Canon 1 to the two inputs X and Y the provider what someone else has written out the forward and backward local step this node then tensorflow pytorch does all the rest of it for you and Munster back propagation algorithm and then play that song haven't have a big symbolic computation engine because the person coding the load computations is riding a bit of code as you might normally imagine doing it weather in Uno see your Pascal I'll staying return play and you know local gradient return one ride and you don't actually have to have a hug to bite computation in German let me stay over picture looks like this I'm schematically we have a competition graph how to calculate the Ford complication we inputs into a computation graph where they're sort of X and Y variables run through the nodes and topologically sorted order and for each node we calculated forward and necessarily the things that depends on them have already been computed and we just to expression evaluation forward and then we return the final gays in the graph which is their loss function of depth of function have the backward pass past we go in the nodes and reverse topological only sorted order and for each of those nodes we return their back without you and for the top no we return back with area of 1 give us our gradients that means knows any piece of computation that we perform we need to write a little bit of code that says what it's doing on the Ford pass and what it's doing on the backward pass on the forward pass what's the down multiplication so it just saying return x x y that's pretty easy that's what you used to doing but we also need to passes local gradients of return what is the partial of l with respect to the end with respect to x do that we have to do a little bit more work a little bit more work first of all on the forward pass Ford parts embassy in some variables what values we computed in the are used with giving up to us in the Ford pass for us we won't be able to calculate the backward pass so we store away the values of x and y and so then when we doing the backward pass into gradient the error signal and now we do calculate an upstream gradient x local gradient upstream gradient times local Grady end and we return backward stream gradient providing we do that for all the notes about graph and we then have something that the system can run for us as a deep learning system that means in practice any of these deep learning frameworks come with a whole box of tools that says here is a fully connected forward layer here as a signal unit here as having more complicated things will do later like convolutions and recurrent layers and the extent that you using one of those does this work for you either Dave the find turn salon light off now wooden deck with already written foot for them and extensive that's true that means that making your nets is heaps of fun it's just like LEGO right you just stick these layers together and say I'm tired and trainer so easy that my high school students building these things right don't have to understand much really to the extent that you actually want to do some original research and think I've got this really cool idea of how to do things differently I'm going to find my own kind of different computation will then you have to do this fast and as well as sort of saying had a computer for Dell you you will have to pull out your copy of will for an Alpha and work out what the derivatives are and put that into the back of pass so he is just one little note on there in the early days of deep learning say Prior 14 what we always Sister Stay do everybody very sternly all your gradient by doing humera gradient checks it's really really important and so what that meant was well you know if you want to know whether you've coded your backward pass right right is to do this radiant wear your son of estimating the slope by wiggling a bit for a bit and seeing what affected has so I'm working out the value of the function f of x + age 48 very small white Eden - 4 and it's a big smile bye-bye 289 saying what what is the Spice at this point and I'm getting a numeric estimated the gradient with my variable x here this is what used Willis scene in high school when you did the sort of fur that's a gradients where you sort of worked out if it's Plus he knew doing rise over run and got a point system of the gradient and exactly the same thing except for the fur one-sided like they're we doing it 2-sided it turns out that you're actually want to do is asymptotically hugely better and so you're always better off doing two sided gradient checks rather than one side of gradient since you said it's hard to implement this wrong this is a good way to check that your gradient said correct if you have to find them yourselves as a technique to use it for anything it's completely Completely hopeless because we're thinking of doing this over our deep learning model for a fully connected layer what this means is that if you've got this side of like aw matrix of in my in and you want to you're partial derivatives to check if their career you have to do this for every element of the Matrix so you have to calculate the eventually NW1 1NT going W12 then gw13 140 so you have in the complex network you end up literally doing millions of function evaluations to check the Grady time so size 4 bat prop and I said it's just as efficient as calculating the forward value doing this is computation time x number of premises now model which is often huge for deep Learning Network you only want to have late Man City could turn off so you could just a run at to check the door code is an IMD buggy in honesty this is just Matchless needed now because you know by and large you can play together your components and layers in pytorch and the code right and it will work I'm so you probably don't need to do this all the time is still useful thing to look at to know about if things are going wrong master the core technology of neural nets we still no and basically everything we need to know about neural nets and I sort of just summarise that they're just a source of emphasise s'more people think even let need to learn all this stuff about gravy really because these modern deep learning frameworks will compute all the gradients for you you know we make you suffering homework to banana homework 3 you can have your gradients and computed for you but you know so you know just like well why should you take compilers writing is actually something useful in understanding and what goes on under the hood even though most of the time we just perfectly happy to let the c compiler do it's thing without being experts on X86 assembler everyday of the world more to it than that you know even though best propagation is great once your building complex models backpropagation doesn't always work as you would expect it to perfectly maybe the wrong word casino mathematically perfect but it might not be achieving what you're wanting it to end well if you want to sort of in debugging improved models it's kind of crucial to understand what's going on then there's a nice medium piece by Andre capacity of yes you should understand backprop and that's on the syllabus page and that talks about this indeed week after next a actually go to lecture about recurrent neural networks and you're one of the places and where you can easily fail I'm doing that propagation turns out there is a good example any questions about play some computation graphs if not the remainder of the time is grab bag of things that you really should know about if you're gonna be doing deep learning and so yeah this is just itsy-bitsy and that let me say them so up until now when we've had lost functions and we've been maximising the likelihood of our day there and stuff like that we've sorted just had this part here which is the likelihood of Our Data and we've work to maximise it never in practice that work usually and we need to do something else which isn't regularized are models and if you've done the machine learning class so something like that you will seem regularization and their various techniques to do regularization hey to anything else regularization even more important and the Deep learning models rights general idea is if you have a lot of parameters in your model those premises can just essentially memorize what's in the day that you trained at and so they're very good at predicting the answers very good at predicting the answers to the day you traded on but the model may become poor at working in the real world and different examples how we want to stop there this problem is especially bad for deep learning models because typically deep learning models have vast numbers of premature days when statisticians rule the show they told people that it was completely ridiculous approached your number trainee examples you know you'll never have more parameters and your model and 1/10 of the number of your training what's the kind of Campbell's of family were told her that you had lots of examples with which sister made every parameter that's just not true with deep learning model common we trained deep learning models that have 10 times as many parameters as there we have training examples but miraculously it work brilliantly those highly over premature Rise models secret sources of my deep learning as being so brilliant only works to be regularized the model if you train a model without sufficient regularization what you find is that your training and working out your loss on the training day that and the model keeps on getting better and better and better and better I'm necessarily pastor improve lost on the training day that are the worst thing that could happen if the graph could become absolutely fat flare what you will find is with most models that we train they have so many promoters that this will just keep on going down until the lost his sort of approaching the numerical precision of 0 if you leave at training for long enough it just turns the correct answer that every example because because of weekend memorize examples ok but if you then say let me test out this model on some different day that what you find is this red curb that up until certain point that you're also building a model that's better at predicting on different day that but after some point this Curve starts to curl up again and all that bit wet seems to Curve down again that was a mistake in the drawer this is then referred to as overfitting on model was just learning to memorise the training data but not in the way that later generalized to other examples stop what we want we want to try and avoid overfitting as much as possible and their various regularization techniques that we use for their and has simple starring one is this one here where we penalize the log-likelihood by saying you're going to be penalised to the extent that you move parameters away from zero so the default State of Nature is all premises there so there is Norden computations you can have parameters that have big values that you'll be penalised a bit for an is referred to us L2 regularization and you know that sort of a starting point of something sensible you could do it regularization but there's more to say later and we'll talk in the sun projects about a clever regularization techniques for neural networks tired number to vectorisation the term that you but it's not only Victor's this is also Matrix isation and higher dimensional matrices what I called tinsel in this field temporisation getting deep learning systems to run fast and efficiently only possible if we vectorise thing what does that mean what that means is way to write a lot of code that you saw in your first C S Class is you say for I in range in calculate random randy when we want to be clever people the doing things we stay rather than work out this ww1 word Victor had a time and do it in a for loop we couldn't stay put all about word vectors into 1 m simply one matrix matrix multiply of w by a word vector Matrix you run your code on your laptop on the CPU you will find out if you do it the vectorise way things will become Shrewsbury faster so at this example the came over and Order of magnitude faster when doing with a vector Victor Rise rather than with a folder this games only compounded when we run code on a GPU that you will getting no games and speed of toll on a GPU when they still code is vectorise but it is vectorise then you can hope to have results of our yeah the transfer 40 times faster than it did on the CPU always try to use vectors and matrices not for Loop what city is for when developing stopped time your code and find out what slow we discussed this idea last time before that after sing the song of affinelayer where wheat the WX + B at your photos and affinelayer sewing bye mate I'm biases we necessarily to have power and a deep network have to have phone of nonlinearity and I just wanted to go through a bit of background on nonlinearities and what people use and what to use if your son of starting from the idea of what we know as logistic regression what's commonly referred to as the sigmoid or maybe more precisely for logistic and function is this picture here so something that squash as any real number positive or negative into the range of 0 to 1 if you're probably the app part this use of stick function was really really coming in early new nets if you go back to 80s 90s and unit absolutely everywhere in Maurice set the time nobody used as the Sunday been found to serve actually work quite poorly the only place the they used is when you actually want between 0 and 1 as your output so we'll talk later about how you have getting in networks and getting as a place where you want to have a probability between two things and then you use one of those that you use them absolutely nowhere else is the 10h curve for the formula for 10 it's like a scary thing with lots of exponential dinner and it doesn't really looking much like I would just stick Curve whatsoever and very few cool Math textbook you can convince yourself that 8NH Curve is actually exactly the same as they would just did Curve apart from you X 2 so it has a range of 2 rather than 1 and he shifted it down one what's the rescaled with just take those now symmetric between one and -1 and affective symmetric in the app but actually helps a lot for putting into neural network 10-hour still reasonably widely used and quite a number of places in neural networks 01080 should be a friend of yours and you should know about that what is the bad things about using functions like the sigmoid or 10 involved this expensive met operation the slow you down nuisance to be kind of Computing exponentials and 10 ages and your computer things that kind of slow so people started around with ways of make things faster and so someone came up with this idea like maybe we could come up with a hard 10 play a stuff for the flat out here and then it has a linear slope and it is flat at the top you don't sort of looks like a 10 square law and well this is really cheap the computer right you say Express and -1 wireless Wan return-path why no just return the number no complex transcendental funny singers that turned out the dyslexie works pretty well might be scared and you might justify bloody scared because if you start thinking about gradient over here there's no gradient right Wheatley flat is 0 dead as soon as they're out of one of the important to stay in this middle section at least for a while and then it's just got a a slope of 1 it's a constant slope of wine this is enough of linearity and that actually it works well and your network train your network tempelhof behold in the opposite direction and people thought I would that work maybe we can make things even simpler the now famous what's referred Wayfarers and railway sorry anything that delete off had 10 mistake which stand for rectified linear unit and so the the railway is essentially the simplest nonlinearity you can have so the relo 0 / 0 as soon as you're in the negative regime and it's just a line smoke one when you're in the positive 18 for this blew my mind that could possibly work I was brought up on these sort of 10h and sigma Lloyds and their holidays arguments about the slow can you get these what's the new move around with the gradient work if half a dysfunction just had output 0 nogrady and the other half is just a straight line show when you're on the positive regime the night density functions and you know I could before that if you just compose transforms you don't get any power but primarily and I'm part of the regime sisters and identity function that's exactly what we doing with just composing linear transformed so you just can't possibly work better turns out of this works brilliantly and this is now by far the default choice when people have building speed for deep networks swelling on realities and they have very fast very quickly and they perform very well and to effectively you know that it is simply just each you depending on the input is Dusty the dead or it's passing things on it's my identity function but that's enough of linearity that you can do arbitrary function approximation still with a deep Learning Network and people now make precisely the opposite argument which is because this dad just has a slope of 1 over its non-zero range that means the gradient is pasteque very efficiently to the input the model train very efficiently where is when you are with these kind of curves when you're over here there's very little slope so your models my train very slowly I'm so free phone network try this before you try anything else play something being a sub literature that says well maybe that's two simple and we could do a bit better and so that leads the leaky relu with said maybe we should put a tiny bit of sleep over here so it's not completely dead so you can make it something like 11 100 that's the stokers this pad and then people had while Let's Build off there maybe we could actually put another parameter into a neural network and we could have a parametric relo so there's some sleep over here but we are also going to bed propagate into our non-linearity for Cheshunt extra alpha parameter we're just how much Lopez hair seriously people of use these you can sort of find 10 papers on a kite with people so you can get better results from using one or other of these you can also find papers where people say it made no difference for them and vs just using a Rolo so I think basically you can start off with her fellow and work from there parameter initialisation when so when we have these matrices of parameters and I'll model it's vital vital vital to initialise the enum values site see the list under some people have discovered first time final project I'm so emphasise a vital so if you just start off with the white thing zero you kind of have these complete symmetry is right that everything will be calculated the same everything will move the same and you're not actually training this complex network with a lot of units that are specialising to learn different things so somehow you have to break the symmetry and we do that by giving small renway Morrison finepoint Pisces you most will just stop them 80s neutral and see have a system wherein the bias that you want etc but in general the why initialise the small random values and your finding play torch or other deep learning pictures a common initialisation that used an often recommended as Javier initialisation and so the trick of this is a lot of models in a lot of places think it's like these ones in these you like the values in the network to source the bass small in this sort of middle-range here and well if you kind of have a matrix with big values in Earth and you x play this matrix you know things might get bigger and then if you put in for another way I will get bigger again and then sort of everything will be too big and you'll have problem so really happy initialisation seeking to avoid that by saying how many inputs are there to how many outputs are there we want to son of tempered are on the initialisation based on the inputs and the outputs because effectively will be using this number that many times it's a good thing to use you can use that optimises now thought about playing SGD plain SGD actually works just fine but often if you want to use just plain STD you have to spend time tuning the learning rate that alpha that we x the I am for Complex nets and situations or avoid worry there's sorted now this big family with more sophisticated adaptive optimizers and so this scaling the premature adjustment by cumulated Grady what's the time 1 per parameter learning rate so that I can see which Predators would be useful to move more and which ones sensitivity of those parameters so where things are flak you can be trying to move quickly when things are bouncing around a lot in my training with just a little says not to overshoot and there's a whole fam with these adagrad Darkness prob Adam they're actually other ones there's Adam xx Amy Adams one fairly reliable one that many people use and that's not bad offside and I'm done yes I'm learning normally you have to choose a learning rate so one choices just have a constant learning rate and you pick a number maybe 10 to the - 3 and say that's my learning raid turning right to be order of magnitude latest to model might diverge on not converge because they're just some Loops you around by huge cramp movements and you completely miss the good parts of your function space if your money is too small your model main assignment deadline and then you'll be unhappy so you said you know commonly people thought of try powers of ten and sees how looks right they might try you know I'm point 01.01.20 one and see look at how the Lost is declining and see what seems to work and general you want to use the fastest learning raid that isn't making things become unstable can you get better result decreasing the learning rate as you train so sometimes people just do that by hand so we use the term epoch for a full pass through your training day that and people might say have the learning rate after every three parks as you train and that can work pretty well you can use formulas to get per learning rate fancy a methods you can look out cyclic learning you want which sort of actually makes the learning light sometimes bigger and then sometimes smaller and people are found that can be useful for getting out of bed regions and interesting ways thing to know is if you're using one of the fancy optimises they still ask you from learning raid but that learning rate is the initial learning rate which typically the optimizer will shrink as you train so commonly if you're using something like Adam you might be starting off by saying the learning razors 0.1 so of a bigger number and it will be shrinking at later as the training goes along Boden see you next week 