I'm happy if you want her last week I'm behaved this class and this is the second I'm going to be giving on rnn and related topics so welcome to week 4 learning about ingredients and some more complex types of orange hands started I've got a few announcements announcement assignment 4 is released today is Thursday next week not Tuesday 2-days more to do it than you did for the other homework what is Simon Forrest probably motorbikes in the other hand work so fast so don't be surprised by that Forest all about neural machine translation turn Mt on Thursdays lecture this week this is really exciting because actually cs224n does never had an end to assignment 4c this is only this year and you're going to be the first year students are going to be doing and then until Simon what's the difference about a timer in for is the event for using azure which is a cloud computing service you train your energy systems on a virtual machine with the GP Huddersfield to do it in a reasonable amount of time morning which is if you're person who perhaps doesn't have a lunch a lot of experience working on remote machine this late show TMAX or remote text editing that I've added you to budget and extra time for Simon for because there's probably going to take you a little while to set up and get used to I'm going to emphasise do you get started early on assignment for because the energy system takes about 4 hours to train on your virtual machine so you really can't start the night before and expect to get it on time the song for is really quite a lot more complicated than the same at 3 so don't get into a false sense of security if you found assignment play Thursday slides on an Empty already on the website today so you can leave until I hear those 31 what's the timer for announcements on the subject of projects next week's lectures are going to be all about projects you can hear about answering and the people can't afford play some tips about how you might choose and define your own custom Project anything not think about project this week that's ok you can stay until next week start thinking about it for the first time use already thinking about your projects onto the 4:20 check out the websites project page because has caused lots of information about a house tutor Project food Smith collected some project ideas from various members of the Stamford so these are 50 Cent's in postdoc 40K t planning projects that they would like cs224n students such as the software looking to maybe get into research later this is a really great opportunity turn the lab and maybe get some mentorship as well review last currently on at work really grateful I was not lying about some problem to Darkness and how to fix them motivated to learn about some more complex are and aryans next Thursday we're going to have some more application based content so we're going to learn about neural machine translation which is a really important task in NLP deep learning this architectural sequence-to-sequence with attention concerned about the vanishing gradient problem it's about two new types of Orange primary and gated community that's another kind of miscellaneous fixes for the vanishing gradient problem or the Exploding gradient problem clipping which is 30 simple quite important turnabout skip connection telling you you're locked extra which tries to fix the phantom green problem electrical about some more than try and variants such as body Ralph Lauren Orange just left to right but also right turnabout that's when you stuck for an answer today so you going to find out the information in this selection is pretty important for Simon for and probably feel project as well let's get started thinking about the vanishing Grange we have an orange forceps has some kind of lost that's aj4 and I'll computers based on the 476 you interested in asking of this last day for with respect to the hidden state the first setting up with this blue Arrow annotations coronavirus and how we have to make the gradient flow backwards in order to compete this screen is we can apply the general and say well it's the product of the gradients of the last with respect to 82 is base to Riverside H1 we can take that again using the general decompose the great in that we were interested in into the products of these various intermediate gradients seeing all these 80 by 80 - 1 adjacent gradients of the handsets ask you is what happens if these gradients are small the lot of them best mall in Bangor problem gradient problem these gradients are small the gradient is going to get smaller and smaller as it back propagates further if you lose gradients is the product of all of these intermediate gradients and when you multiply something by something small than half and get some water presenting here with use a smaller and smaller my dear Blanche ingredient more formal the last time play na na na aht is computed as a function of the previous inside 80 - 1 and the current input x indeed previous likeshuo we said that xt-1 hot factors representing words and then et is the embedding this letter going to be I'm getting rid of that detail and we just give me thinking very abstractly about an hour and then that has some kind input XT next Tuesday as any kind of bags are probably done facts about you could be words or not what's the the definition that we like last time for the Larne is the derivative of HT in state on September with respect to the previous Wednesday is just an application for train roll and a few multiplying by wh at the end because you have the modification of wht minus one on the inside remember on the penis what's the gradient of the loss on some steps use 8HJ on some previous that Jay and maybe Jay is quite a few steps before I write this despite playing the chain rule on the first timer saying that this to observe the interested in can be decomposed into the derivative with respect which is kind of last step and then do all of those intermediate gradients of the adjacent insects as well the first line is just exciting same thing as we were looking at on the picture can you give me figure it out what is dht by dht - 140 drop the side then we can just have to sit on it everything is overrated interested in has this time Matrix x self I managed a x because I'm on his chain many step J and Step by which is I'm here is if This Way matrix why is small time is going to get vanishingly small exponentially small as inj get further apart detail think about the L2 matrix of all of these matrices raped facts of the day you have this in a policy that the norm of the products of celebrities is less than or equal to the product of the nonsense never seen that the norm of the screen that interested in is less than or equal to the products - Jay many times of the norm of the whey Matrix I mean when we say we can sound about wh being small is a bit smaller than the thing on the left play in this paper that you can take the bottom if your with all showed that if the largest eigenvalue of The Way m w then this gradient on the left intuitively while listen find something with suppose that why is not a matrix but simply a scalar it was just a single number why if that number was Grace and ones and how's things going to expose and if another is less than 1 then it's going to drink exponentially as he must buy the same number again you can check out the paper for more details but here the bound is one party because it has the sigmoid London this why if the wmatrix is small or his largest I can buy a small then we're going to have fun a you can see that there's a similar if the largest eigenvalue is Grace and 12 having exploding Green is get bigger and bigger as he back from Father Minster that ranch ingredients is a phenomenon that happens in Ireland yes my for getting larger and larger or smaller and smaller what is a picture that why is it bad Agatha supposed to think about what's the job of the last on the first step with respect to the first situation where the gradient is getting smaller and smaller think about what is the gradient of let's say the loss on the second Step also reflect the first in the States that with the orange arrows pointers here the magnitude of the gradient signal from close by is a lot bigger than the magnitude of the gradient is that when you update your model weights you getting from close by is going to be so much bigger than the signal from far away the century you're only going to learn your only going to optimise with respect to these nearby effects and not the long time you going to lose the long-term the nearby stop alarm the operation was given the example you might be something over multiple classes as a lot and every step and you some all of them and then you do want to update more with respect to the nearby losses than the fall off yes design of your objective function is the distance under lots of every step that you do want to wait all of the equally I think my point was more about fluence of the action of the weight matrix at this early stage one is it implements on a loss that's nearby and what is its influence on a loss that far away and due to the Dynamics of how the vanishing then the influence on the last as far away is going to be much better than putting it example letter of why you might want to know Mis in situations when you do want to then the connection between something that happens early and something that happens later then you're going to be unable to them honest why are you talking about like eh eh volume that's a great question so you're asking why are we interested in some kind of DJ by th not updating age agent activation all the way the reason why we thinking about that is because when you think about DJ by dw2 update that's always going to be in terms of DJ by dhw you know and how it acts on the transformation from 10th to then dj4 by W an optician is going to have to go through DJ for by dht vanishing gradients has me back further than it's kind of like a bottle now then you're certainly going to have phantom pregnancies current Matrix and indeed the Matrix applied to the turn off now another way to explain why vanishing gradient is the problem is you can think of it as a gradients you can think of it as a measure of the effect of the past on this little is like saying if I change this way towards activation and little bit then how much and how does it affect the singing the future if I grate into becoming vanishingly small over longer distances 72 szepty percent can't tell whether in one of two situation is maybe there's no dependency between step tea and step 2 we're learning on a task wherein the top there truly is no connection or relationship to be learnt between what happens I'll septien what happens on September nothing to be learnt and is actually correct that there should be no small gradients with respect yes there is a true connection between his two things in the data and then the task and really ideally we should be learning that connect have the wrong parameters in our model that is why the gradient so small because the model doesn't even connect what's 9 in in between the vanishing gradient problem is that we are unable to tell in this situation which of these t-shirts so pretty Rascal I think this is not pushing making a little bit more clear why the wrench in granny Prague last week we learnt about coronavirus language modelling is a task we have some kind of text and then you're trying to text it says when she tried to print tickets she found at the printer without a toner Three store to buy more the princess she finally printed her what way do you think computer do because it makes sense logically but it don't think she's trying to do that thing to do when she's gone the whole piece or for the for the toner questionnaires can Arden language models with this particular language modelling exam Bananarama Cruel to be kind example then they need to learn from this kind of example in the training data what is on from the training data play some of the new model the dependency on the connection between appearance of the word tickets earlier on the 7th hardware tickets at the end we have the van the last emperor to speak to me earlier play long distance Paul is going to be unable to learn this dependency during training in the model is going to be unable to predict a similar kind of long-distance dependencies example the text says listen to full sentence the writer of the books black you two options if you are the writer of the books is Irish of the books are again shout out which one it is the correct answer correct possible continuation for sentence with the rate for the books is planning at continuation that goes the right from the books far that would be great bring up the six there is a kind of tension between two things called syntactical recency and some s is the idea that in order to correctly predict the next word should be more is than our word writer is the kind of syntactically close word here so we say the right of the book is because it's on the right it is as the word writer and is our syntaxity close because if you looked at the dependency parser example then there would be a short par recency similar concepts of how close words are just in the sentence as a sequence of words example books and are a very recent because they're right next to each other bring this up is because the second one of the Inca a tempting offer mostly only pay attention to things that happened recently then you might get distracted anything out of the books is there are a number of models from sequential procedure to use the vanishing gradient perhaps if you're in tightly related word is actually kind of far away information from the syntactically recent word especially if there's a lot of strong signal from the second where's the paper to show the rnn language models make this kind of our I'm saying or rather than more often than you if you have multiple of these to certain words such as the registrant projects and the true word that 30 mentions that explode ingredients is a justify why is it fitting into problem and why does it what does it look exploding gradients or problem is if you remember this is how test play the new prams of the model which we have sent by theta the Old promises and then you take some step in the direction of negative is my salah TJ if your gradient gets really big send your Sed update is going to become really a very big step when you go to be drastically changing your model crime to speak her you can end up with some bad update 2 largest changing the parameters to match I'll be kind of take a big step and we end up in some area where the cramps are actually very bad example of them have much larger last night worst case manifest as seeing infinities or nan's not a number at work when you're training in practice is can happen because if you take such a big step that maybe you update your parameters so much that now the infinity or minus infinity something like that then you can have all of his infinity is within your activation as well and then all of your losses are going to infinity and the whole thing just isn't going to work at all turn on this happens unfortunately and if it does then you have to essentially restart training from somebody on your checkpoint before you got the nan's and infinity it's because there's no kind of salvaging it from his music solution to this exploding gradient problem ocean is actually pretty simple and is this technique called gradient clipping gradient clipping the norm of your gradient is greater than some threshold and the threshold is ibuprofen to the YouTube to scale down that great for your party are you still going to take a selfie and send erection you going to make sure that this morning mysky a screenshot of some pseudocode from the relation paper proposed it's pretty simple as you can see is the vector which is the derivative of Ciara with respect and it's saying that if the norm of this gradient is written thresholds than you just scale is it still pointing in the same direction it's just a smaller step picture to show how that my work out this is a diagram from the Deep learning textbook which is also linked on the website turn on here the picture here is the loss surface of a simple or racing Killarney having a sequence of vectors that they didn't stated simply just two Singles instead of having a weight matrix w in a by Spectre by have a scalar w in a scalar you just have this like two-dimensional parameter space and then the z-axis is your hi loss is bad in the losses good here in this picture you've got this kind of clip 3 have its very steep a weather lost changes very quickly this is really dangerous because it has CC volume down two of taking a really big updates because you're on the area with the really stupid left you play what might happen if you don't have left so you can see that you start kind of at the bottom of the cliff fuse small update makes a bad updates because you see there's a small kind of dip before it goes up the cliff side of the true local minimum the optimum they're trying to get tumours of the bottom of that small kind of day it's sort of kind of near the edges that ditch and then there's a an exocrine and going into the update overshoots and end up going a long way off agitation was taking a bad update and now it's got a much bigger Lost At The Fall on the images the gradient and the gradient is my a update with a gradient then because it takes a really cute the the wants the right you can see the step going to the right very bad update because it's just throwing it really far to some probably round awm you can see what can go wrong if you're taking his really big steps because you were in arrears with very call Joss on the right you can see what might happen if you do have a great day much less drastic right you've got a similar pattern where it takes two sets into the turn up the little bit but not too much because the great it was listen again a really see brilliant but it doesn't take such a big step because again the great it was quite some kind of comes back down closeby by using this grating clip you got a kind of safer update rule where you're not going to take any any big crazy steps anymore like he's kind of find the the true minimum which is at the bottom of the Dead the question are you 3 in barrel the Adam optimisation algorithm which has the single momentum which eventually says that kind of like physical and mental in the real world then if you've been travelling in the same direct and if you change directions and you spell the elements as well similar kind of idea I suppose to different criterion right so what they both have in common is it a kind of criterion for when to scale up or scale down the size of your updates play some Different notions of when should you take bigger steps and when she text myself when should be cautious or last the Criterion shut off bedroom to 4 which regulations is this similar to regularization of some kind so suppose yeah that is there something in common so for example L2 regularization says that you want for example your way matrices 2 L2 Norm and the idea is that you're trying to prevent two model from overfitting the data by I'm having some kind of constraint that says you have to keep your weights fairly simple who's the relationship is that here with saying we go on the normal gradients to be too big forfeit I guess it's a similar kind of I'm going to be off now so how you might fix the Exploding gradient problem with grading how to fix the vanishing gradient problem to recap I think 1 ways to characterize the problem with the vanishing gradients in Ireland too difficult for the orange pen to learn preserve information over many sample with printing the tickets and remembering the tickets if she wants to about as it's hard for the Orion and model to correctly predict tickets because in the words to her doll to learn to retain the tickets information and usually if you look at the equation formula and understand how we compute the end Amazon Satan and Ian didn't say it is no way constantly being rude computed based on these linear transformations in the you know the Normandy art all that easy to preserve the information from one in six opening play Smooth asked what about an hour and with some kind of separate I'm kind of separate place to store information that we wanted to use later this make it easier for our land to learn to preserve what's the most basic idea behind LSE answers for long short term memory is there calcium is a type of our Nan and I was surprised back in 1997 explicit solution to the vanishing gradient problem 10 differences here is that I needed to you instead of just having a hidden state HD we have both the hidden zht and the cell States which we denote CT these are vectors of some same length is that the sellers meant to store a long time information that starts on memory unit the thing is that the lcm can erase and writes and read information from the sun think of this a bit like memory in a computer in the you can do these operations Reading right can you keep your 14 thing the way the lstm decides whether it wants to erase rights read information and decide how much which information that's controlled by these Gates is the biggest of themselves also vectors of length m don't need time stop which are vectors between 0 and 1 one representing open represents values anywhere good idea where to farm on the next size but the over idea is that if the gate is open that represents some kind of information being passed through the gate to close that means that information does not that's really important thing is that the gates of dynamic they're not just set at some point the whole sequence which means that they're different on each time step 2 decision of whether open or closed and in which way treated based on the current is his the the equation fallacy and which might make it clearer input text sequence of HT and sulphates is what happens on time septic Bicester equation shows you the three gates talks about before what is corned beef this one is controlling what is cat vs what is forgotten previous cell state the previous mail this forget date is computed based on the previous and state HT - 1 and the current input x so that's what I meant when I said that it's computer you can also see that is computed using the sigmoid function which means that somewhere between 0 and 1 state is called the input controls what parts of the new s this memory cell in the second of controlling write to them what song is called the Opera controlling what parts of the cell are outputs to the hidden this is kind of like the read need some information from a memory cell and that's going to get put into a tin function as we know it Nexus for questions shows how use these dates this seat older as the new cell what is the new console you want to write to the soul computed based on your previous installation your current in this is the main content that you are computing based on the context and you want to write this internet explain what's happening into use the forget gate to selectively forget some of the information from the previous me the weather doing this element-wise products that's what the little circle is if you remember that ft is a vector full of values between 0 and 1 between FTE and the previous salts 8ct - 1 then what you're essentially doing is a kind of masking itv7 yourself f is 1 then you're cutting over the information whenever the other half of this equation i t x c till the templegate controlling which parts of the new cell contents spoken to get written written to the to the soul the last thing we do is to be a past the cell through a tennis that's just adding another none in the RC and then you pass that to the Albert Elysee and we often think of the head state is being like the outputs of the are this is that you kind of you these soul States as being as kind of internal memory that's not generally accessible to the outside slaves are the parts you gonna pass onto the next person go to kind of output and this is yeah just reminded of those circles our products and that's how we apply the any questions all of these are factors of something turn better from diagrams than equations and here's a diagram presentation of the same idea so this is a very nice diagram from a blog post by Chris Toler about aliens and that's a good place to start if you want to get an intuitive understanding of what diagram of the green boxes represent time set and that seemed in on the middle and see what's happening here 1 x 2 this diagram is showing exactly the same thing as 6 equations showed on the previous slide the first thing we do is we use the can you put exterior to the bottom and the previous insane 80 - 1 computer forget reply the forget gates to the previous cell singers forgetting some of the the South on 10th of March off that you can compute the input case and that's computing in the same way as the forget games use the input gates to decide which parts of this new s to the Selsey see that you can do the gates and new contents and then you use that to get back can you sell CT who is computer new output gate that OT lastly use the output gate to select which part of the cell contents you gonna read and put in the new insane that's the same thing calcium the question is why are we applying a tonnage on the very last equation get to the cell before applying the Alp your question is spell content already went through tanning so I suppose a general answer is it must be giving some kind of more expressivity some way and that is not find teenagers because you do have the gates in between there must be a reason kind of Simulator when you play apply a linearly once have an injection for the next in the later I suppose maybe with you and use cases are kind of the new layer that's a Sims recall we were pressure previous question is why is the computed only from Cleveland state and the current input why is it not computed based on CT - 1 itself right cos when you want to look at the thing to figure out whether you want to forget it or not that's a pretty good question you might think that this this works fine is that by Alicia might be learning a general Aldo Star Wars different types of information in sliding that in this particular position in the sole I'm lying information about this particular semantic thing and then in this situation I want to I'm not used to get it ok convince myself why you don't want to look at the contents of the cell itself in order to size sing is there one was red from CT - once I suppose there is some information there but not necessarily all of the information I'm not sure that's nothing play the Sims and introduced to try to solve vanishing gradient problem question is how is actually is this architecture making the band in your problem when you're better see that the lstm Architecture actually makes it easier for enhanced reserve information over many timesteps well it was kind of difficult for the middle or an end to preserve the information of all the states there's actually fairly easy strategy that makes it simple for the lstm to preserve the information if the forget gate is set to remember everything on everything play somebody that will ensure that the information in the cell is going to be preserved in definitely over many times actually good strategy to do but my point is that there is at least Broadway for the lstm to keep the information over many as We Know said that's relatively harder for the Minotaur this is the key reason why lstm are more able to preserve the information and that's are more robust to the vanishing gradient still no jealousy and don't necessarily guarantee that we don't have a vanishing exploding gradient problem you could say that where is it easier to avoid Alicia have been shown to be more robust to the vanishing gradient turn off all about how they've actually be more successful why is it because you have these lcm to find for equations why do you not have the mansion going problem why does the the logic about the chain rule kind of getting smaller and smaller or bigger and bigger here is that Aaron in the hidden state the kind of like a bottleneck write like gradients must pass through them regard the cell as being kind of like a shortcut connection at least in the case where the forget gate is set to remember things then that sounded like a short will stay the same if you have the forget Gates so is staying mostly the same 10 having the vanishing gradient to get a connection from the gradient of something in the future with respect something in the past there is a potential route for the gradient to go via the cell that doesn't multiple information to travel I suppose this someone relates to what we talked about last time with the multivariable chain rule about what is the derivative of the last for this picture of repeated by Matrix and we saw that if there are multiple routes than the multiple channel says that you add up the greatest how do you do the characters correctly make sure it's correct I guess you just what channel is more complication the settings yourself if you don't impress yourself then you might have more difficult time so inhaler working in a room 2013 to 2015 lstm started this evening of state-of-the-art results on of raichu different tasks including for example handwriting recognition speech recognition machine translation parsing image captioning over this period as TMS became the dominant approach in a lot of these application areas because they were what better than an orange today in 2019 things change believe Austin other purchase for example Transformers which you can see that about later in the course in some of these optician areas they seem to have become the dominant I have a look at wmt which is machine translation conference and also competition when people submit their empty there seems to be evaluated the report the summary report for mwmt 2016 I found the word foreign in a page 44 where is people and trainers competition were building their Mt systems based on our hands and in particular support from 2018 just two years later and I found out the oven in 9 times and the word transformer appeared 60 noted then everyone what most people think I'm using Transformers now this is the things change pretty fast into planning the thing that was missing you just a few years ago being passed by perhaps by other country purchase Transformers later but I guess I give you a kind of idea of where LSD and are currently in applications the animal and about is gated recurrent units fortunately or simpler than lcms in fact that was motivation for them being in 2014 as a way to try to retain the strength of our CMS but getting rid of any unnecessary complex is your you we don't have a cell States me again just have a hidden state in common with honesty and December going to be using gates to control the flow of information questions for a ju start off with Two Gates so the first gate is called the update pause what parts of the hidden states are going to be updated vs preserve you this as a playing the role of both the forget gate and the impact due to the match the same way as the gates in the Halcyon where call the reset Gates RT controlling which parts of the previous in states are going to be used to compute new content the reset gate is kind of selecting which parts of the BBC states are useful verses garden things and selecting other things is how those Gates get used tell the hey this is you can think about is the new hidden s what's going on in the equation is that we're playing the reset gate to the previous instate HD - 1 goes out through some transformations and a tan gives us the new content which we want to write didn't sell onion cell is going to be a combination this new contents and the previous sentence please notice here is that we have this one - you and you too it's kind of like a balance right you in the balance between preserving things from the previous hidden state vs writing used tm42 completely separate Gates that could be whatever value he we haven't constraint that you was being a balance of you have more of one you have less of the other on way in which medications of the Gru sort to make lstm more simple ways by having a simple paper adds to your dues and I think it's a little less obvious just looking at it why do your use help the vanishing gradient problem because there is no explicit memory so like there is do you use you can do this is also being a solution to the vanishing gradient problem play Galaxy mgo used make it easier to retain information languedoc example if the updates UT is set to 0 then we're going to be sing the hidden stayed the same on every step and again that's maybe not a good idea but at least that is a structure you can easily do in order to retain information like the same explanation of how do I use make a potentially easier for our nan's to retain information on time 22 different types of Ireland yes to the question was should we view these two Gates in the Gru as being a precise analogy to the gates in the lcm or are they more of a fuzzy and more the fuzzy analogy because changes going on in here like for example the fact that there's no set resole means of Performing play cm and joy use which are both more complicated forms of Aaron's more complicated than and they are both what's your problem you should be using in practice which one is more successful the lstm Odeon reading it looks like researchers have proposed a lot of different types of gated Ion and so it's not just your used as many other papers with lots of other different barriers definitely the truth and most widely used the biggest difference between the two for sure is the fact that you use our simple and quick as compare so this makes natural practical difference to you as a deep learning practitioner because if you build your net based on to your news that is going to be faster to run forward faster to train therapists to be no very conclusive evidence that one of these lstm with your consistently outperforming the other on lots of different tasks seems that often sometimes they are used to perform as well as the cases where one of them from seems like lcms option a good default choice to start if your data has particularly longer and this is because of evidence the thing so you're better at keeping information of a very long distances if you have a lot of training data you might think Elysee and a better choice because they have more maybe you need more training is the movie want to start with our CMS and if you're happy with our performance and happy with how long it takes the train then you stick with that's but if you feel like I need to be more efficient than maybe you should switch to your news and see how that goes with the performance had if it's awesome we talked about how the vanishing exploding gradients are problem that occur along in our nan's but the question is is it only problem is this a car another kind of your network as well say is no it's not just our no problem in fact of vanishing exploding gradients are pretty significant problem for parallax section 34 in convolutional especially when serious problem because there's no point having a really cool and you'll ask Alexa if you can't learn it efficiently because of the vanishing gradient in these people the competition on networks you often have a great day becoming vanishingly small over backpropagation because of the chain rule books of this multiplying by all these different intermediate gradients or sometimes due to your choice of nonlinearity Funk this means that you're the lower layers of your that's a convolutional or FIFA 19 add smaller gradients than the higher levels and this means that they get change very slowly during STD overall your network is very slow train because when you take updates in your lower layers are changing very slowly solution can you buy a family of solutions to be seen in recent years lots of proposals for new types of snow architecture who is the Admiral Direct connections in the network and the idea kind of as we talk about before is it if you add all of these Direct connections between layers like maybe not just adjacent layers much easier for the gradients to flow and you go to find it easier to train your network overall examples of these particular because it's very like you're going to run into his kind of architectures when you're doing your products and reading papers sample is something called residual connections or the network itself is under is referred to resnet got a figure from the related paper going on in this diagram is that you have the usual kind of you got white layer and a non in the Arches venue and another way they're so if you regard that function is being FFX is the set of just transformed ex2f effects that they're taking fxplus identity skip connection where the input x is skipped over those two layers and then to the output of the two lights this is a good idea open a skip connection connection is going to preserve information by the tabs if you your network and your initialise a white lez to have small random values then if they're small and kind of clothes 20 then you're going to have something like a noisy identity function c going to be preserving information by default through all of your letters and if you have a very deep network that means that even after many many loads of still going to have something like your original this paper they show that if you don't have something I stick connections that actually you can find the Deep layers deep networks perform worse on some tasks than shallow networks not because they're not expensive enough but because they're too difficult that so when you attempt to London at work so just doesn't learn effectively Newent getting worse performance of the Challenge this paper they show them when they are disconnections then they made the Deep networks much more effectively manage to go performance so another example which hotel this idea further is something called dance connections or dancing something proposed I thinking setting and it's just kind of connections to accept connect everything everything so add more of his skip connections kind of from all latest all Steven Bassett last one was talked about which I do have a picture for it's something called highway connections similar to the residual or skip connections instead of just adding your ex adding your identity connection serious you're going to have a gate that controls the balance between am adding the identity and Computing the transformation text sexy going to help you know gate x fxplus you know something like that so this was actually inspired by lstm but instead of applying it to a recurrence guessing they were seeking to apply it to a C4 Picasso you go for now the question was you know how much vanishing exploding gradient a problem outside of the setting Varna open takeaways that is a big problem but you should notice that it is particularly a problem finance the particularly unstable and this is essentially due to the repeated multiplication by the same way last time the characteristics of the Arnos that makes them recurrent is the fact that you are applying the same weight matrix over and over again the reason why they are so prone to the vanishing exploding you can see some more information about that in the paper so I know there's been a lot of dense information stay a lot of so here's a recap if I've lost any point now good time turn back in because it's going to get a little easier to understand parts so ok we can talk about today the first thing you learnt about with the vanishing gradient problem we learn what it is what happens and we saw why it's bad for our Nan's for example and language vitality and enjoy use which are more computer on ends and they used to control the flow of information and by doing that they are more than intervention grandium Prague I think about 20 learning about two more advanced out of our nan's so the first one is bi-directional and that's all about information showing going to learn about multilayer and ends which is when you apply multiple and answer popular I say they're both of these are pretty simple conceptual so shouldn't be too hard trance start with bidirectional rnn this is a picture which you saw the end of last lecture so if you remember sentiment classification is the task when you have some kind of simple sentence such as the movie was terrible this is positive vampires should be seen as positive this is an example of how you might try to solve some classification using a fairly simple and Disney Orlando is a kind of encoder of the sentence and the hidden States represent the sentence and we'll do some kind combination of inside the computer what we think the sentence if we look at let's see the hidden stay the course once the word therapy are we regarding this hidden state as a representation of the word terribly in the context of the sentence call hidden States in this kind of situation they can text or represent a sensation in the context of the sentence back here is that this context or representation information about the left left contact stays the words the movie was state the one that's got the blue box around 16 information for that it hasn't seen the information of the words exciting of exclamation mark what about the right is the word exciting an ex I think that the right context is do you think this is something we want to know about music in this example it is actually coming from Port you got the phrase Tara Bjork how do I terribly in isolation terrible terrible you need something bad right but terribly exciting you can be something good is it just needs very if you know about the right context the words exciting than this might quite modify your perception of the meaning of the word terribly in the context of this sentiment classification this is kind of important is why you might want to have information from both the left and the right when you're making your website when your kids your parents told you to look both ways before you cross the street you might regard it as the same kind of idea that down the right you'd like to know about anything what's the motivation is how a bi-directional on my work in practice I have accidentally festive colour scheme so the idea is that you have to our nan's going on you have the for Darren and as before that includes the sentence left to right you also have a backwards Ireland and this has completely separate weights to the floor darling the back of an is just doing the same thing except that it's encoding the sequence from right to left so YouTube The Hidden state is computed based on the one to the right you just take the insights from the two ions and then you can have them together and you've got your representations particular Queen I think about representation of the word terribly in the contacts this this vector has information from both the left and the right right because he had the forwards and backwards foreign and respect of the information from both left and right these unconcatenate hidden States those can be regarded as kind of like the outputs of the bidirectional easy sentences for any kind of further competition then please can you going to be passing on TV next part of the network here are the equations that just say the same thing so you have your phone on in and here we've got a notation that you might not have seen before play some roses are red and in brackets the TV 7 st in the input that simply saying that you know HT is computed from the previous owner States and The Importance and Aaron and forward could be or else it doesn't really matter with abstract are and forwards and backwards and generally operates although I have seen some papers when I have shared so seems that sometimes that we have enough training data and finally I will regards these concatenating States which we might just noticed HT as being like vegan states of the bidirectional Orange the previous album is pretty unwieldy so here's a simplified diagram and this is probably the only kind of diagrams we going to see from now on Sudan bi-directional and always on here is just made all of the hall and arrows go left this is a boiler ignoring them 2 cm is that the hidden States to be to tell you these red rectangle with you gonna see with those of the concatenated forward backward sitting States from the bidirectional rnn so the question is would you train your forwards and backwards on any kind of separately on some kind of task and then maybe concatenate them together 1 per 73 networks or would you train them all together there's much more common to training together anyone training in separately so yeah it seems like the standard practice is usually to train him to go play we were trying to build a central application system using the bidirectional rnn then what you do wish we go to the pictures of space is you would do the same thing that you were doing with the unidirectional and then which was not saying why is min or Max to get some coding maybe you just do that but over though concatenate an important thing to note is that when talking about applying butter exploring and we've assumed that we actually have access to the entire input sequence so it seems we have the full sentence the movie was very exciting and in order to be able to run the forwards and backwards or that there are some situations where you can't pull in language modelling you only have access to the left contacts kind of by definition of the task you only know the words that comes before you don't know what's coming can't use a bi-directional because you don't have do you have access to the entire sequence soap example during any kind of encoding similar to this sentiment example directions to by directionality is pretty powerful and you should probably regarded as a good thing to do by 2 beginning this information from both the left and the right makes a lot easier to learn architectural representation as a previous something going to learn about later in the car vrt and encoder representations from trans pretty recently like a few months ago proposed system and it's this pretrained contextual representation system it's heavily reliant on the idea of bi-directional TV turns out that the bi-directional nature of birds is pretty important to it's success more about that later but that's just an example of how One Direction and she can give you much more powerful can text or representations nothing is going to talk about say is multilayer or an answer you could regard or an end it's already being that's because you already unroll them over potentially very many time steps and you could go that is a kind of death another way to coroner's could be t sample if you are applied multiple r and ends kind of one after another then this would be a different way to make your own n d what's the idea between behind and multilayer rnn why you want to do this is because this might allow the network to compute more complex represent a cheapest white behind deep networks in general so if me with your idea of way Deepa is better for a letter convolutional networks then this is kind of the same are you Lauren and might be confusing a lower level features like that suppose maybe syntax and your higher level and going to complete high-level features like maybe semantics I know time sometimes called sax harness quotes Manchester Eden majan so here's an example of how a multilayer rnn my work so this is a unidirectional rnn but it could be bi-directional Alexa saying tighten sequence the main thing is that the states from one iron and Leia are going to be used as the input rnn layer it's coming time to think about what order when you compute all of the 7 States there's some flexibility right but you could compute all of the stamp 110 call of the V1 and then all the movie ones or you could do all of our and their one and then all of our Nan I think that when you no call the pytorch function to do with multi there and then it will be all of our and then one then that's what I think happens there's no reason why you couldn't do it so if they were but you would have to do all of them so what's the earnings in practice terms of company well in that when I look at rnn basis instead are doing very well on some kind of tasks they usually are some kind of multilayer alright assassin Leon to seem as the Deep convolutional people that works in majesty Ninfield sample images hundreds of years now and you certainly are getting on instead of that Who from Google they doing the kind of large hyperparameter search for in your machine translation to find which coins hyperparameters work well they found it to the four layers is best to the encoder rnn and four layers is best for the morning you'll find out more about what and coding decoding mean next find a different Andy skip connection to my son's connections and then it makes it much easier to learn some even deeper and more effectively like maybe up to eight 100 reasons why are an instance be nearly asleep as there's other networks is that because as we coming to before and after be computed sequentially they can't be computed in parallel this means that predicts Spencer steps in like two dimensions you have the depth of the time steps in the depth over the iron and layers to then it becomes very very expensive to compute good morning just mention Transformers are you going on about Transformers later but these it seems can be deeper people who says transform based networks can be pretty deep so bird for example there's a 24-hour version and a 12 version and that was my Google in they have a lot of conversation power but I think part of the reason why the transformer based networks can be quite deep is that they have a lot of these skipping like connections and the whole innervation of Transformers is it then built on a lot of connections alright so here's the summary of what we've launched a I know it's been a lot of information but I think here for practical takeaways from today that probably useful to you in your projects even if you very interesting in themselves that probably be useful so the first one is the lstm a very powerful they certainly I'm not powerful then more powerful than alarm also more powerful than that and the only difference that is consistently the same as the go used are faster than other teams is the YouTube clip your gradients because if you don't take your gradients you're in danger of walking ending up with nan's in your model is that One Direction LT is useful if you can apply anytime when you have access to the entire input sequence you can apply by directionality so you should probably do that by default is that multilayer Ireland's are pretty powerful and again you should probably do that if you have enough conversation do so how to make your mods there are an empty deep then you might need to get connections thanks 