started play slick chart what we going to do is look at the topic of having three recursive neural network a topic which I feel especially fond of in a test to because actually when we started doing deep learning 30p at Stanford in 2010 and released for the period from 2010 to two thousand the dominant side of ideas that we were working on with this topic of how you could build have a recursive tree structure into neural network kind of funny that I'm only getting through it now I mean there's a lot of reasons for that but I think they're a bunch of interesting ideas here which will a closely to linguistic structure and such good stuff to have seen but in practice these ideas are proven kind of hard to scale and not necessarily to work better in practice than the kind of things that we've spent more time on needing things like located lstm and looking at Transformers and things like that and so that's kind of Iris of them towards the end of the curriculum but I want to say something about the motivations and the way you can build tree structures and your own networks and look at some of the possibilities and we explored in during this class another fact about this classes actually this is the last class I'm going to give an 30 more classed as next week don't forget about next c24 in classes on Tuesday we've got in the final invited speaker make Mitchell has a great speaker and has tons of interesting stuff to say about fairness and if and then for the final lectures one I'm another my PhD student Kevin Clark is going to give bad and talk about some of the recent what's been happening in deep learning 18/19 of some sort of recent developments in NLP and deep learning I'm so stairwells at the end of this one answer hopefully everyone has submitted stone for their final project if you haven't you should really begin your milestones in you know it's inevitable that's somewhere around here they start to be people have nothing works and everything is too slow and you panic happens I wish you luck of course I mean what can you do about it I mean it'll be really hard when you have things that don't work as the work out why they don't work and how to fix them I mean I think often to this thing to do is really to go back to something simple that you can get working and to work forward from there again so really help small datasets I'm really recommend the strategy of sort of having a 10-item or 20-item day the third and checking that your model works perfectly over trains to 100% accuracy on that kind of day they said they've to huge amount turn the top after you've gone something simple working on a small amount of data that's the right time to sort of them forward again you should definitely always make sure that you can completely overfit on your training dataset that so and not quite a proof but at least the Enfield model being implemented properly being a successful deep learning researcher is actually managing to get things done and not wasting a tonne of time and so it definitely always helps just to be you know plotting on your training and their parents so that you can still tell if things are working or things aren't working and you should abandon and start again with the new experiment to put just things like that say few hours and get you more what's interworking this song a whole bunch of things to make it work better there's regularization with the altar and drop out this time to do a premature search open doing these things and difference to watch your final results are inside going to have time to do those things because you want to get things I'm working first before you go onto there and sort of really encourage people do it still stop buying office hours if you've got any problems and will try our best to help them here within the limitations of what we can do from just think it's called with problems I wanted to say I've just say some general remarks and language and theories of language the context that motivate this tree recursive network this is an art installation at Carnegie Mellon University and as an NLP person I really love this art installation so we need better at installations around the Stanford School of Engineering so this is the bag-of-words art installations there's the bag with a lot of words in it and he down here they were the stop words that in the who won out of the bag into the ground as the stop words beautiful artwork b o one of the interesting what's been found about nop and I think this is even more showing the Deep learning well that used to be previously is what with bag of words model tries you often get a lot of power and by saying well let's get out new word sisters we have done it every thermal mix pull them or something like this and do nothing more than that just me a pretty good sentence representation or document representation that I could use the classifier something and sometimes you can do not much more than that and get even better so people done things like these everything networks where you're taking the output of a bag of words model sort of feeding through a couple more layers and improving things medicine complete distinction to what's been dominant in linguistics of looking at language structure that typically in linguistics the emphasis has been on identify and kind of huge amounts of linguistics after answers through very complex formalisms I guess this is sort of a bit of a picture of a pumpkin minimalism synthetic tree and the one at the top it's a bit of a picture of and head driven phrase structure grammar theory that was developed at Stanford and in the in the 90s and it's a very complex data structures and articulated structures used to describe linguist these two things might think some good points in the middle where we have structure and that's going to help us do what we want particular medical interpreter language it seems like we don't just wanna have word vectors we won't have meanings of bigger phrases so play borders leaping over a mogul and a person on a snowboard jumps into the air and what would like to be able to say is that the snowboarder means basically the same thing as a person on a snowboard so he won't have these chat language switching linguistics of the constituents phrases and say that they have a meaning and would like to be able to compare them meaning looked at least all the loudest have chance of lightweight right because we looked at convolutional neural networks where you good take three words and make a representation of the convolutional neural network but the fundamental difference is human languages you have the chance to have meaning that are different sizes so would like to say the snowboarder and is pretty much to medical equivalent to a person on a snowboard but the top one is 2 words long and the bottom one is 5 words long and so if we going to be able to do that I want to have these sort of constituents work with and neural networks and that sort of the central idea of debated some of the structured neural networks out to show you there's another related thing that you might want to think about is you know a person on a snowboard human beings managed to understand what that means and then a person on a snowboard jumps into the air how the to understand what that means it's sort of seems like possible answer today what's normally referred to as the principle of compositionality the people know the word person they know the word on they know the words therefore they can work out one on a snowboard means and they can work out what person on a snowboard means by knowing the meanings of components and putting them together into bigger pieces petition statistician at Brown University Stewie man and I guess the way he summarise this is either the principle of compositionality is true or God exist as you want but you know I think what he meant was well you know play please infinite number of infinitely long sentences in human beings understand they just have to be that people can know about words and wait for combined meanings and and make bigger meanings for how else could have possibly work that people can understand sentences be able to do that we want to be able to work out semantic compositions of smaller Elam meaning to Bigger pieces obviously isn't only and linguistic thing compositionality and appears and other places as well right so if you want to understand how some peace and machinery work different subcomponent stand Albert the different subcomponents work and how are fitted together and then you might have some understanding of how the whole scene works Aldi seems to be worked at work and Vision as well so here is a scene and again it seems like this scene has parts so they're little parts that go together and I said there people that go together into a crowd of people and there's a roof and a second floor and another bit of roofing the first floor that go together into a picture of this church and so this is also kind of a compositional seen in with pieces go together sort of seems like suddenly for language understanding and then really for one of the other things that we use for intellij I need to be able to understand bigger things from knowing about small parts computational sometime the most famous linguist is Noam Chomsky at MIT and and really computational I'm haven't been that friendly to linguistics linguists and particular some of Noam chomsky's theory of language because really he's never been sympathetic to the idea of machine learning or general that the empirical ability to learn from data he's 14 to refuse to that exists but if we never left look for a little bit of inside on the you know this is a recent paper of trump he's with authors and their sort of trying to give a version of what is unique about human language and essentially what they zero in on is that world if you're suddenly looking at you know humans vs other Philly intelligent creatures they suggest that the defining difference of human being is the day have this ability to model recursion and so the this paper argues that the singular allowed language to develop a human beings with that we could put together smaller part to make bigger things in a recursive process and that was the defining you ability not sure I believe that or not what you think but what I think is certainly the case is this furball with the structure of human language sentences pieces constituent for them hierarchically or recursively into bigger pieces as you go up in the tree and in particular you get this recursion where you get a little man phrase me in a bigger noun phrase like spaghetti with meat and you can repeat that several times giving you a Curse of structure and I have an example of that in blue of the top so the person standing next to the man from the company that purchased the firm that used to work out and that whole thing is a big noun phrase dad there's a noun phrase the man from the company the purchase the firm that used to work out which is another big noun phrase and well and said that there are play It's Like a company that purchased the Firm used to work out but you know it's still got inside of noun phrases like the firm that used to work hard and FC the even that's got in the tide of the smaller noun phrase which is just little pronoun is also a noun phrase the structuring of language list of hierarchical structure the same kind of things inside them I think that's just some early career finding that you're our language is recursive all sensors not quite clear the bedside sing and that's the reason to say something this recursive it has to repeat out to infinity I so as soon as you put any bound on something and you say look let's it noun phrase you just gave me with 5 levels and this thing that pretty implausible that someone is going to say there and so as soon as you sort make an argument like ok even if they said that no one is going to say noun phrase with 10 levels of nesting and if you put some hard limit on at like there sense is not truly recursive gutted I can't get out to infinity but you know regardless what you think about that that doesn't the gate the basic argument that you get this hierarchical structuring with the same kinds of things like noun phrases sentences verb phrases appearing inside each other in a way that has no clear bound like to the extent that I show you a complex sentence I can make payment even bigger more complex sentence by putting it inside you said to me that and then say my sentence tried so that's the same to which it does appear to be recursive generator process practically there are limits to how complex sentences people say kind of structure that please constituency structure trees so before they only time when we talk about passing and you guys did some of that I emphasise dependency parsing but the other kind of Passing which is actually the kind that the models and going to talk about today it was using with this idea of what happened called constituency parsing or Linguascope and call at phrase structure grammar what inside Theory these are context-free grammars where we're having terminals like noun phrase and verb phrase that's inside another noun phrase other prices heading up the sentence constituency grammar the pin Tree Braintree this was kind of an original pin treebank tree which is basically a phrase structure grammar like this with sort of various extra annotations and put on the nodes please pop days it seems like with light to have a new role model that can make use of some of this same kind of tree structure and so what to do for working out semantic similarity of constituent not only have a word vector space like we start off with right at beginning of the quarter but would like to be able to take Sichuan slight noun phrases the country of my birth and a place where I was born and also give them a meaning and so it seems like what would like to do is have a method of Computing the meaning of any phrase in a compositional manner such that the end result is also that these phrases Inside Out vector space model so it's still going to stick with a vector space semantics of phrases and we want meanings of phrases question is go about doing that answer number one in the hit use the principle of compositionality and show it's right and so won't what the principle of compositionality essentially says if you want it work out the meaning of a sentence for the meaning of any phrase any constituent is you're going to build at by knowing the meaning of it and then having rules that can bind these meaning so starting off of the country of my birth I should be able to calculate a meaning of my birth and meaning of the country and meaning of my birth and then a meaning of the country of my birth so it have meaning composition walls which will lead us calculate meanings up constituents or census seems kind of the right thing to do and said in the question as well can we then build all of how to do that a straightforward way of doing this so we have word vectors for the word and what would like to do is workout any representation of this Centre my son had two things to do we have working out what some write structure of the centre meaning computation to do of working out what is the meaning representation of this sentence for passing with should have been building sort of noun phrases prepositional phrase the prays sentence kind of you cat sat on the mayor's and then we if we had that we can then run some kind of meaning computation program and give us of a vector space please sentences what we won isn't do both of those in a little bit I'll show you an example of the kind go about the protein do that just sort of stepping back from I remember it's just what's different here right here we had neural network which in some sense of being out winners class up to now and it gives you a representation of the meaning of the country of my birth sort of you need a say that's the meaning of the country of my birth or we talk about other tricks like doing Max pulling across all of these or you could have a separate which store does attention over the Easter does give you a sort of representation of the meaning of the of any some sequence of words as well but this sort of different right this what the top the tree recursive neural network it requires turn sore any kind of phrase to have a tree structure so we know what it's component parts are but then we're working out meaning representations the phrase that are sensitive to what it's synthetic structure is that how the words go together to build phrases what current neural network with Justin and oblivious way running a sequence model along insane computer thing the obvious it doesn't in the obvious way give a meaning representation of of my birth or my birth contained and sided we sort of only have a meaning representation for the whole sequel things this way and we do have meaning representations for the different meaning for parts of the sentence that makes sense what we trying to do so how can we go about doing that how we could go about doing their dinners if we work bottom up at the very bottom we have we want to recursively compute the meaning of bigger constituent to compute the meaning of on the map what we can do is say well we have already have a meaning representation of on and mad b1020 All Network we could maybe get out of a 2 things the Vert Widnes score so this is what we gonna use for parsing we going to say do you believe do you believe you can put together on and the map home a good constituent that's part of a parse tree and this will be a big positive number if the answer is true and negative if it's not true and then we have a meaning composition device ok if you put together these two things what would be the meaning representation of what we put together is the first model that we explored which was doing this in a pretty simple way right so he was out meaning composition device patinated the two vectors of the constituents liver and bimatrix adult I'm a biased as usual put it through a 10 this work as old enough it sorted before things like really use became popular but maybe instead of having 10 age anyway but more like recurrent neural network and so this was down meaning composition that gave the meaning of the parent and then to the side what the score that was as to whether this was a good phrase we were taking that parent vector representation and x and that was giving us out a number think about the bit while we doing this you might think of this isn't quite a perfect model on meaning composition and later on in the class I'll talk about some more complex models and that we've been started to explore the Sword of a knock to get us going and it's gave us a way of building a recursive neural network parser which both houses meaning representation for them and so the way we did this was in the simplest possible way really which was to have a greedy parser so if we start off with the cat sat on the mat what we could do is say well maybe you should join that and cat together let's try that run at 3 on your network it will get a score and a meaning representation and what we could try doing that for Cat and Sam play possetting on we could try it for on and that we could try do it for that and Matt is PT with say ok well over the best phrase that we can make word vectors is the one for that hard so let's just commit to that one and it has this semantic representation and this point we can essentially repeat we did over there we can just reuse because nothing has changed also consider now joining the cat has a constituent with sass and get a score for there and so this point we decide ok the mat is the best constituent to build commit to that any representation on the mad that looks good commits to there on charging up and so we've got a mechanism for sort of choosing a part of the sentence in a in a greedy manner but you know when we looked at the dependency parsing rule for doing that greedily right and coming up with the meaning representation so that was our first model having a tree recursive neural network and using it for parsing more details here probably on I'm important this point so we could score a tree by summing the scores of each no working out PlayStation real working out for using this kind of Max margin lost that we've looked at in other places which way do things is completely breezily you local decision Leeds point and make that structure and keep on going if you want to do things a bit better and we explored say in search we could explore out several goodways emerging and then decide later higher up the tree as to which was the best way to merge talked about in this class mention something case people have seen at his traditional constituency parsing where you have symbols here like in PVP and they're exist efficient dynamic programming algorithms where you can find the optimal path terms in polynomial time so in cubic time so if you have a regular context free grammar and also regular probabilistic context-free grammar and you want to know what is the best part of the sentence according to the problematic context Free Grammar you can ride a cubic time dynamic programming algorithm and you can find and in the old days of cs224n all networks we just have everyone do that breaking the sainement of the old cs224n with riding with dynamic program to do context Free Grammar passing of Centre this sad fact is once you go to these kind of new network representations you can't programming algorithms anymore coz programming algorithms only work when you have symbols for a reasonably small sit for your non-terminal because if that's the case Tinie Tempah collisions write you have lots of ways of Passing stop lower down which colour turn out to be different ways to make a noun phrase or different ways to make a prepositional phrase and therefore you can save work with dynamic programming if you've got a model like this bring me to build is going through layers of your network and you've got a meaning representation and some high-dimensional Vector things and never going to collide and so you can never save work by doing dynamic program you either doing exponential work to explore out everything or whilst you're using some kind of being to explore bunch of likely stuff fight this Hunter vision at the same time so it wasn't just served completely a vague motivation of seems have parts that we actually started exploring that well you could take these pieces of scenes and then work out and representations the scene using a similar form of compositionality and so in particular this data said there was being used for a multiclass segmentation in vision where you start off with very small patches and then you wanted to combine them out into parts of a scene of so recognising which part of the picture was the building the sky the road various other classes at the time able to do this really rather well I'm using one of these three recursive started to all networks better than preceding work and Vision had done in the late 2000s decade old new network do this kind of stuff turn off exploring history structures neural network this was a cool original idea and no one had worked on tree structure neural network successfully before we were wrong that there were a couple of Germans in the mid 1990s and Hood actually started look at restarting your network stop the method of them so corresponding to the backpropagation Through Time algorithm and the Debbie talked about when we're doing recurrent neural networks they've worked out the tree structure of taste with a cold that propagation and through structure simple slides on this in the slide sort of Skipton can you look at them they're on the way then you can look at them I mean there isn't actually anything so if you remember scarring or something as early lectures of this class of working hard and the derivatives of neural networks and how work with recurrent neural networks it's sort of the same right you get this recurrent matrix at different levels of tree structure your something that derivatives everywhere I'll be only differences sorry because we now have tree structure your sort of splitting things downwards so Ford Pop we had a computer for play Doin' backprop when we have the backward propagation we have the error signal coming from above within combine it and with the calculations of this node and we sort of sending it back in a tree structure down to each of the branches underneath us that was out first version of things you know we got some decent results you got this good vision results today so do seem to do word for language both of Passing and doing we had some results I have next included here and sort of doing paraphrase judgement between sentences and model things and fairly well once we started thinking about it more it seemed like that very simple neural net function what's a brake impute the kind of meanings that we wanted to compute for sentence meanings and so we then sort of said about trying to come up with some more complex ways of working out kind position functions at nodes that could then be used to build a bed and your network some of the essence of that play slide that you know for the first version we just didn't have enough complexity of your network frankly right so when we had two play the bum play that by a white white Matrix that was so essentially all we had hope you've done more of a sense of in this class if you just click add me and X Wait mate modelling the interaction between these two vectors that you can think of this weight matrix has just thought of being divided into and half of it x that multiplies the Spectre so the meanings of these two things don't act on each other and so somehow you have to make your new network and more complex than that play which the theme to Simple is tomorrow we had just one white my tricks between used for everything if you are linguists do you think about the structure of language you might start thinking love well wait a minute sometimes you're going to be putting together a verb and an object noun phrase sometimes you going to be putting together an article on a noun a ball sometimes you're going to be doing adjectival modification blueball these things are very different in their semantic you ok I have one white nitric for this universal composition function meaning of Fraser work and you thought I might suspect it doesn't work and some go on and show some of those different things before I show the different things I'm going to show 1 more version that's or related to the first thing which actually gave a pretty successful and good parser for doing constituency parsing so this was another way of getting away from the passing being completely greedy and which was to actually split up hard the two parts of G we have to come up with the tree structure for all Centre compute the meaning of the sentence thinking was cancel free structure for a sentence something to do pretty well with a symbolic grandma does the problems with and B grammas answer they can't put three structures over sentences the problems you have a programmers that they can't computer meaning representation and they're not very good at choosing between alternative tree structures stop the two part who is sable let's just use a regular probabilistic context-free grammar to generate possible tree structures for sentences we can generate a k best 50 context Free Grammar structures for this sentence not something we can do very efficiently with dynamic programming algorithm we can annual net little work out the meaning representation of the sentence lead to syntactically untied recursive neural network what this is saying is that we had each node in the centre pedigree of a symbolic context Free Grammar for their category A and B and C things together will be able to say ok we've got a troll that says x goes to BC so that why scences this mode here parsing is symbolic then we want to work out the meaning of this phrase and well the second problem I talked about was just having one way of doing composition is what too much to be able to have sort of verb and object vs adjective and noun composed the same way so we have this idea of well since we've now know about the syntactic categories of the children that we maybe know that this as an adjective and this is a noun is have different weight matrices for composition depending on what the categories are so rather than where we for there was just this one universal wait mate you all meaning composition have this is the white Knight together the meanings of an adjective and a noun and it will compute and the meaning of this constituent but then we'll have a different weight maitre together the meanings of a determiner and a noun phrase or something like that play this one I guess some we want to be able to do things and so I'll solution to be able to do that is we sort of used the probabilistic context-free grammar define likely parsers and then only worked out meaning for ones that work quite probable and so we call this resolve a compositional vector grandma which was a combination of a pcfg and the tree recursive neural network essentially at the time this actually gave a pretty good constituency parser so they're sort of what the results here the top ones are kind of our classic older and standard pasta which is a pcfg of a kind of puzzles that people would build is that compositional vector grandma the time of this being done in 2013 in wasn't the very best available there been some better work by using Chania could Brown had a pretty good parser coming out of that sister more interesting was we we didn't only have a parcel that was meant to give the right path trees we also computing meaning representations of no the consequence of that you can look at not only meaning representations of nodes you couldn't learn about the white matrices that these models were learning and when they combined together meaning category-specific w matrices they were going together with the children to work out the mean little bit hard to interpret but the dealer please matrices win Nisha lies them as a pair of diagonal matrices he's also 2 by 1 rectangle matrices because their two children is left child the other half is multiplying the right child and we initialise them sort of like her 2D matrices next to each other which would give us the source of the faults Amanda averaging until something different was learnt in the in the spectre stent interesting has been learnt by the model you'll get yellow along the diagonal all in the store play win the rest of the field and the extent of the it's weren't something interesting to take out of the semantics of a child will then start to see reds and oranges on the diagonal one that Blues stop in the rest of the field so what you find is that if you train this model it's learning about which children are the phrase actually the important one time for saying that if you're combining the other noun phrase and the coordination so something like that cat and that most of the somatic has to be found in that cat and not much of the semantics is going to be found in NZ binding together a possessive pronoun something like her or he is with noun phrase Insider like or something like that in most of the mean is to be found inside the teddy cat constituent does actually learning very important semantics of sentences is an example this one Sonic for this one shows their variety of modifications structures with adjectives or adverbs either noun phrase and adjective phrase adjective noun phrase is seem to notice that there are particular dimensions which are kind of capturing sort of modification meaning so dimensions 11 instead of showing up in these different combinations here's sort of Catrin meaning components that was kind of neat slightly more complex tomorrow actually worked pretty well at capturing a meaning of phrases and sentences we were giving it the system a test centre well what are the sentences that are most similar in meaning nearest to paraphrases in our corpus for this sentence all the figures are adjusted the seasonal variations the tumor sentences in the corpus worth all the numbers are adjusted the seasonal fluctuations that's a pretty easy one or all the figures are adjusted to remove usual seasonal pattern you working pretty well one comment on the offer passcode kinda save what country place the order the semantics there a bit more different for this seems like it is capturing something similar coastal won't disclose the terms that kind of a really interesting one because that one is actually very similar and meaning for thus expressed that very different way in terms of the words and the syntactic structure that they use that was progress how we could have different matrices but different constituent type but there's still some reason to thing and that was we're still at Heart using this very simple structure Mary just concatenating x matrix so that means the two words interact with each other in terms of their meaning it seems like we want to have them interact in their meaning pixel art if you about human languages and the kind of things that people look at it linguistics Amanda word playtime modifiers or operator Woodbury the main match by itself I mean that means something like strengthening or or something like that but you know it doesn't really have a meaning I just doesn't have any denotation you can't show me very things I didn't show me cheers and pins and children that you can't show me very things that the meaning of very seems to be that something comes after it good and have an operator meaning of increase on the scale this thing what's on the scale in either Direction you can have very good or very bad if you want to capture when does the man take cardcaptor that kind of semantics by just concatenating two vectors and multiplying them by Matrix what we really wanted is it's going to grab hold of the meaning of good and modified and some way to produce anew meaning for very good that's the kind of approach that typically and being done in linguistic semantics so in Mystic theories of semantic should normally say ok good has a meaning very is a function that takes in the meaning of God in return the meaning of very good to have a way of putting that into a neural network and so to try and come up with a new composition function as the how to do that where is Waze think about doing that Adam other people had a couple of different of 10 what was in our head is well word vectors and if we want to say that very takes the meaning of God and Returns a new meaning obvious thing to do is to say very has a matrix attached to her because then we could use the the very Matrix and multiply it by the good vector and we get a new words have victors and which words have matrices and that's kind of play the answer to I mean in particular that actor's operators can often themselves be modified good also right so that's have a good person that's also an operator and very is modifying that good we came up with his left I'll try and predetermined all of this say that every word and every phrase has connected to her nectar so he is a very good movie Sophie each word we have a vector meaning and it hasn't matrix meaning and then as we start to build up phrases like very good they're also going to have a victim meaning animatrix meeting what we propose play festival with we would like to be able calculate the vector meaning the victim meaning of a phrase like very good each word has a matrix meaning and so it going to combine their opposing Matrix and vector meaning so we're going to take the Matrix meaning of God and X the Victor mean are we going to take the Matrix meaning of vary in x the Victor meaning of gourd and so we're going 2 things don't have a new network we're like before the combined those together until they're talking to red box things we can caffeinated and put through the kind of your network layer we had before the give us a final victim meaning phrase so needed a matrix meaning for the phrase matrix meaning for the phrase kind of simple model which maybe actually wasn't very good which was the say let's just play the two matrices of the Sichuan play then by another Matrix play me give us some m turn up the parent note Davies Avenue Morecambe composition procedure did seem like a could do some kind of good things that captured operated semantics where one word modify the meaning of another word kind of a neat thing that we were able wanting to be able to what does Samantha Rita modifying another word so unbelievably annoying and believably awesome and believably set Ed not awesome not sad this contrast thing now call vs the new model play all is a scale of positive to negative weakly negative pause and so the kind of contrast are the four annoying that the simple model thought that this is pretty negative where is the new model thinks this is pretty neutral and meaning and that seems to be reasonably correct sad that means a little bit positive and both models were trying to catch results little be then people and put with sort of seems that they sort of God little bit in the direction of what we want yes Chris Brown truth was we actually asked the whole bunch of human beings to say race meaning of knots ahead and on the scale of 1 to 10 maybe this was a very good clear task because he can see it bounced around who what what kind of ratings and we were getting for things but yeah that was actually going against human judgements then use this auto disabled could we do and semantic classification task so if we wanted to understand relations between different noun phrases play the set where licence Mark between two noun phrases my apartment has a pretty large kitchen that was seen as an example of all I have part of relationship between the two noun phrases and there were other relationship different kinds of noun phrases Surfers the movie show dwarf then a message topic for the some communication medium that contain some topic relationship and so we were using this kind of neural network to sort of build out meaning representations and then putting them through another new network later as a classifier to see how well we did what's a really good results on that to this was a day the said that people and worked on with traditional in IP systems of different kinds of machine learning methods what were interesting was we seem to be making progress and having a better somatic composition system that are old recursive asking about 75% and then our new one was going about 79% which we could sort of push-up further by putting more features into a system progress didn't stop there and we kept on trying to come up with better ways of doing things even though things were it's sort of seems like doing matrices necessarily very good it's sort of had two problems alarm was the humongous number Predators cos you know just about everything that we've done otherwise words of head of Victor and well maybe sometimes we use quite high dimensional vectors like 1024 but you know that's a relatively modest number of parameters where is once we introduce this matrix here we've got that numbers with additional parameters for every word essentially because of that number of parameters to be able to compute this model at all wheel making the Victor size small so what we actually use it was a DS4 just 25 dimensional vectors so that the 25 squared 625 still save sort of decently within the Range and which we could compute first problem the second problem is have very good ways of study meaning of bigger phrases you know this sort of seem something simple we could do that I didn't know feel a very good way of getting a matrix mirror frame come up with some other way of doing things both of those problems play into work on recursive neural tensor network kind of a nice idea here of these Newark answers which is an idea of actually been used in other places including work pudding spectrum beddings of knowledge graphs and thorn which is the kind of a bit of a nice idea so I wanted to this model work does the se place where we applied this model was on the problem of sentiment analysis I think the term sentiment analysis has come up a few times there's something you can do next Richard the last skipjack talk for 5 min pass on sentiment analysis tool sample of that sentiment analysis is actually been a really coming in an important application in natural language processing and you're looking at a piece of tech say is that I'm positive or negative that's just something that's very useful for lots of commercial applications of looking at product reviews or doing bread some things like that of some looking at sentiment connected to things extent 2-in cm in an oz is easy right that you can kind of stay well look at a piece of text if you see words like love grade impressed marvellous tennis positive is a positive review and if it's awful to negative review and to some extent that the baseline of sentiment analysis that you can use just collective word features the all words in a bag of words and if you do that you don't actually do that badly what's the city have longer documents just looking at bags of words and give you 90% in centimetre analysis on the other hand things are often do get trickier ISO and this is from rotten tomatoes with this cast and the subject matter the movie should have been funnier and more entertainment pretend you're a bag of words model the only words in the play Leisure Centre entertaining and funnier and both of those are pretty positive word that is fairly obvious that this actually isn't meant to be a bad review of the movie and so well how are we meant to know that what sort of seems again like what we have to do is meaning composition we have to it should have been funnier and then realise that that's actually and negative meaning for a phrase to explore how we can look at those sort of meanings for phrases building up those meanings has doing meaning composition over trees first thing we did was we built a tree bank of sentiment trees where we got people to rates in demand and so this website the Stanford sentiment treebank which is still a dataset you often see yous then I'm very directions for the whole bunch of day the States indeed showed up and Decker NLP last week what we're doing this was taking which were rotten Tomatoes sentences from movies Nottingham to give tree structure and then we were asking the chemical turcas to re free the different words and phrases on a sentiment scale of very positive to very negative so lots of stuff is white because it's just not sentiment laid and write this words that have that and there's phrases like the movie and the movie was which don't really have any sentiment that then you have pieces that sort of very posh and negative pieces of tree that have been shown in the blue and the red typical in sentiment dataset believe labelled the entire Centre this is our positive sentence for very positive sentences and negative sentence for very negative Centre what we were doing differently here is every phrase in the sentence according to structure was being given a labelled With positivity or negativity and have not surprisingly just affected you have a lot more annotations like that proves the behaviour of classifiers kind of can do better attribution of which words in a sentence a positive or negative results of the preceding model green is a naive bayes model accepted not only uses individual words that uses pairs of words building a traditional classifier and you want to do sentiment analysis as opposed to something like topic classification you get a lot better results if you also use word peer features and that's because it doesn't baby bit of composition for you you don't only have features for not an interesting that you can have a feature and not interesting and atleti model surname out of stock older generations of neural networks out of original tree structure in your network now matrix-vector simply having fix model simply having a rich shark supervision that comes from a nutri bank it sorted moved up the performance of every model so even for just the naive bayes models performance is going up about 4% and because of a knows more about words of positive and negative in the sentences none of these performances are really great we still thought that well can we build better models of how to do this circular if you look at sentences with sun times of allegation you know things like should have been funnier these models and general still couldn't capture the right meanings for them do they switches this idea of recursive neural tensor network what we wanted to be able to do is go back to just having meaning words to be vectors unless the spike bad to be able to have a meaning for a phrase where the two victors acted on each other this kind of this is the picture of what we did when we were doing a tension in a bilinear way right we had victors for two words we stuck in Matrix in between and we use that and gave an attention in Ghana detention score two vectors interact with each other but it only produced one number as the app fix that which is thus a well rather than having matrix here what we could stick here is a three dimensional cube deep learning people cut now call a tensor2tensor installer ready Hammond computer science term if we sort of made that a tensor you know it's like we have sort of multiple layers and matrix here and so the end result of that is we get one number here and one number here so in total we get it out of flies to Victor which is all we need my baby example where we only have these two component vectors for words then generally have a tensor with the extra mention the mention of the size of our word vector and so therefore will get a word Victor free Spectre app from the composition that the same size of the input vectors and will allow them to interact with each other in working out the meaning of the entire thing that point on we use the resulting vector so we had on your tensor network we actually can find it together with this of previous kind of layer we used to have house of first rnn maybe you didn't need to do this we just decide where they've been as well put things to a nonlinearity and that was them giving a sound new representation of phrases we built that up the tree and then the end we could classify the meaning of any phrase what kind of way with softmax regression we could train these weights with gradient descent to predict sentiment play it didn't so really work it any better with just the sentence labels for the free train the model without treebank we could then get a kind of the burden of a couple of percent in performance and so that seems good Littlewoods seem to do a much better job of actually understanding meaning composition Kinder wave have they are slow and repetitive parts but it has just enough space to keep it interesting and the model same you're pretty good and understanding ok this part of the sentence is negative what sentences positive and actually when you stick the two hard together the end result is the centre what's the different meaning singing a little bit more but seems like it's especially good was for the first time the sexy did seem like it could do a better job when you do things like medication it's just incredibly doll and it's definitely not dull so if it's definitely not dull that actually means of good night can we work it out the meaning of it's definitely not dull this showing a solid when you have negative sentences further negated go from 50 Jason of a negative things should become moderately positive right so that if you have done with negative and if you say not doll it doesn't mean it's fantastic but it means that the moderately positive and so far time naive bayes model Orrell preceding models they weren't capable of capturing that so going from dolls and not doll your your can you taste and did not come out any more positive where is this Sunday new Tintin network was capturing the fact that not dull meant it was reasonably good that was progress yeah so I think that's as much as I'll show you really know about applying history structured neural network think of summary I thought of said at the Beginning is that I think the other kind of interesting ideas and linguistic connection I mean reason just haven't been can in recent years natural language processing you no one is in all honesty people have found that snow vectors in things like the kind of sequence models that we've looked out with those meeting things like this of lstm model any of the more recent contextual language models that work incredibly well and it's all these models way better result of a computational reason which is is work great when you're doing uniform computation and the beauty of having something I can sequence model is that this is just one determinate computation you're doing along the sequence or in the convolutional neural network there's one determinate and computation you're doing up convolutional layers in there for things and be computed efficiently on a GPU the huge problem with these kind of models was what computations you were going to do dependent on which structure you're signing to the centre sentence with going to have a different structure and so therefore there's no way to fetch the computations over group of sentences and have the same computations being done for different sentences with some of undermined ability to solve efficiently build these models in the lodge alright just say women about at the end the funny thing is please haven't been used much for language in the last few years and they've actually had some used and found different applications in different places and which is just the same kind of cute so this is Akshay and application from physics switch off after read this since I have no idea what have the words mean but what it says is by far the most common structures seen in collisions at the Large Hadron Collider a collimated sprays of energetic hadrons referred to as reproduce from the fragmentation and hybridization of quarks and gluons as described by quantum chromodynamics guess what that means I hope your follow along here one compelling physics challenges the search for highly boosted standard model particles decaying hadronic play there's a large background from Chips produced by more Mandy quantum chromodynamics processes let me propose instead a solution for jet classification based on an analogy between play my dynamics and M languages is inspired by several works from natural language sentences composed the words virus and texts extractor organised as a parse tree at Jess is also composed the former mentor following a structure dictated by qcd and organised by the clustering history of a sequential combination algorithm so anyway network and to Model S Robert Fairfield attitude as one more example the another place where these models have actually been quite used for his for doing things in programming languages Linkin Park this is because the application is easier in programming language I can natural language where we have this uncertainty as to what is the correct past 3 because there's a lot of ambiguity in natural language witches and the past trees are actually pretty determinate I'm so group of people at Berkeley don't song in her students have worked on doing programming language translation by building 3 recursive neural network encoder decoder building up a tree structure new network representation of a program in one language this is a coffee script program and then you wanting to build a 3 to 3 model which is in translating that to a program in a different language go to do that and get good result play Si to retype this table so this is probably a bit bit hard to read but once contrasting is for a number of programs this is a copy script JavaScript translation they comparing using 323 model who sings sequence-to-sequence models and then they try both other combination free entry to sequence what pay fine is you can get the best free neural network models and in particular these treated three models are augmented with a tension so they have attention like we talked about the sequence-to-sequence models wear your then they have to do a tension back to Nodes in the tree structure which is a pretty natural way of doing translation and indeed what these results show is if you don't have these results show it's if you don't have detention operation it doesn't work at all it's too difficult to get things I'm so done if you just so trying to create a single tree representation and then say generate switching from that but if you do it with the sort of putting attention into the different nodes and that's great coffee scripters you might feel like white that's cheating slightly because copy script is a bit too similar to JavaScript but they've also done it in other languages so this is going between Java and C Sharp is this sort of a handwritten Jarvis c sharp converter that you can download from github if you want but it doesn't actually work that well and and they're able to show that they're able to build a far better Java to c sharp translator sexy kind of tool and it's good to know the tree structure of recursive neural networks good for some things so I'm for your story he work like this that done but I thought before and finishing I just mention one other thing which is sort of nothing to do with natural language processing precisely because about AI but I wanted to sort putting up little bit of advertisement and that's something that the number of us have been working on very hard for the last year or so is developing a new Stanford Institute for human centred artificial intelligence play the once this Institute is going to be on Monday of exam week just when you're mixing we concentrating time of things such as this I hope is that we can have a 1 of new activities around artificial intelligence border for spectacle artificial intelligence centrally viewing at from the viewpoint of humans and working out borderlines of issues then brace a lot of the interests of the rest of the university weather's the social sciences and humanities also various in professional schools like a wolf school and the business school and so just quickly saying about Dad motivating idea is that so from most of my life sorted like a kind of a fun intellectual write bits of software that did everything and halfway intelligent but that's clearly not what's going to be what's happening for the next 25 years that we now at this point in which artificial intelligence systems are being under Messi and well hopefully they do some good things but as Ruth increasingly been seeing there lots of also lots of opportunities for them to do bad things and even if we're not imagining Terminator scenarios there are just lots of places where people are using machine learning and AI algorithms cision some of the worst ones are things like sentencing guidelines and Court biased algorithms making bad decisions and people are really starting to become a lot more aware of the issues and so effectively with wanting to have this Institute what are the work of social scientists episys and other people to explore how to have an AI that's really improving human lives around having the opposite effect seems that way emphasising for this Institute is the first stop galloping AI technologies particularly interested in making linkages back to human intelligence cognitive science and neuroscience early formula of work in AI was done including Zoe work and your networks like the development of backpropagation it was actually glad she's on the context of cognitive science writing that was so the link it's that tended to get lost in the 90s and 2000s machine learning in 430g good to renew that I'm the top right my pensions the human and societal impact of AI and so this is mocking it legal issues choose labour 430f power politics whenever you the bottom is something where it seems like this just kind of enormous opportunities to do more which is how can we build technology that actually augmented human lives like to some extent we've got with AI augmenting human live so all of yourself phones have speech recognition in them now so you know that say I can augment your human lives not very much of artificial intelligence is actually been put into a server I think human lives like most of what a cell phone has on it is still sort of clever and cute stuff done by HDI people and designers which is very nice to one of the time when you're using your map program or something but we don't really have much inside these devices helping to make people's lives better and so we're hoping not only for individuals been applications like Healthcare to be doing much more so than putting artificial intelligence into human-centered application I'm looking out for this why you're not studying for your exams and I think they'll be so lots of opportunities for students and others to be getting more involved in this in the coming months thank you very much see you later 