tyre run let's get started see you all here welcome back for wk20 cs224n little preview of what's coming up in the class for this week and next week speakers pub the worst week of this class so our hope is to actually kind of go through some of the nitty-gritty of neural network Calvert rain and how we can learn good neural networks by back-propagation which means in particular we're going to be sort of talking about the training algorithms calculus to work out gradient from proving little bit at word window classification named entity recognition so there's a teeny bit of natural language processing in there but basically sort of week 2 is sort of math of deep learning and neural network model sense of relief Jol network fundamentals hope it kind of a good understanding of how these things really work and will give you all the information you need to do and the coming up homework in week 3 we kind wheat-free is going to be mainly about natural language processing so then going to talk about how to put synthetic structures over sentences and for building dependency parsers of sentences which is then actually what's used in homework 3 so it's charging along rapidly and then we'll talk about this idea of the probability of a sentence which leads into neural language models homeworks homework 1 was due 2 minutes ago I'm so I hope everyone has submitted their homework one I mean just sort of in general you know homework one we hope you found was a good warm up and not too too hard and so really best to get homework 1 in quickly rather than to burn lots of your late Days doing homework right now out on the website and this homework to so sing along so homework to kind of corresponds to this week's lectures on the first part of it we are expecting you to grind through some math problems are working out gradient derivation the second part of verb is then implementing your own version of Word to Vick making use of numpy and so this time so I'm riding a python program so long when I type on Notes get early look at the material I mean a particular corresponding to these days lecture there's some quite good tutorial materials that are available on the website and also encourage you to look at those generally just to make a couple more comments on things I mean I guess this is true of a lot of classes at Stanford but you know when we get there news for this class we always get the full spectrum from people who say the class is terrible and it's way too much work I'm just people who stay at the really great class one of their favourite class does it stand for obviously instructors Kia etc and I mean probably this reflects that we get this very wide range of people coming to take this class on the one hand modern pets we have the fish Easton on the left-hand margin we have some Frost you think this will be fun to do anyway we welcome we welcome everybody but in principle this is a graduate-level class me want to fail people out with like everyone to succeed but also like graduate-level class and would like you 3000 take some initiative in your success there are things that you need to know to do the assignments you don't know them then you should be taking some initiative to find some tutorials come to office hours and talk to people and get any help you need and learn to sort ok so here's the plan for today so that was the course information update so you know in some sense you know machine learning neural nets intro just to try and make sure everyone else is up to speed all this stuff so took a little bit about classification introducing your network insurance named entity recognition then sort of bottle of doing window wood window classification and then the m park and we sort of them dive deeper into what kind of tools we need and to learn your network play we're gonna go through review and primer of Matrix calculus we didn't next time like shark where it's talking more about backpropagation and computation graph material especially apart at the end he's out for some people it all seemed really baby it if you do every week people it might seem impossibly difficult and but hopefully for large percentage of you in the middle it will be kind of a useful review of doing this kind of Matrix calculus and the kind of things that we hope that you can do on homework to ok so boring some people if you sent through to 29 last order is sore for a classifier was like and hopefully this will seem familiar but I just thought of hoping to try and have everyone and week to sort of up speed on roughly the same page so he is a classification setup so we have assume we have a training Day The Third where we have these Victor exponents and then for each of one of them we have a class the input birds of sentences documents or something there a d dimensional vector and the why I have the labels or classes that we want to classify two and we've got a set of C-Class as that we trying to it might be something like the topic of a document naked of of a document or later will look a bit more named entities so if we have there tuition is we've got this vector space to which we can have a 2D picture and we have space which correspond to our exercising want to do is look at the ones in our training see which ones are green and red flower two classes here and then we want to sort of learner wine that could divide between the green and the red ones as best as possible and that learnt line is there a classifier is snow machine learning or statistics excitech disregard data Rider fixed but we going to then why those xib weight vector and an estimated white vector will then go into a classification decision and the classifier that I'm showing here is a soft next classifier which is almost identical but not quite to logistic regression classifier which you should have seen and cs109 or steps class or something like that which is given a probability of different classes and in particular if you've got a softmax classifier or a logistics logistic regression classifier these or what I called linear classifiers so the decision boundary between here is a line in some suitably high-dimensional space for the plane or hyperplane once you've got a bigger X Factor can you sell softmax classifier in the result of two parts to their so in the weight matrix w we have a row corresponding to each class for that row we're so dot product doing it without take the exam and that's giving us a kind of a score for how likely it is that the example belongs to that class running let through a soft next function and just as me sore and weak one this softener numbers in turns them into a probability distribution last week is it to this in detail but I mean play this is what I would just stick regression does as well the difference is in this setup Victor doing logistic regression is they say wipe that gives us one more number of weight vectors that we really need we can get away for see classes we can get away with C - 1 white vector some particular if you doing binary logistic regression you only need one white Victor this softmax regression formulation you've actually got to White vectors 132 class a little difference there which we could get into but basically the same as just mixology certification doesn't matter what to do if we want to be able the correct class the way we're going to do that I just we're gonna want a train and model so it gives us highest probability as possible to the correct class give us a low a probability I'm too and the wrong classes criterion for doing there is we going to create this negative log probability of our assignment I want to minimise the negative log probability which corresponds to maximizing the log probability which corresponds to maximizing and the probability now we're going to start doing more stuff with deep learning frameworks hi tour discovering that that there's actually a thing called in ill illness negative log likelihood play No One used as a custom or convenient thing to use it's what's called the cross entropy loss you're here everywhere that we're training with cross entropy loss briefly mention that and explain what's going on there script of cross entropy comes from baby information Theory which is about the amount of information Theory I know so we're assuming that there's some true probability distribution p will be built some probability distribution Q that's what I've built with Airsoft next progression and we want to have a measure of whether a probability distribution of the good one and the way we do it and cross entropy is we go through the classes and we save what's the probability of the class according to the true model waiting within work out the log what's the weather according to our estimated model and we sumners up in the Gateshead and dad is that cross entropy measure listen general gives you a measure of distribution in particular case for each example we've sorted of assuming that this is a piece of labelled training day that so we're saying for Dad example the right answer is class 7 our true distribution LP is this example it's class 7 with probability one and it's class GLS with probability zero think about them what happens with this formula you've got this summation of all the classes but PFC is going to be the 1 or 0 and it's going to be one only for the true class here and so what you're left with is this is going to be cold minus the log of QC for the true class work then computing in the previous slide basically where you get with cross entropy loss what are the concepts to mention so when you have a full day the third of a whole bunch of examples the cross entropy loss is been taking the poor example average so gets what information Theory people sometimes called the cross entropy effective if you're training it on and examples that one on infect that's coming in there they're ok yeah picture of the actual labels in the Grand Prix set your gold data someone is henway ward and they've label one and the rest of the euro open spaces where that isn't the case I mean one case as you could believe that human being sometimes don't know the right answer so human being said I'm not sure whether this should be clear you can imagine that we can make play the way we put probability half on both of them and that would be a crazy thing to do and so then yeah have a true cross entropy last using more of a distribution can you swear it use an actual practice many circumstances in which people want to do semi-supervised learning so I guess the topic that both my group and Chris Rea group at work done quite a lot where we don't actually have fully labelled Daisy but you've got some means of guessing what the label for the day they are and if we trying to guess label today they will then quite often will say he is this data ride and it's 2/3 chances this label but it could be these other for labels and with use the probability distribution and yet then it's more general cross-entropy loss this bottle b a little bit different which is the same one Ellie this is the son of the full day the same the other thing to know this when we have a full Paul Davies head of Exeter and then we have wait we're here we're working vector for the weights for one class for all classes so we can sort of simplify what we're writing here and we can sort of using matrix notation and just work directly in terms of the Matrix w sofa traditional optimisation what does Rd for the different classes so for each of the classes dimensional driveway dot product without d-dimensional input vector so we have the aw mate the parameters of our model model using the ideas of to send we going to do so that's what we started to talk about last time we have the setup parameters median the partial derivative what's with respect to all of these parameters and we use that to get a gradient update on our loss function and we move around the w's and moving around the w correspond moving this line that separates between the classes and downsides to minimise cells times to Tuesday line that best separate the items of the classes in some sense that's a basic classifier so the first question is hanabi different with a neural network classifier central observation is the source of most of the classic classifiers that people use the lot of the time so there include things like naive bayes model basics support vector machine logistic regression simple in particular those are all minion classifiers which are going to classify by drawing a line on high-dimensional Space by drawing some kind of plane the separate example simple classifier like there can be useful in certain circumstances I mean that gives you what in the team learning of the highest classifiers that lot cs229 if you haven't Davis set and this like this very good job at classifying all the points correctly if you have a high bias classifier cos you can only draw a line so you'd like to have a more powerful classifier centrally what's been powering a lot of the use of deep learning is that in a lot of cases when you have natural signal so those the things like and speech language images and things like that you have a tonne of data so you could learn a quite sophisticated classifier and but leaving the classes in terms of the input data is stored of very complex you could never do it by just drawing a line between the two classes you'd like to use some more complicated kind of classifier new networks the multilayer neural networks that we're going to be starting to get into now precise see what they do is provide your way to learn very complex almost complex and fair classifiers so that if you look at the decisions that they making in terms of the original space in cases like this couple of the slides here personalisation that was done by Andre capacity you as a PhD student here until a couple of years ago so it's a little Jarvis that you can find off this website and it's actually a lot of fun to play with to see what kind of decision boundaries you can get in unit to come up with getting more advanced classification out of neural net use for natural language Goa do that I want to talk about featuring in something when it comes down to a brothel and mention separately at the beginning that one of them is these word vectors second one is that we're going to build deeper multilayer networks expose crucial difference said and we already started to see ring last week having a word being this is the word hell say hey Victor real numbers what we can do isn't change the vector that corresponds to hell why build better classifiers which means that we're going to be sudden movie presentation around the space interesting like word similarity and allergies and things like that so this is actually you know kind of a weird idea compared to conventional steps for aml so rather than saying we just have the parameters w we also say that all of these word representations are also premises of our model so we're actually going to representations of words pacifiers to do better so is simultaneously changing the way changing the representation of words and we're optimising both of them at once to try and make and model as good as possible and so this with people often talk about the Deep learning models that we doing representation learning are two ways I was going to mention two things once this sort vector representation learning and then the second one is that we're going to start looking at deep and multilayer neural network turn over here on the slide is the observation that really you can think of would bedding ASDA having a model with one more neural network layer that vector weather for the different word types in your models see you header at your 150000 dimensional vector with the so one hot encoding of different word you could say you haven't a matrix l which is sort of your Lexicon Matrix vector for word you'll Ned which multiplies the one hot vector one hot vector and since this is a one hot vector what that will have the effect of doing is taking out a column of l and so we've got an extra layer of Matrix in our new net and we'll turning the parameters of that mate play as we learning a deep neural network for other purposes that completely makes sense and that sort of a sensible way to think about what you're doing with word embeddings and you'll network mentation why's this makes no sense at all and no one does this cause it just doesn't make sense to do a matrix multiply when the result of the Matrix x will be ok this is word ID 17 construct a 1 hot vector of length 250000 with a 1 and position 17 and enjoy matrix multiplied makes no sense you just take out and Colour actually does 1 obligatory picture of Newry so don't miss them I'm not going to show it again all parts show the Irish network in sunset construe artificial neurons that seem to in some sense kind of capture the kind of computations and they go on in human brain very loose analogy for what was produced but you know how model here is these are out this is what about human brain so here and neurons this is a neuron and so what does a new one consist of so at the Beck it's got these dendrites what's a dendrite nobody Inn if there's stuff coming in on the dendrites body will become active and it'll start spiking down this long thing which is called BX on and so then these exons lead to the dendrites of a different cell or lots of different sales try this one some of these are kind of going to different cells and so use them have these buttons on the ex song which is kind of close to the dendrites and have a little get in them and some in miracles biochemistry happened there and so that the synapse of crossed activation flowing which goes into the next neuron starting off all the people wanted to try and simulate in computation and so people came up with this model of an artificial neural coming in from other neurons at some level of activation that's a number x 0 x 1 x 2 synapses very how excitable they are it's the how easily they let signal cross across the synapse and said that's being modelled by multiplying them via wait w0 W1W 2 nobody is Sword of something this amount of excitation it's getting from the different then dried and then it can have its own biases to how likely is to fire that's the be so we get there and then it has some overall kind of threshold or propensity for firing so we sort of stick it through an activation function all the terminal firing raid and that will be the signal was going out on the output Excel starting point of the app but you know really what we've ended up confusing we just have a little bit of baby mat here which actually looks very familiar does kind C in linear algebra and statistics really no particular a neuron can very easily be a binary logistic regression unit so that logistic regression you're taking for your input x you X Wait better you're heading I'm bias term and then you're putting it through and and nonlinearity like the logistic function and then so you're calculating a logistic regression this sort of neuron model what is the dance between the soft Max and logistic regression as by saying that where is the soft next to two classes as two sets of parameters is still just has one set of parameters and your modelling the two classes by giving the probability of one class go to 1 depending on with the input to the district regressions highly negative or highly positive really we can just say these artificial neurons like binary logistic regression unit variance of binary logistic regression units by using some different if functions and will come back to that again pretty soon this one year on so one year on is a logistic regression unit for current purposes what we wanted to do with neural networks a well why only run one would you stick regression why don't we run a whole bunch of logistic regressions at the same time input liberal logistic regression unit but we could run free logistic regressions at the same time any number of that score conventional training of a statistical model with stud of have to determine for those Orange logistic regression you know what with training each of them to try and capture with Deezer to predict what they're going to try and capture some of them building begin you all network decide ahead of time orange logistic regressions are trying to capture we want the neural network to self organise orange logistic regression you know something useful something useful will allowed ideas to say we do actually have some task that we want to do tasks that we want to do so maybe we want to sort of the side with a movie review as positive or negative something like sentiment analysis or something like that there is something we want to do at the end of the day and we going to have Western classify they're telling us positive or negative but the inputs to there I'm going to directly be something like words in the document they're going to be this intermediate layer of logistic regression unit get a train this whole thing to minimise our cross-entropy loss essentially what we going to want to have happened in the backpropagation algorithm or do for us is to say the middle it's your job to find some useful way to calculate values from the underlying data such that will help our final classifier make a good decision particular you know back to you know the final classifier it's just a linear classifier a soft NEXO logistic regression it's going to have a line like this intermediate classifiers like a word in bedding they can kind of Surrey represent the space and shift things around so they can learn to shift things around in such a way as you're learning a highly non-linear function of the original input space order of saying wealth why stop there and maybe even better if we put in more layers and disorders gets US into the area of deep learning precisely this is that sort of there 3 comings of neural networks as the first work in the 50s which is essentially when people had a model of a single neurone like this literally worked out related to more conventional statistics second version of new networks with her so the 80s and early 90s and where people and built new networks like Facebook had this one hidden layer where a representation could be learnt in the middle time it really wasn't effective weren't able to build works and get them to do anything useful so you sort of head these neural networks that one hidden layer I stay with research that started learning the precisely the motivating question is will be able to do even more sophisticated classification for more complex tasks things like speech recognition and image recognition if we could have a deeper network which will be able to effectively learn more sophisticated functions of the import which will allow us to do things nice sounds of a language how can we possibly train such a network so at their works effectively sting sample go onto more so the next lecture before we get there just to underline other game so once we have something like this is there a layer of a neural network we have a vector of import vector of output everything is connected so that we've got this sort of and why 1 and face backlight say a 1 is wait times each component of X1 and having a biased running features of this part and then running it through our non-linearity that will give us the output are we going to do that for each from A1 A2 and A3 we can kind of regard as the Victor and we can kind of collapsing into this matrix notation for working out the fix of layers so fully connected layers are effectively matrices of weights write them like this where we have the biased terms of Victor of biased terms is Son of a choice there you can either have an always on in Port and then the buyers terms become part of the weights of a slightly bigger matrix of 1Xtra the column or row just sort of happened separately written as these clothes here right so this part we always put things through a nonlinearity was referred to as the activation function and so something like the logistic transform I showed earlier is an activation function and this is written as sort of quiet function giving a vector output and what this always means is that we apply this function element wise logistic function which is sort of a one input one output function like a little break I showed before play that to a vector we applied to each element of the vector element wise better very soon to sort of saying more about nonlinearities and what nonlinearity wondering is why have these nonlinearities and say they are histamine if functions there you know why that we go sequels wxb in one layer and then go onto another layer that also die equals W2 £50 going with m like that precise reason for that which is if you want to have a new network learn anything interesting you have to stick in some function linear functions such as and logistic Curve I showed before that is Sword of doing linear transforms like WX + b and then W2 that's pw3 Z2 Plus be and you're doing a sequence of linear transforms multiple linear transforms just composed the become a linear transform so 1 linear transform is human stretching now and you can rotate again but there is all there is just one bigger rotate and stretch of the space any extra power for a classifier by simply having multiple linear transform as you sticking almost any kind of nonlinearity then you get additional power general what we doing when we doing deep network middle of them we not thinking although it's really important to her nonlinearity thinking about probabilities or something like their general picture is well we want to be able to do effective function approximation or curved beading would like to learn a space like this give me do that if we sort of putting in some nonlinearities which allow us to learn these kind of curvy decision if is used effectively for doing accurate approximation north of pattern matching as you go along show to baby neural networks is there any questions you have feature 1 and feature for multiplying together with my relationship too just good question so in steps you have your basic input features and when people are building something like a logistic regression model by hand say well something that's really important for classification is looking at the pair of feature for and feature set if both of those are true at the same time something that's referred to normally and steps as an interaction term and you can buy Han add interaction terms to your model so essentially a large part of the secret here is having these intermediate layers they can learn build interaction terms by themselves the search for higher-order terms that you want to put into your model yeah so he is the brief Lidl in Toulouse on a teeny bit more of in Opie which is sort of Us kind of problem we're going to look at for a moment so this is the Task of named entity recognition that I've very briefly mentioned last time if we have some ticks have some tea in all sorts of places people want to do is find the names of things that are mentioned normally as well as finding the names of things you'd actually like to classify them another not organisation some of them are people our places it has lots of uses people like to track mentions of companies and people and newspapers and things like that they do question answering that a lot of the time the answers to questions are what we call named into these the names of people locations organisation movie names all of those kind of things and named entity how to start building up a knowledge base automatically from a lot of tea all I want to do is get out these and get out relations between them so this is a common task go about doing that a common way of doing there is thus a well we going to go through the word I'm and they're going to be words that are in a context words of egg and what we're going to do is run a classifier and we going to assign them a class so we're going to say organisation second words organisation third word isn't a named into the fourth word as a person fifth word as a person and continue down so I'm running a classification of a word within a position in the Tech those got surrounding words around it to save what the entities are mini into these and multi-word term what thing you can imagine doing it's just they will take the sequence all classified the same and call that the indecision golfer there's a reason why that slightly defective and so what people often use that biao encoding that I show on the right but I'll just going to run ahead and not do that now termites seem at first that named entity recognition is trivial because you know you have company name book a company names and when you have UC Google say and how could you be wrong but I'm Texas or what of subtle the it's easy to be wrong and then dinty recognition so this is just some of the hard cases add to work out the boundaries something into the sun this Centre donate to vans the future school of Fort Smith presumably the name of a bank there but he said national bank and the first is just the first word of sentence which this catalyses you like the song food or something so kind of unclear what it is had nowhere the some things an entity at all so that the end of this sentence future school the name kind of 21st century school or is it just meaning it's a future school that's going to be built in this town it is at an interview or not at all fast of an entity is often difficult so to find out more about zigzags and read features by you know what car has big Z if you don't know I'm Sexy person's name the various entities that are ambiguous right so Charles Schwab and in text is 90% of the time and organisation name because this Charles Schwab brokerage and but in this particular sentence here in Woodside where Larry Ellison and Charles Schwab been with the Speedway and wood Estate is there a reference is Charles Schwab the person a fabulous understanding variously that's needed to get right do with that The Chase what we want to do is build classifiers for language inside a context in general is not very interesting classifier word outside of context we don't actually do that much and NLP it's interesting to do named entity recognition as one case the other place that comes up and here's a slightly cool one that there are some words that can mean what's the opposite at the same time right ascension something can either mean to allow something or it can mean to pawn to seed something can either mean to plant seeds and things so your seed in the soil or can take seeds out of something like a watermelon tonight you just need to know the that suggest the past that we can close the fire word in its words in Nea example that in the questions how might we do that and a very simple way to do it might be the same while we have a bunch which have a word vector from something like words of egg maybe we should just average those words vectors and then classify the resultant vector that doesn't work very well because she lose position information you don't actually know anymore which of those words vectors as the one that you're meant to be classifying ways to do better than that is to say well why don't we make a big Victor of a word window have a word vector and so the classify the middle word in a context the pie plus and minus two words with simply going to concatenate this 5-bit does together and say now we have a bigger vector and let's build a classifier vector so we classify Miss X window which is there a Victor in r5d if we using d-dimensional word vectors do that in the kind of way that we did previously play ok for that big vector we gonna learn wy can you put it through a soft Max classifier and then we going to do the decisions perfectly good way to do things what I want to get to in the last part of this is the start looking at my matrix calculus yeah we could use this model and do a classifier and learn the whites of it and indeed and for the handout on the website that we suggest you look at it does with a softmax classifier of precisely this kind but example doing class naked a bit simpler and I want to do this I think very quickly cos I'm fast running out of time so one of the famous early papers of Newall nop and was this paper by collarbone Western which was first and icml paper and 2008 which actually just a couple of weeks ago and one the icml 2018 test of time award more recent journal version of that 2011 they use this idea of window classification to assign classes like named into the words in context they did in a slightly different way so what they said is well we've got these windows man with the location named entity in the middle and this is one without a location into the in the middle and so what we want to do is have a system that Returns A Score and it should return a high score just as a real number in this case and it can should return a low score location name in the middle of the window in this case explicitly the model just returned the score and so if you had the top level of your new network a and you just send product with a Victor you you then kind of forgot final dot product you just return a real number that is the basis of a classifier so and full glory what you had is you had this window of words word vector for each word x you can catenated the word vectors for the window you multiply them by Matrix and edited by hidden layer which is a and then you multiply that by final Victor and that gave you a score for the window and you wanted the score to be large if it was a location and small if it wasn't a location so in this sort of pretend example when we have four dimensional word vectors and that's meaning you know for the window this is a 20 by 1 vector for calculating the next hidden layer we've done 8 by 20 matrix plus a buyer Spector then we've got this of 8 dimensional second hidden layer and then we computing a final real number this is an example of what the question was so bad we put in this extra layer here right we could have just said here's a word spectre a big word Victor of context let's just stick a soft NEXO with just a classification on tour play location by putting in that extra hidden layer precisely is extra hidden layer can calculate non-linear interactions between the input word vectors flight things like if the first word a word like Museum and the second and the second words the word like the preposition in or around then that's a very good signal that this should be at location in the middle position of the window extra layers for annual network let us calculate is kind of interaction terms between our basic features Primo slide to hear that so go through the details of their model but I'm going to just skip those for now cos I'm a little bit behind and at the end of it we've just got the score so this is our model which is the one that I just outlined where we calculating a score and we want a big score and for a location and so far we've done they want to do we can you learn our parameters and annual network titular remember it's the same story we had before we have a loss function J and wanting to work out the gradient with respect to our current phasor parameters of a loss function then we want to sort of subtract multiple of them play the wording rate from our current premises to get updated premises and if we repeatedly do that and stochastic gradient the same will have better and better parameters which give higher probability to the things that may actually observing in our training data sing we want to know is well in general how can we do this Asian and work out the gradient of a loss function I sort of wanted in this lecture go through how we can do that by hand using mass dentist generally the backpropagation algorithm for the next one if we doing I'm gradient by hand well we doing multivariable calculus multivariable derivative normally the most useful way to think about this matrix calculus which means with directly working with vectors and matrices to work out out radius normally sort of much faster and more convenient for summarising annual network layers than trying to do it in a non victimised way mean that's the only way to do it if you're solid confused about what's going on sometimes I think it through in the non detector riseway can be a better way to understand what's going on and make more progress so like when why did the word the Vic the river too small on that board sorry that was due here in a non Victor Rise way of working out the weights talking about them individually can I do it Tracy's and again look for the lecture notes to cover this material and more detail in particular so that no one misses said what I mean by lecture notes so if you look at the close to love column days the slides that you can download and and straight under the slide that says lecture now the lectern in the middle column it then has some readings and actually they're some different additional things there that cover similar material they might be helpful as well but first the thing that's closest to what I'm about to present it's the lecture notes for the Pier immediate land of the slides like My Hope here My Hope here is the follow if you can't remember to how to do single variable calculus sunken eyes will leave now I'm assuming you know how calculus and I'm assuming you know whether the matrix is but you know I hope that even if you never did multivariable calculus or you can't remember any of it it's sorted for what we have to do here not there hard and you can do what you do if we have a simple function if x = x cubed is gradient and to the gradient is the slope ride at saying how steep or shallow is the slope of something and then within in also saw the direction of smoke when we go into multiple dimensions its gradient has just it's the river that flows derivative is 3 x squared so if you at the point x = 3 that the 27 of sleepy is very steep so well what if we have a function with one output but now it has mini input ways of doing that sort of that was like the dot product where we doing the son of dowt the value well then what we going to calculate is a gradient which is a vector of partial derivatives with respect to eating pork so you take of the functions as you change X1 the slope of the function as you change x to through the slope of the and functions you changed explain and each of these you can just calculate as if you're doing single variable calculus when I picked up and that's them giving you the gradient and the gradient and multidimensional I'm giving you the direction and slope of the sort of a surface that touches your multi-dimensional if function area but it gets a little bit scarier than that because if we have a neural network layer and we then have a function which will have in import in output that's the case you then have a matrix of partial derivatives which is referred to as the Jacobi jacobian your son of taking these partial derivative respect to eat poured along the road put down the colon is MBA in partial derivative every combination of an output and an input you can fill in every cell of this might single variable calculus provide you don't get yourself confused we already saw when we're doing word to there central tool that we have to use to workout how to rid of something like a neural network model is we have a sequence of functions that we run at one after another so in the new network of sort of running a sequence of functions one after another so we have to use and the chain rule to work out derivative dysfunction one variable function so we have 3 y and Y equals X squared if we want to workout of Z with respect to x that's a composition of two functions the chain rule and so that means what I do is I multiply the derivative so I take that's too x set alarm is EZY sorry it's just free that's my best to do a bit of the top line and then divide play those together and I get the answer but the river dove 2 x 6 get slogan free KB it's true if you have flame multiply the jacobian when you get the right answer so if we now imagining a new net will sort of this is a typical pneumonia right so it doing net layer where we have a weight mate input vector yes and then we're putting it through and nonlinearity and then if you want to know what's the parcels of the XX we just say this is easy to do we work it up our first to Kobe and which is the pass rate of each respectively and then we just x the parcels and dizzy with respect to answer an example jacobian which is the server special case that comes up a lot so it's just good to realise this one see you at down your net so well have these element wise activation function so we have 8 = if of the what partial derivative of H with respect remember that we son of apply this element wise directly saying h y = fmci formally this function has in input port so it's partial derivatives there going to be an invite in jacobian what's happening there actually going to find a sort of when we working out the terms of this so we're working out how does f of Zi as you change play well sequel to I was going to make no difference at all out so if my f function is something like putting it through the logistic function or anything else absolute valuing a number it's going to make no difference to the calculation if it's the eye if I change the job because it's just not in the equation and so therefore the only terms that are actually going to occur and been on 0 other terms where i = j this partial derivatives of I does not equal j80 if I does he called jay then we have to work out as single variable calculus what's the derivative an activation function what to Kobe and looks like for an activation function it's a diagonal matrix 0 and we've got this activation function we work out it's derivative and then we calculate that for the difference Different Kind di value that's for an activation function what in the other main cases from your network threw a little bit more sly way in the same lecture notes kind of similar to what we saw in the very first class the work out the partial derivatives of WX + b with respect to x recap is w want to work out the partial derivatives of wxb with respect to be that's mean so we can identity love like a 1 b Rider salt are you just getting the ones coming out to preserve the bee this was the case saw doing the word vectors that if you have a vector dot product what's the partial derivatives of debt with respect to you then you get a house age transfers rainbows hand out and see if you can compute them in that makes believe those clothes to see how we can then work out derivatives and side in your network it's a new network with sore before so we have a window detectors will put it through a hidden layer and then we just doing a pic tomorrow the dot product get this final score and so far we want to do to be able to train and you'll network is we want to find out changes on all the parameters of the model the XX with W the be the you so we want to work out partial derivatives switch respect to each of those because name workout ok if you move be up better which is good if it sexy Paris in the middle and therefore your one and not chop and elements of be appropriately doing the gradient with respect to the school here and I skipped over those couple of slides stare at this picture and say will how do I work out the partial derivatives with respect to be probably doesn't look obvious things that you want to do is sort of break up in the simple pieces that composed together right so you have the input goes into zwx + b and then you can pose that with the next thing so 8 = fmc how activation function and then this 8 goes into the next thing of S = UT sequence of functions and pretty much you want to stop as much as you can I mean I could have broken this up even further I could have said the one = wa the oneplus be it turns out but if you've just got things added and subtracted you can sorted do that and one step because the store pathway separating the when doing the derivatives that sort of anything else that can poses together you want to pull it out for the PC non-unit is doing a sequel composition play ok we know how to do that the chain rule so if you want to work out the partial respect to be it's just going to be the river doves of each step along the way so it's going to be the parcel with respect to age x 8 with respect to the x the respect to be and that will give us the right answer all we have to do is actually compute that it's ok we taking the parcels with stepper that composition compute their and so this is where I'm going to sort of you phobias that I sort of a certain without much proof on the preceding slide ok so first of all we have dsda active two vectors being for that is just a transpose start then we have 8 = f of Z well that's the activation function Toby anaesthetic is this diagonal matrix made of the element wise and derivative of the function f and then we have 12d with respect to be and that's the bit that comes out of the identity Matrix giving us calculation of the parcel is with respect to b so we can see that the density matrix sort of goes away so we end up with this composition of HT x if pregnancy I suppose we don't want to go on and computer now the partial of it's with respect to w starting off point is exactly the same chain rule that we work out each of the stages that first of all working out the Z from the WX part then put it through the nonlinearity then doing the dot product of two vectors so that part is the same you should know that compare the parcel this with respect to w vs s with respect to B and another same and it's only the part at the end that's different make sense in terms of annual net write that annual net turn on the Bee were coming in here and once you start have done some stuff with them you're putting things through the same activation functions and doing the same dot I'm doing the same calculations that you're then composing instead you should be getting the same derivative partial derivatives to the current at that point actively these partial derivative the computations in the neural network that are bar br those are commonly referred to as a delta delta which is and so delta is referred to as the error signal and your network to work so it's the what's your calculating as the partial derivatives above does that you're working at the parcel delivery so a lot of the secret it will see next time a lot of the secret of bands with backpropagation is do efficient computation in the solid way that's computer science people like to do efficient computation and so precisely what we want and Moses is that there is one error signal that comes from above every want to computer one news at when calculating expect 2wn with b the son of two things to still do what is useful to know what the partial derivative actually looks like I mean is there a number of Vector and Matrix pincer and then we actually want to work out its value added value will have to work out the partial derivative with respect to w or we just try and work out it's sure Kinder sexiest of a bit tricky and his thought of a dirty on the belly of doing this kind of Matrix calculus weight vector is an inviting Matrix result it's with respect we as we have a function within x in import elements of w in simplyone output which is L score sound like according to what I said before we should have a one by in times in jacobian that's not really what we want right because what we wanted to do is you inside radiant to set days algorithm and if we're doing this like the hair Matrix subtract a b format to get a new white mate goodnight what about a co being with the same shape as w general what you always want to do with new Olney what we call the shape convention which is we gonna sort of represent tuco being starts in the same shape as the input this whole thing is kind of the bear pad part of doing matrix calculus like there's a lot of inconsistencies help people represent matrix calculus that in general he just go to different feels like I can understand physics some people use a numerator convention some people use at the nominated convention this shape convention to rematch the shape of the inputs that makes it easy to do await updates so monster look like in the final thing we need to do to workout and the parcel best with respect to w as we have the error signal delta that's going to be part of the answer and then we won't work out the partial of the with respect to w well it turns out Saved by the Bell here's some fun down to 2 minutes left it turns out that what we for that is we take the product of the parcel delta am x x effectively we got the local area above w and then we have the input eggs and we're working out and outer product of them way to think about this is sort of for the w the elements of the w m different connections between an urine one of these is connecting one output to one input and so we going to be sudden making this inviting m derivatives the going to be the I mirror signal for output x input and those goes give us the partial derivatives quickly my last one minute so reviews the shape I'm going to skip that ok so the teeny bit at the end but I mean I think most of the idea of how you can sort of use the chain rule and workout the derivative in terms of these viktor.and m serves an essentially what we want to do for backpropagation is the say how can we do at get a computer to do this automatically for us and to do it efficiently and that's what sort of the deep learning frameworks like tensorflow pytorch to and how you can do that will look at more next time 