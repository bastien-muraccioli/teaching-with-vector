turn to machine translation light show which is kind of like a combination of the sequence of 3 lakh shares on rnns and related topics you probably noticed when he came in with taking attendance today so you need to sign in with ETA is who outside your tutorial you missed it don't get up now it's fine I'll be time to signing up to that show and then if you have any kind of questions about special cases with the attendance policy you should check out the payout supposed to be put up last night with some clarification reminder for contents is going to be covered today to go have everything you need to do is leave it for again today and do get started early because the model takes 4 hours to train is that we're going to be sending out mid quarter feedback survey sometime in the next few days probably so please do you fill it out you'll get 0.5 percent credit and you're also going to help us to make the process of the rest of the Quarter what we're gonna do today play faster going to introduce a new task in an LP which is machine translation introduce a new neural architecture Accord sequence-to-sequence here is the machine translation is a major use case of sequence going to introduce a new neural technique called attention something that improves sequence-to-sequence alarm section 1 of this is going to be about a bit of machine translation history translation empty translating a sentence ex which we call the source language whatever language translated from why which is in another language which we call the target ex is this French sentence anyone in the audience of French speaker translate this English Russ free everywhere he is in the Iron ions that was a very literal translation is usually translated this Crowborough so is usually translate this man is born free but everywhere he is in Chains but you know there's an ambiguity should first chainsaw so you could choose to translate man or maybe humankind everything translation and there's already you know quite a few choices you can make machine translation is an AI task began in the early 1950s in particular there was a lot of work translate in Russian to English because the west was very interested in listening to what the Russians were saying during the Cold War got a fun video here which shows the state of machine translation in 1954 they haven't reckoned with ambiguity when they set out to use computers to translate languages video I think there's a number of interesting insane example of about how AI hype is nothing you even in 1954 they were talking this machine Translation system as if it was an electronic rain don't think over States really hard general it is play me some of them very optimistic that this machine Translation system was going to be replacing humans anytime soon so three interesting and citizens actually were mostly rule-based by which I mean they were mostly using a bilingual dictionary between Russian and English and they were essentially mostly just looking up the Russia words are looking at the English counterparts and they were storing these big bilingual dictionaries on his large magnetic tape suddenly it was a few technical feet at the time some people probably too optimistic about how quickly it will replace human forward several decades and time and I want to tell you about statistical machine translation statistical machine translation is the you're going to learn a probabilistic model from the data in order to do the translation example as before suppose that was translating from French to English you want to find the best English sentence y given the French sentence matter can you can formulate this finding out Max y of this conditional probability of y given x your learning is this probability distribution p we do is we break down this probability into a it's two components using bayes rule finding why vets maximizes probability of y given x is equivalent to finding the why that maximizes the probability of x given y x the probability of White components here on the left we have a translation mod track of how words and phrases should be translated where is it it knows how French words and English words might be translated to each other or maybe small small phrases and chunks of wood should be translated a lot of parallel data and I'll tell you later how would you that component Psy language ask model is a system that can predict the next word but it can also be thought of as a system that tells you the probability of a sequence of words translating from French to English PSY is an English language model the reason why we want to break down the single conditioner conditional probability distribution into the product of two different ones YouTube business logo instead of a single or conditional probability distribution unit understand how to translate and how to write good English sentence structure and everything up so that the translation model on the left blue mostly just knows about local translation of small chunks of words and phrases underwrites more take care of writing good English good sentence structure word order and someone do you know how to learn a language model could be learnt about that last modelling or data in this case English data any more about how you would learn this translation model that needs to be learnt from parallel data we need a large amount of parallel data in Autos Lane this translation example of a parallel corpus is there a Dr stone stone that has the same text written in 3 different languages this is a hugely important artefacts for trying to understand ancient century scholars discovered the stone and it helped them to figure out ancient Egyptian because that was as follows are the same same texts in other languages that they did know a really important parallel Cork weather in London you can go to the British Museum and see this is that you get your Carlow data obviously you need a larger amount than the stone and hopefully should be written on a stone either but you can use this to learn your statistical machine translation model you're trying to learn this conditional probability distribution of x given y is the actually break this down even further consider the probability of x and a given y the alignment alignment is this is a how the words in the English sentence and the French sentence correspond to each other temperature this fine example in example where we translating the sentence Japan shaken by Tunis quakes to French there's a pretty simple one-to-one alignment here French words and also pay in the exact same order doesn't conform to that is the word in French which we called a serious word because it doesn't have a direct counterparts in English Japan for the French Lycee a bit more complicated than that alignment can be many to one you have a words that have multiple English words that correspond to them call many-to-one alive turn the other direction to alignment can B12 men add a single English word implemented which has a one-to-many alignment cos there's a three word French phrase that corresponds to it that's on the right we have two ways of depicting the same alignments is either it can be a graph play Example too many call elders word implemented that is wanted retire words that has many children in the in the target Centre there's someone for two very fertile sample web ESO sentence alarm on tati means he hit me with a pie French this verb means to hit someone with this word has no single word equivalent in English we don't have a single verb that means to hit someone with a pie find the franchise the words you wonder maybe they do it so often that they need a single word that I don't know example of a photo because I'm used to have several corresponding English words to translate it once many and many someone you can also have many to many alignments you recall that kind of phrase level translation of phrase to phrase the does the town have any money how many money corresponds to the French phrase many-to-many alignments because there's no obvious way to break down this phase to phase alignment into smaller words word alignment is thinking about how would you learn this probability distribution of what the alignment is in order to do statistical machine translation what is the you learn probability of x and a given y nation of many factors or many features instead of for example what's the probability of a particular word aligning to another particular word like this English word of this French word health and they'll depends on for example what's deposition in the sea it's a both a pain in the end of the sentences then it's more likely that they're line where is it one of the Beginning wanted the end that's left instead of things like what's the probability of this particular French word having this particular facility like what's the probability of this word having three corresponding English words and someone statistics alone from your paradise you're taking play the kind of overview of statistical machine translation today and not going to understand it in full detail understanding an overview about works because we're going to be your machine trance snt system and so far we've broken it down into these two main components we've got the translation model and we've got the language tomorrow we understand a little bit about how you might learn this translation or by breaking it down into Alliance how do you do the Old bags over why your French sentence y that maximizes this probability can you put four solution is you could say let's enumerate every possible why that's kind of every possible sequence of French words maybe up to something calculate this probability for love yeah that's just a no go that's way too expensive and we're not going to be able to get anywhere with that how you actually this in practice is your you want to use some kind of heuristic search algorithm to search for the best translation why are you going to discard hypotheses that are to low probability search you going to discuss did you go to make sure you're not keeping too many I got play this process of finding your best sequence is also called decoding review of how that works for smt example of where you have this German sentence that translates to he does not go home as you can see the Dozen kind of prayers to praise alignments here an overview of how this decoding with work in smt you kind of consider the different type of translate these individual words can you build it up vol I translate visual phrases and Afraid of getting bigger what you can see that on the top right of the last few small you can see that the the German word for house could be translated into the English word house or home or chamber and so on so we can say that all of these different hypotheses and look into how we might put those together to translate phrases but you don't keep all of them all the time you get rid of the ones in the tula probably take me to picked it as a kind of tree where you are exploring different options you're searching through the space of options but then you prune the tree as you switch the very very high level description County code in my work and in fact later his lecture you going to see a details Calvert county county works for neural machine translation so what's are overview of statistical machine translation with a vector he was a huge research field from the 1990s to about maybe 2013 present during this time were extremely complex there extremely sophisticated and impressive systems and smt made the best machine translation systems in the world very complex so for example you know there were hundreds of important details that we haven't mentioned here at all there were many many techniques to make it more complex and more sophisticated than what I described today the sisters had to have many separately design are you a breakdown be translation model into two separate parts mini moto components head office engineers had to do a lot of feature engineering in features to capture the particular language phenomena that you're interested in do they add to require a lot of compiling and maintaining of extra resource different resources for different languages so the work kind of multiplied the more languages you had this is he had to have a tables of equivalent prices so for example if you do in French and English translation then they would be collecting these phrases of these tables of phrases of the considered for the day so but this was a lot of information had to be stored and maintained this is just a lot of human effort to maintain she had to put more human after 10 if you wanted to learn and that sentences questions about smt so moving on and I'm going to be going to section 2 of this action back to the Year 2040 reenactment of what happens in the world of machine translation research sing something rotomatic happens and that thing that happened is called neural machine translation and I think it looks a little bit like this if I'm not being too dramatic what is the emission charge is the energy is a way to do machine translation but using just a single neural network network architecture that they use is called sequence-to-sequence or somebody just called 62c revolves to arnotts it's cool sequence-to-sequence because your mapping one sequence to the the source sentence to the target Centre basically to handle this two different Centre the diagram to see what sequence of sequences in D cell stuff with us so sentence and we're going to use our example from before one party which means hear me with the pie FIFA sin2a encoder arnon and this is as you seen before I've drawn a unidirectional rnn but this could be bi-directional it also could be multilayer it could be another could be an ATM and someone thing to know is that we are passing word embeddings into this and code rnn but I'm just not explicitly depicting that the encoder or Nan is that it's going to produce some kind of encoding of this source sentence let's assume that the encoding of the sentence is going to be the final hidden state of this in code Orange text is we passed this encoding of the sentence we passed it over to the decoder r&m which is going to translate into English when is a language model in particular it's a conditional language tomorrow we talked about last time no because it's going to produce the target sentence but conditions on this and code is that better than the Orange Box we start off by feeding at the stop token into the decoder we can get the first date of the decoder because we using the encoding of the source sentence as the initial hidden office output from the decoder which is the probability distribution of what word Mica play some old cracks over the words because that's probably the word you should start with let me just take the way he and then we feed it back into the decoder on the next I'm getting the word and get heated you can continue doing this operation and in that way you're going to Jenna target sentence will be something like he hit me with the pipe stop once your decoder produces the end what is it here this picture is showing you what happens at test time they shows you how to generate what happens during training I'll show you what happened to you in training later but this thing with the pink dotted Arrows when you feed the word back in this is what you do to generate tax test another thing I should know it is that you need two separate sets of word embeddings right you need word embeddings for French words meaning English word embeddings that's kind of two separate sets to separate vocabularies this optical sequence-to-sequence is actually pretty versatile it's not just a machine translation of text quite a few nop tasks as sequence-to-sequence pass sample at some addition is a sequence-to-sequence task because in goes your long text and outcomes your shop P60 because in goes the previous utterance and outcomes your next lottery can even be thought of as a sequence to sequence task because you could say in goes the input text and the output cars going to be expressed as a depositing but it is a way you can try do something like code generation so suppose you want to build a system that takes some kind of natural language input structures sum up the numbers from 1 to 10 and outputs that's a sum Python code that says open brackets Grangetown or something like that wanted to train you could in away view that as a translation task why you're translating from English to translation task that probably requires a lot more logic than just a French to English but you can try and people have tries there are research papers where people have used c2c is an example of a conditional language how much model because the decoder is a language model that's predicting the next target word additional language model because it's also conditioning on your source sentence which is represented by the encoding of the saw Centre you can do it like this directly calculating the probability of the target sentence y given the source sentence x you see that this is just breaking down the probability of the sequence y which was supposedly a capital the probability of the first word of y given x and then the probability of a s y given the words that came before and see the each of the terms in this product on the abilities of the next target word given all the one so far and also the conditional probability that your language model produces highlighting this is because if you remember in smt we didn't directly learn the translation model p of why give into you in an auntie we are directly learning this poison advantage because it's simple to do you don't have to learn all these different systems of optimise them easier this is this is all that word is how do we train you should already have a good idea of how this would work give them with red parallel corpus and then let's say you have your sentence pair from your parallel corpus so this is what I was doing cancel sentence in Iran and then you feed your target sentence into the decode arnon and you're going to pass over that final you can say to be the initial hidden for every step of the decoder Dornan you going to produce the probability distribution of what comes next which is the White House from those you can complete your loss what's the same as we saw for unconditional language models it's the us a negative log likelihood of the true next word put on those selected ones the loss is the negative log probability of the correct thought we're going to average all of these losses to get the total loss for an example I think you might notice people saying in for example research papers is this phrase end-to-end an example of learning a system end-to-end my guess is that the backpropagation is happening and send one end is is losses the loss functions and the other end I guess it's kind of like the the beginning of the encoder you are back propagation flows throughout the entire system and you learn the entire system with respect to this single can you text the question is if the decoder rnn outputs the end token too early than how can you measure the loss words that came off the difference between training time and test time which is pretty confirm during training we have this picture when you feed back in so in this scenario won't you produce and then you have to stop because you can't feed ends in the in-between next you don't feed the thing that you produced into the next step meaning you feed the target sentence from the corpus so like the gold target the model so no matter what's the projects on a step you kind of you don't use that anything other than navigate to ent training model and then together is is there a reason why you would want to train end-to-end when for example you might want to train the encoder peoplevue training end-to-end and favorable because the ideas that you can optimise the system as a whole you might think that if you optimise the parts separately then when you put them together they will not be optimal together necessarily so if possible directly optimising the things you care about about with respect to all the premises is more likely six there is an ocean of preacher maybe you want to learn your decoder rnn as a kite unconditional language people do you might learn a very strong language model and then use that to initialise your decoder rnn and then fine tuning play garden for the question is is the length of the sentence and the length of the target sentence fixed so for example is as always like for know that Stephanie not true because in your parallel corpus going to have sentences of all what kind of an implementation or practicality question the idea is that this is what you mathematically want to be computing during training for each example and you can have batches of vehicle can you add the internet in please me do just because it's easier to assume that your badge is this kind of even sized 10 so everything is the same length is UPAD any short sentences up to some predefined maximum length for maybe the length of the maximum example in your back you make sure that you don't use any didn't save the game from the pad together universal you wouldn't want to train things and under no circumstances in which you might want to train think separately and you mentioned for example having a different languages map to each other this is a totally valid point and in fact so far we kind of assumed that you want to learn languages and language be as a pair and that's different languages language c or even language that does mean you have kind of n squared many systems in the number of languages considering so yeah that's actually a valid idea and this is something that people have researched the idea that maybe you could have a kind of mix and match with your encoders and decoders and you could try to train a kind of general-purpose let's say English decoder and then Match Attax with your difference and Co this is I think I did contact the training to make sure that they all work together but that is certainly something that people done the time one more question what's the meaning of the come from the same purpose that you're training on the question is does the word embedding also come from the corporate training on so I think there's a few options just to say would language models you could download pre-trained word vectors like where to buy Coke Anarchy that kind of freeze them or you could find them as part of the end-to-end training or you could just initialiser word axis as you know close to there around them and then London bus crash play movie play now understand how you would train and your old machine Translation system and we talk briefly about how you might do decoding generator for something called greedy decoding which is this idea that unused app you just use the og Max the top 1 best words and then you feed that then on the next step decoding because you're just taking the but you can see right now and then you don't have a way to go back see a problem with this method you said to expense is expensive in that you have to do a sequence and a sequence of Duty worth something in parallel but I suppose maybe what's wrong with the greediness and they want to just what's wrong with the green that's kind of what ingredients that's my give you something like this we trying to translate already example sentence and then expose on the first episode he and then he say he hit and then he say he had a go no that was a ride that wasn't the best thing to choose but we can't have no way to go back now alright we just have to continue and try to make the best of it after saying he hit a which isn't going to work out well ambergris decoding is kind of no way to Backtrack no way to go back how can we back to you when I told you earlier about how we might use a a kind of searching algorithm to decoding and smt the fact you write is it going to do more probably not because still a bad idea for the same reasons as before switch off to search and search through the face of all possible French translation would be again trying to consider which y maximizes this product of all of these individual probabilities 4 if you try to do this each structure of the decoder you're going to be happy to track views the power of tea possible partial translation is your vocabulary size play Potter translation I just need kind of you know like something like that pause this excellent role in V complexity is just watching is it going to use some kind of search how to use a beam search decoding idea of beam search decoding is that on each step of the decoder track of the k most probably translation type of kind of track in multiple of them and we're not sure which one is Best so thinking about several he is a integer and record this the beam size and practice for an empty this is usually maybe 5 to 10 think of a kind of is how big is your search space of anyone play the new going to be considering more different options on each step and you might hope this for me and you guess the best quality solution in the we want to keep track of the k most probably are you some kind of notion of how probable is this hypothesis or whatsits School Corby hypothesis and we represented as y1 up to yt is just it's log probability probability of this partial translation according to the language model can be broken down as he stole before into the sum of the individual of probabilities of the word you came before is not obvious these scores are all negative because they're taking longer and score is better do you want a higher probability of according to the is the Wigan to use the score and the search algorithm to search for high scoring are Pisces and we're going to track the top k when you said are you at the moment but the important things to know are the beam search is not guaranteed to find optimal search the one where unimate enumerate obesity possible translations that is guaranteed to find the optimal solution but it's just convenient feasible because it's expense search is not guaranteed to find the optimal solution but it is much more example of beam search decoding in is the beam size = k is 2 set a reminder we have the score that you reply to a partial iOS translation start off with a starting token is that we're going to compute the probability distribution of what word might come next computer that probability distribution using r68 model then we just take the top k that is top 2 possible talk to you with the word he and I we can compute the score of these two hypotheses by using the formula of log probability of this word given the contact so far say that he has a score of -0.7 and I had a score of -0 the better one are we doing so we have our 2ork hypotheses which of those the top k words that could come now they both he and I we find a top two words I can come next please for possibilities the score of the hypothesis is equal to the log probability of this new word given the context so far plus the score so far because you can accumulate this some of love probability you don't have to computer from scratch see that we have these four possibilities and that the top 2 scores are - 1.6 and -1 it's and was are the two best ones is it all these k squared = 4 hypotheses we just going to keep the cake equals to top keep doing the same expand to get the Two Necks we can keep the score 2 best ones the others and then of those we expand again again expanding and then just keeping the top k and expanding until get some kind of Finnish translation I'm going to tell you more in a moment about what exactly the stopping criterion Derby stop here get the four hypotheses that we have on the far-right the one with the top score is the top Pi one with what's the weather going to stop now we decide that this is a top is just back track through the story in order to find the fort give me with a pie play let me tell you more detail about how exactly was side when to stop greedy cow reading usually we just keep decoding and tell the model produces the end number this means the model is actually producing the sequence could you start new give it starts with any produces the sequence he hit me with a pi pence for considering all these different type of CK different I got season 1 is there a type of disease might produce and tokens at different turn off Facebook shop practice is when hypothesis produces the end soken then we regard this hypothesis is we have a collection of computer type of we no longer keep exploring decide can you exploring other processes within search which train is when do you stop doing such a 20 stop iterating through the sower multiple possible stopping criterions but two common ones are you might say we're going to stop doing beam search once we reached I'm septi where t is some predefined threshold we're going to stop being searched after 36 because we don't want any output sentences for the longer than 30 we're going to stop doing being search once we've collected at least end completed hypothesis I want at least 10 complete translations before I stopped doing research play what's the final thing Homesearch and we have this collection of completed Argos how to choose the top one the one that we're going to use as a translator how do we smoke the top 1 that has the highest score is a simple given that all of these hypotheses already have schools attached but if we just look at this formula again what the score is abuse hypothesis anyone see a problem with this set up opposite which using the top 1 based on the one who has the best score can anyone yes so the answer words you can end up shooting with shortest one hypotheses have lower scores in Germany because you're multiplying more probabilities and getting a smaller and smaller overall value-adding love from the 80s and get some more negative value so it's not quite that you're definitely choose the shortest life artists because you could overall have a lower school but there's definitely going to be a bias towards shorter cancellations so they can fix this is pretty simple you just normal Eyes by legs so instead of using the above you're going to use the score / something sister Fletcher top 1 so that it is possible to you about your question because that I'll point is your relying on your language model your decoder to produce the end token in order to know when to stop so you need to train it would you see an token by give me examples for training sentences with and depression is why it uses normalised scored the one at the bottom of the screen during the insertion the first that's not Siri you could but it's not necessary it's because during been such we only ever compare the scores if I policies that have the same length right so need to be steps so when we look at like say the top k squared and you want to choose which ones are the top-k we comparing the schools of 14 hypotheses that are blanks 12345 it's true that the scores are getting lower and lower but in the same way because I will like five how you would training energy system and how would you would use your trains and auntie system to generate your translations using leads a team search take a step back and think about what are the overall advantages of an empty in comparison to smt the first advantage is just better perform listens tend to give better I'll put the Nessun t-systems in helping oftentimes to be more flu because nmt this is particularly good at learning language models as you learn play the better is they often used the contacts better that is there better at conditioning on the source sentence and using at to change the oven better is they often more able to generalise what they learn about phrases and how to transfer example of how to translate a certain sorceries and then later it sees a slightly different version of that sort phrase it's more able to generalise what it learnt about the first phrase then FM advantage of energy systems compared to snt that we talked about before is it a single neural network that can be optimised enter largest here I suppose as primarily simplicity and convenience comparison used to be individually optimised advantage is that it requires much less human engineering tell you about all the different things so people had to to to build systems there is relatively less engineering effort but it's less complicated play there's no future engineer define what features of linguistic phenomena sequence of words although there are different views on see a great thing about an empty is that you can use pretty much the same method for all language pack build your first English Translation system and he want to build the Spanish English one you can probably use basically the same architecture in the same method as long as you can go find a big enough parallel corpus in Spanish disadvantages of Mt remaining 70 there are some disadvantages nnt is less interpret what I mean by this is you feeding your sentences in your network and then it feeds out some target sentence and you really have any way to figure out why that the target sentence contains some kind of error you can't really look at the Neurons and understand what happened it's pretty hard to attributeerror energy systems smt systems were more interpretable in that you had all of these different subcomponents that were doing different you are more able to look at those they won't you know your answer probabilities of certain words give another word to someone and you know that's fine no means easy to interpret but it was at least more disadvantage of energy is pretty difficult to control example if your energy system is doing a particular error it's not very easy for you the programmer to specify some kind of Rule or guideline that you want en anti-system 24 how to say I want to always translate this word in this way when when this other things presents like that's not particularly easy to is there a rule on the energy system can't easily control step-by-step basic so somebody has some kind of post processing rules in my try to do but overall you can't how do you expect to try to impose a Feliz he has some kind of safety concerns say you don't want 20 system to say bad things right it is pretty hard to actually put these controls in place to stop it from saying these things you wanted to say I mean on the level of maybe just never saying particular bad words and sure you can remove them from the vocabulary but overall they're pretty hard control and we actually going to see some examples with an empty systems being you know doing things for their designers so how to evaluate then I'll be talking you to have it automatic metrics so that we can measure our progress most commonly used evaluation metrics for empty is called Blue and that stands for bilingual evaluation understudy who is going to compare the translation that's produced by your machine Translation system it's going to compare that to 1 or maybe several human written translations of the same sent computer similarity score n g precision I mean you gonna look at all the 1 2 3 and 4 g of the pier in your machine written translation and you're human written translation and then n g precision is basically times for the page in the machine written translation how many of those appeared in you know at least one of the human is AdBlue is a brevity sing you get a lower please score if your system translation is significantly shorter than older you need to add the precision alone doesn't really punish using a fuel maximise m g precision by being breakers short sentences that only contain words you're really sure about and then you'll get a good precision translation cos you're probably missing a bunch of information that you need to translate from the source add a property penalty blue is because we need an automatic metric in order to measure progress you can't measure progress on human evaluation alone because it takes too long to compare Christine perfect sample are there many ways many valid ways translate sentence at the very beginning of the lecture I asked how to re translate that sentence by reserve and they released a few different options that came up so if there's many valid ways to translate a sentence how does Blue recognise sentences that have a high-end overlap with 01 or some of the human written Translate you write one of your model right one about the translation and the humans a different Gaelic translation and they don't have high and G overlap them blue it's going to give you a lower about blue and he shouts in the Simon for and infanticide before has a full description mathematical description of what the news is about that now everything about blue and the ways in which it's in perfect Graham e161ba 121 equivalency I'm not trying to question you asked me about alignment or something else 34 doing all my doing like window science of 1 make a difference because you can't permuted one because you're asking for example and they checking whether this exact sequence of four pairs or any permutation of it is exactly so by definition and G are sequences where the automatic that's how you evaluate machine translation so now I can understand this metric power we evaluate our progress on machine translation I can show you the scruffy might understand the one that means so this is a pop-up which shows in a nutshell how nnt change the machine translation landscape in just appears so in this plot we've got blue score is the y-axis and you have two different types of s&t which is the red and the dog blue Bar Plus happening is that in 2015 new LMC enters the team for the first time and it isn't doing as well as I sent and then the next year it's only outperforming FM he's a blue scores on some particular fixed dataset shared task that many people were submitting systems for the main thing is that the progress that was being made by s&t systems was you know fairly gentle increase in blue yeah but yeah and then in just one year and Mt arrives and it's only doing much more rapid progress so I think there's just why the picture of the meteor maybe isn't to Jurassic here turn back or 10mt the biggest accessory of NLP and Heathland if you think about the history of this being a fringe research activity in 2014 to be an actually the leading standard method for machine translation in the wild in 2016 in 2014 the first 60 papers published and in 2016 Google Translate switches from smt to nmt remarkable turnaround for just 2 years was amazing no just because it was a quick turnaround would also if you think about the love of human effort involved system for example the Google Translate as a system with built by Dallas hundreds of engineers over many years this isn't the same without phones by an energy system there was trained by a few like a handful of engineers in a human diminishing had difficult it is to build an energy systems and certainly I'm sure Google's an empty System today is built by more than a handful of engineers and a few months I'm sure it's a very big operation now but when and he began to shout promise and tea it was pretty remarkable how was able to do that based on the amount of deaths there are some disadvantages of an empty even in comparison to SMP is there any work on combining two there is there's a lot of an empty research on going and I'm sick of people sometimes focus on these particular shortcut there's a lot of work in kind of taking techniques and ideas and wisdom from the many decades of s&t research and integrating them into the new nmt paradigm so yes ok is machine translation salt can we go home I think the answer is clearly now NMC definitely is not doing machine translation perfectly so just to highlight some of the difficulties they remain with an auntie one is out of vocabulary words is the kind of basic problem but it's pretty tricky you know what you do if you're trying to translate a sentence that contains a word that is not in your source vocabulary or if you're trying to produce a word that's not in your target vocabulary they suddenly been lots of work on doing this and you're going to hear later in the class how you might try to attack this with regards Paul subword modelling can make it easier but this is this is is domain mismatch so let's suppose that you train your machine Translation system on a bunch of fairly formal text Wikipedia something like try to deploy it to translate informal taxlite people chatting on Twitter or something then often you'll find it doesn't perform very well on this different domain because you got at the moment that's quite a problem is maintaining contacts over longer talked about so far has a seem to you just translating a single sentence to a single sentence and there's no other wider context if you want to use a machine Translation system to translate a whole news article or maybe even a book then you're probably going to want to use the contacts that came in previous sentences in order to translate things correctly in the current Centre this is an active area of research how can you get sanity system condition on Logic is the contacts without it becoming too expensive and so on low resource language everything we talked about so far as assumed you have access to a very large parallel corpus that one if you don't want if you trying to translate to or from a language that has relatively little text available online free samples of machine translation screwing up with specific example of how common sense is really difficult for an empty system have the English phrase paper jam which means when your printer gets jammed up with paper and it's all the rights we have a very literal translation of that into Spanish and it's only saying jam edible jam made of paper which clearly isn't the right interpretation we have an anti system that's just doing very little conversation and clearly doesn't have any notion of common sense you can't make jam from paper example pick up bias in the training data we already talked about this at the the representation of words but it can also be a problem at by translating things example on the left we have to sentences in Malay they works enough and playwork the programmer the point is on the left there is no information about gender in the prison translate to English then we certainly got gender coming out of nowhere she works and ass and he works as a program happening because you're not training day so we had more examples of female nurses and male programme understand why from machine learning objective point of view the English language model has learnt to do that this isn't good machine translation here the system is making up information that was not present in the saw Centre assassinate an error for the machine translation should be doing cos it's just simply inaka propagating generals pretty weird example on the last we have that's just kind of a syllable repeaters and with hollesley translating Somali translate this into English and then we getting this out of Noah as the name of the Lord was written in the Hebrew language it was written language of the nation and you might be thinking where on earth did that come from and infected got reported in the media as you know if Google translate once to convert you to its religion or whatever so for sure it is very startling but the thing is actually quite turn on here is that the resort languages such as for example Somali one of the best resources of parallel texts is the Bible in English using the Bible as a training text maybe among other text puzzle pieces see nonsensical imprint so when the infant isn't really Somali or any kind of tax rate is just the same syllable over and over system doesn't really have anything sensible to condition on its basically nonsense is just do I can't really use the current condition on the source sentence and what it does is it just uses the English language model right you can think of it as like the English language model the decoder are and then just kind of goes into autopilot and start generating random last week when we saw a language model trains on Obama speech until Harry Potter we're just generate texting that style that's kind of what's happening here with the Bible because we don't have any useful information from the sentence on that so my new machine translation in particular make this kind of error system is an integrable so you don't know this is going to happen until it happens and I had to Google don't know this was going to happen until it happens and I got report the one Downside of on interpretability is that really weird effects can happen and you don't think coming and it's not always even needing to explain why they Irish translator Irish support with Google translate what language is more like I said if you did put Irish to English there's probably more training data for Irish to English so maybe won't be so Bible Focus yeah I know there's a lot of examples of things online when you do different kinds of nonsense syllables in different languages challenges remaining in an empty the research continues so nmtui think remains one of the flagship tasks Tettenhall Peter planning NMC research has pioneered many of the successful innovations of nipt planning in general today in 2019 the research continues to thrive there's so many many papers published all the time on Mt researchers have found lots of improvements to the valley vanilla 60 miles auction you today in fact there is one improvement that is so integral to see to seek that you could regarded as an even though about today and is called potential what is attention motivate why we need to think one attention this diagram that we saw before of sequence in sequence and remember when we assumed that this encoding of the source sentence the the one in the orange box is going to represent the whole sentence volunteer a problem you can see with this architecture after problem with this idea that single vector is the encoding of the source sentence it doesn't your own emoji on word and not working out calculate you don't know what's going on something like looking at one word you mean like a sentence and you're not being more information it's something like that and the other day have a kind of informational bottle sing all of the information about the sentence to be captured in the single vector because that's the only thing that gets given to the decoder if some information about saw sentences and their back garden there's no way that he could have going to be able to translate so quickly informational bottleneck kind of too much pressure on the single Factor to be a good representation of the encoder this is the motivation for attention and it provides a solution to the bottom and problem is it on Easter for the decoder you're going to use a direct connection to the encoder to focus on a particular part of the source sequence first I'm going to show you what is engineers via diagrams that kind of intuitive explanation and then I'm going to try the equation later sequence to sequence of the tension works the first ever body colon office sticker didn't said is we take the dog parlours between that decoder and States and the first encoder inside get something on the tension school which Are Everything by a dot that's a scalar between the decoder and stage and all of the encoder hidden state we get one attention score one scalar for each of these sorts words effectively what we do is we take this phone numbers scores and we apply the software distribution function to them probability description change represent a probability distribution as a bar chart for the attention distribution and this one sums up to 1 you can see that most of the probability mass is on the first word that password isn't she means he and her we're going to be producing work he first in our target sentence got this attention distribution going to use it to produce something for the attention output the attention helpers is a weighted sum of the encoder hidden States and the waiting is the girl from the tension distribution state pension output probably there should be dotted H also fully code are and that's part of the pic is that your summing up using kodi are an end hidden States but you're going to wait each one according to how much attention distribution it has on it is the attention output which is a single vector is going to be mostly containing information from the hidden states that had high attention in this case is going to be mostly information from the first insight do you do this you're going to use the attention outputs to influence your prediction of the next one do if you can catch me the attention output with your decoder Dynasty I use that kind of concatenated pair in the way you would have used Anika didn't state alone before you can get your probability distribution whitehat what's coming next we can use at the sample in x word on the next step you just do the same thing again you got your second decoded states again you take top product with all of the encoder in States you take stuff back over that to get the attention distribution and here you can see the attention to Street in ASDA attention on the word on top because you're about to produce the word sending a little bit to the second one because that's telling us that it is past tense everything is happening here is we getting a softer light call me look setalignment and smt Systems it was mostly this hard binary thing with all these words are aligned or they're not how much more flexible which word has a distribution over the corresponding words in the sentence to know kind of this I don't is that sometimes we take the attention output from the previous sentences kind of feed it into the decoder again along with the usual word that would mean you take the attention output from the first step and kind of company steps of the word out of the key and then use it in the decoder for this is sometimes useful to have this information from the attention on the previous tab on the next step so I'm telling you this because there's something with you inside before and it is very common technique but also sometimes people don't is do this attention going to be turned into different thing people on this side step we look at an apostrophe which means me when we produce me and then on the last through the probably most just give me looking at this fertile word on Thursday what a time here are the equations to describe attention I think it's probably easier to look at using your own time later rather than look at them for now but these equations did essentially say the same thing as what the diagram to said encoder in States 8180m 1 x 30 of the decoder wheel to have a decoder in States at St get the attention scores which you can put UT by taking the dot product of your decode and stay with each of the encoder in space you are a vector of same length as the encoder sentence if you got one school per softmax over the scores to get attention distribution that sounds up to 1 is Alpha to take a weighted sum of the encoder in States and that gives you your attention out is a vector that's the same as your anchor insides you take your attention output a and then you can sit with your decode in state seed with that as you would have before in the no attention model if it's ok is pretty cool as a number or before search is there attention just improved an MP it's because it turns out it's a peaceful to allow the decoder to focus on certain parts of the source sentence when is transmit this makes sense because there's a very natural motion of alignment and if you can focus on the specific word or words that you're translating you can probably do a better job my tension is cool is this sing that the problem with having a single vector that has to represent the entire source sentence and that's the only way information can passed from encoder-decoder means that if that encoding the graveyard then you're not going to do with attention in the decoder can look directly at the encoder and the sentence and translate without the bottleneck attention is it helps with the vanishing gradient problem especially if your sentences are quite long the reason why tension helps is because you have visa Direct connections between the decoder and encoder kind of over many time steps so it's like a short connection and just as we learnt last time about skip connections being useful for reducing vanishing gradients here it's the same notion we have these long-distance error connections that help the gradient flow faster to provide to interpreter look at the attention distribution after you've produced your translation are you can see what the decoder was focusing on it translator are running example here then we can produce a plot kind of like this that shows the attention distribution larkmead high attention and white means no attention so you might see something like this where and it was it was focusing on the different words in different the same kind of took the we had earlier with the hard notion of alignments in snt except the we are we have more flexibility to have a more software version when we produce the English word hits how to remove looking it on top but we also looking a little bit this evening alignment play for free is because when you remember the snt systems the whole point there is that you had to learn and alignment system deliberately and set define the notion of the line that you had to find the model of calculating what the probability of different alignments were in train we never told the nmt system about alignments we never explicitly training alignment system we never had a loss function that tells you how good your alignment ENT system the apparatus to do something like a lion maximize the cross entropy loss for doing machine do I just learn to lie coolest thing about the tension is that it's learn from structure in a summer unsupervised minutes I'm just going to generalise the notion of a 10 general that you can apply in lots of different circumstances play attention is a great way to improve the sequence to sequence model for empty attention of other architectures that on C2C and also tasks that aren't empty what's Santa's I'm going to the somewhat Redefined attention to a more general definition channel definition have a set of values YouTube witches of x single vector which recording the query how much is a way at computer weighted sum of the values weighted is dependence on the Latin phrases a saying that the query is attending to the values the idea by having this information that in the values and the query is somehow determining how it's going to pay attention to the values call in sick decoding state is the query the didn't sign on a particular time step is the query to all the encoder getting States which are the value definition of kind of understand this intuitively to alternative ways a bit like this as the weight of some is like a selective summary of the information in the values because your choice of how much you choose to draw from each value depends on the tension distribution the distribution depends on the query the query is determining how much you're going to select from different value kind of similar to lcmc roundabout earlier this based on the idea of a Gates that the defines how much information should come from different date depends on the context so the strength ability and came from the idea that based on the context you decide where you're going to draw information the same idea I think about the tension is you could say that it's a way to obtain a fixed size representation from an arbitrary set of Epson take and say they have the set of vectors called the values right and you could have 10 values 100 years you could have any arbitrary number of inspector gives you a way to get a single vector summary of that which is the attention out the last thing or is there there's actually several can you go to the capital in the sign of four the normal general setting with scenery have some values in the query always involves Computing the attention scores Airsoft next to get your attention distribution can you set a tension distribution to take a weighted sum who is the outline of our country work different is this number one motorways you can compute the scores ways you can repeat the scores how much of anything today is basic dot product attention is it the score for a particular a particular value HR products of the query and that particular value this assumes that be size of your query better and the size of your body Becker's has to be the same because it's taking forever what version of attention is called multiple of 10 is the score of your value HR linear functions of your query and that putting a weight matrix in the middle you're learning the best way to make weight matrix in order to get the score additive attention here is that the score of the value HY you get it by A linear transformation to both the value and the query and then you add them together turn on the RC like Tanner are you take a picture and you take dot product with a weight vector to give you a single number that is the school got two different weight matrices and also awake back so which other than a promise different here is that there's kind of an additional hyperparameter which is the attention dimensionality the I think it's the height of the W1 and W2 in a selector I can choose what size that dimension is is kind of like a hidden layer in the computation you can decide how big you want that intermediate reputation to be that because that's actually one of the questions in the assignment assignment is to think about the relative advantages and disadvantages of these models the summary of today at the last fight second last last time but this is the last night so we learnt about the issue then fee 2014 new LMC revolutionise ent turn the sequence is the right after tenancy and their uses to our names attention is a way to focus on particular parts of the 