taleban start again festival let me just say a bit about assignment 5 time at 5 is coming out today it's a brand new assignment so you guys for the guinea pigs for that and so far it's going to be the centrally billed son assignment for so it's ok if you don't do perfectly the time at 4 that I think actually most people did then what we going to be doing is heading convolutional neural networks and subword modelling to the neural machine Translation system seeking it to make it better assignment is coding heavily written questions like so I mean the coding that you have to do so that is an actually really more difficult than assignment for it's kind of like a silent for that's what we're hoping is the distance time you will be able to do it on your own by that I mean by that is for assignment for well there was tons of scaffolding telling you what everything should be and they'll all these autograder checks and you can keep on working on your code until I passed all the autograder text and everybody did it was very kind of cuddled shall we say what women really wanting to achieve is the heaven what has a question what way hoping is the disk can be useful will be short-term pain but useful has big and more effective ramp to doing the final project and indeed for the rest of your life for writing the reality is then the rest of your life you sorted doing things with deep learning UK you have to work out what kind of model to build and which pieces to stick together and how the right some tips to see if it's doing something sensible and it's not doing something sensible to figure out I could change things and try different things and get it to work since supply and so that's what we're hoping that people and can do an assignment 5 so you've got a figure things out and should write your own testing code we don't have a public autograder station if that's part of working out your own sanity check like what I talked about last son of getting simple confirming at they work on minute amounts of his data and so on and doing things more sensible titular particular part of that that we are planning to do I meant I was looking for it but it's on the next slide I'm so for this assignment and beyond we going to enforce rules like more like they are in CS 107 for those of you for a undergrads meaning that the two years don't look at indeed back your code for you and so you know show on TV help for come to them with your problems and talk about how you're meant to use different things in the pytorch library but you should be regarded as the two years job Python file can you tell me what's wrong with her and fix it up for you and the precise policy that written up on the outside any questions about that or do I go straight on it yeah so today's lecture since today's lecture is an easy lecture and so last time lecture there was really sort of a ton of new stuff of other stuff in your networks that you haven't seen before and we did company and we did pulling wires and we did highway is a joke connections and batch norms and 40 did 51 convolution tigers tons of new stuff is lecturer in terms of Southern new network machinery there isn't any new stuff at also this is really easy and this is also really and new lecture pudding for a reason and the reason for this relates to a kind of Mark I made last time about how lots of stuff keeps changing in your network land thought the time we first designed this class did one of the structure of it still is around 2014/15 we designed this class it was basically axiomatic that all deep learning models for natural language processing worked off words completely made sense that we start with word vectors and then we start looking at things like the current models over words the fact of the matter is in the last approximately 3 years there's been a ton of new work including some of the most influential new work there's building language models that isn't isn't an and being built over words that they're being built over pieces of words or characters and so this lectures submit to give you some sense of these other ways of doing things and patient to some of the things that's going on but the actual kind of models that we looking at house of using all of the building blocks that we've already looked at things like are Indians and comfy in today I'm going to start off with a teeny bit of linguistics of learning about the structure of language for sort of low-level you plates and then we'll see how that pans out and do things like Character level models mistakes and if you start at the bottom of the totem pole the first level spinetix which is sort of understanding the sounds of the physiology of human speech for that sort of like physics physiology or something by there are spots that move there a part selected filters and there's audio waves in between the two of them so that's kind of uncontroversial in some sense but above that level the standard thing that people do for the analysis of human languages is to say well human languages makes seem to make use of a relatively small set of distinctive units which then commonly called phone names which are actually categorical and the idea here is continuous basis right that they've got these various bits of them out like you no tongues and fairing Susan so on but it's a continuous space play we can make an infinite variety of sounds right so if I open my mouth and apply Boy sing and just wait all my tongue around I can have different sounds languages are like that that out of that infinite variety of sounds we distinguishes small space of sound and something that happens when language is changed is space sofarsounds seen as important in distinguished in a language change even within side one language as English and I'm about to give an example of Dad in Cox I talked about the phenomenon of categorical perception and what that means is that really there's something continuous humans perceive as belonging to faley shop categories and you know you can you you know styles of clothing or whether someone counts as bad or not but the most famous examples of categorical perception are in language where we can make an infinite variety of Sounds but people Turners categories and so effectively what that means is when you have categorical perception the differences within a category of perceived shrunk you barely notice them at all where differences across categories are expanded and very clear and so one of the cases that sort of study day what is what's Court referred to a sudden voice onset time so what's and languages including English have pairs of sounds like peeing be apart and a bath and they differ based on when voicing starts but it has a voice sound like a valve with an hour in it and well that's a continuous premature you can sudden make any point along a Spectrum between AP and IB but human beings to speak English to see if just two points on that spectrum really noticed the find differences between them and some languages distinguish more points on a Spektrum sotai distinguishes three different consonant sounds and in depending on the voice onset time something might be more accessible to you is this is an example of language change so for speaker like me there is Cortana different vowels in IKEA than a different vowel someone who grew up in the southwest of the United States then these are exactly the same value don't distinguish them I said the same thing twice even though I saying two different out that's where in dialecto level the people develop categorical perception ASDA actions and sounds are sensitive to or not sensitive to stomach and I'm employment training this is in some sense these sound distinctions of categorical Sound distinctions that of what a lot about language writing systems that will come to in a minute record linguistics have sound don't have any meanings and language so put and both don't have meanings and art and we don't have meetings and so people then normally distinguish as the next level up morphology is parts of word does the minimal level that has meaning here is lots of words are complex and can be made at made up of pieces of these pieces do have meaning so fortune has a meaning fortune attending which sort of gives it gives fortune to somebody so that means you're having fortune and that has a meaning and has a meaning which means to reverse that so unfortunately means that you don't have fortune and Lee and then has the meaning of turning this all into an adverb and you can say unfortunately not having gone fortune something happened and so these sort of pieces what's the minimal things that have meaning no work and deep learning has tried to make use of this sort of morphine level of structure actually me and a couple of students did actually try and build a system where do peas tree-structured neural networks the put together meanings of words out of their pieces that really is in an idea that's taken on wildly reasons why it hasn't taken on widely which is doing this and working out the semantically meaningful pieces of words and a lot of the time in NLP what people have found is you can just about get the same kind of results if you just work with character in grams but kind of units that you put into a convolutional neural you just have a model that uses and character trigrams and you have sort of start of word on nfo and so on going through the Lyn dove word suppose different units character trigrams in a distributed way will pick up all the important meaning components of the word and that's just good enough very classic idea that sort of being Revived incoming of neural networks in the mid-80s into the early 90s professional work on the structure of language anything particular daydreamer hardened J McLellan said Gemma Collins still looks like department here if you want to look him up in your spare time and they proposed and model of Howden model past tense forms in English so this was a conker experiment of can we build a system that can learn past tenses of English verbs and the difficult part there is some mini verbs irregular you had the kind of Ed ending that some words are irregular and you had to sort of learn about the irregular pattern did that I mean partly because this was sort of early days with respect to models is that they used a representation with a representative word please spell character trigrams and that was the representation of words they use instead forward in their model the Earth was met with a lot of controversy by linguists philosopher's and other people with their ideas of language instead of love debate in those days about that but from a purely engineering solution that sort of proved to be a pretty good way to do things and so this decade there's been another work which includes the model and developed in Microsoft what they using in these kind of character in G to put meaning over words now we might be interested in building models that and over words word written as characters and we're going to do something with that such as field character in g just useful variation between witches when you do this does not all the same stuff the first problem is and there are some languages that don't put spaces between words examples Chinese perfect for European ancestry is 13 ancient Greeks read ancient Greek put spaces between words either play lighter invention of mediaeval scholars who are re copying their manuscript who they decided and they started doing language is do put spaces in between words but even then they're sort of a lot of fine cases some particular a lot of languages have some sort of little bits of stuff pronounce or prepositions or serious kind of joining words like and so and which sometimes they write together and sometimes separately so in French this kind of pronominal TUI you have Antidote these little words and pronunciation just sort of run together as reviews and arguably at almost 1 word that is written as separate words and whether other languages 6 things together we're arguably that separate words so in Arabic and you can pronominal clitics and some of these joining words like so and and and that they sort of written together as one word play arguably that they should really be for words famous case of that is with compound noun in English like compound nouns with spaces between them so you can see each now even though in many respects compound nouns something like whiteboard behaves like it's one word or high school and where is other languages 20-minutes the most famous case but also have a germanic language is just like them always one word and you get very long words like they're so we gonna get different words for free doesn't don't do much else ok yeah so for dealing with words there are these practical problems already started the touch on that if you try and build word based models there's this huge space of words play the infinite space of words because once you allow and things like numbers FedEx tracking hope you allowing just morphology when you can make those ones like unfortunately hear that you cannot expand the space of words so you get this large open vocabulary a bit problematic it gets way more problematical a lot of other languages so he is a lovely check word and to the worst flammable one and where you can make so much more complex words and lots of other languages and many Native American languages at the European languages like finish have these sort of very complex word a complex word that's bad news would like to be able to look at words below the word level to know things about them so when you're translating space of things especially names where translation is essentially transliteration that you're going to rewrite the sound of somebody's name hopefully correctly that roughly correctly according to the sound systems of the different language and motor want to do that we essentially won't work operate at the letter level not the word level huge modern reasons why would like to start modelling low the word level is been living this age of social media and if you're in the social medial and there's a lot of stuff that's written not using the canonical words that you find in the dictionary and somehow it want to start and to model there so in some sense this is the easy case goodbyes this is spelt with 1234567 and 1234 scholars writing is very common and well bank if we treating things at the word level and we're trying to model this tried that clearly not like human beings are doing with sort of looking at the characters and recognising what goes on in some kind of the easy case that you could imagine preprocessing out and there's a lot of harder stuff that then turns off I guess they sort the abbreviation speak of I don't care but then you sort of get a 1 of creative spelling I'm off annunciation like I may go salmon and it seems like somehow we need something other than canonical work how to deal better with a lot of this text I'm so that suggest so don't want to start doing that with all models and so that's lead to a lot of interesting using Character level models I mean they're solid too which you can do this and will look at them both the bed level is thus a look we still going to have words and out system basically we're going to build a system that works over words what to be able to create Word representations for any character sequence and would like to do it in a way we have to recognise parts sequence that look familiar so that we can probably guess what vibes means and and so that sort of them solve the problems with unknown words and we get similar words similar embeddings for words with similar try the other alternative is to say I know just forget about these words altogether he needs to why don't we just do all about language processing on sequence of characters in all work out fine these methods have been proven to work very successfully dwell on that for one moment and that sort of goes back to my slide here when people first started proposing that they were going to build deep learning models over characters my first feeling was all that is never going to work because it's sort of seem like ok words heaven meaning that makes sense open do something like build a word the Vic model and that's going to really be able to sort of c words and their distribution and learn the meanings of the words cos words ever meaning the idea that you're going to be able to say what I'm going to come up with a vector representation of h and a different vector representation of a in the representation of T and somehow that will be useful for representing what a heck means once I put it through enough new network layers sounded pretty unconvincing to me only work so I'm convinced now chemical proof and I think what we play need to realise is that yes some level we just have these characters that don't mean me play some very combinatorial with a water parameters in them things like recurrent neural networks and convolutional neural network effectively able to sort of stonebuild representations of meaning for multi-level groups in such a way that they can model the meanings of morphemes and larger units and therefore put together word meanings yeah so I'm one more detail on using characters and from writing system inquest you tend to think of sounds as primary those with the phoneme that we I mention beforehand you know play Deep learning hasn't tried to use phonemes at all traditional speech recognise as often did yous phone him learning land it won't have a lot of data and the way you get a lot of data is you just used recent stuff because you know it's the easily found days away you can get millions and billions of words of stuff sorry makes sense from a data point of view in such as a little weird about that is that when you're then building a character Level model what's your character Level model is where is the pin me on the writing system of the language can you kind of have these quite different writing system so you have some writing systems Which are just completely phone me make that there are letters that have a particular sound and you say that sound something like Spanish is pretty much phonemic sometimes it's a teeny bit complicated so you might have a digraph so this diagram and it's kind of like the energy of English that is used for sound like the above saying but you're basically this is just try it why does the sound you can read that contrast from something like English where over non-native speakers know the spelling is terrible it's got this sort of highly fossilize time for the mick system in the 10th century or something but now we have the system filiale betray spelling the doesn't actually represent the sound destiny mix system units Canadian and Jupiter to potatoes put in there because it's such a pretty writing system of languages that represents syllables by their characters have something like this in Korean for example with trained hungo that each letter is them being a syllable of this sort of constant dial combination like bar go up a level from there and if we get back to Chinese again will is a soluble salt syllabic system you could say but really the Chinese characters and much more than just the sound they also have a meaning that this is really then and idiographic system where there are characters with particular meaning to attach them so there's a home orphans in written as one letter another example of natural language and was Egyptian hieroglyphs if you seen those that they sorted easy breakfast systems where you have letters with meanings and then you have language systems that sound like several of those so Japanese is sort of a mixture of partly partly idiographic system mixed together so start off and say ok I'm going to build a character based system that's fine but effectively your character units like letter trigrams and just very different in a language like Chinese with commonly a letter trigram will be sort of a word in a 3:30 with meaning where's if you and something like English your character trigram will be something like pho which is still sort of much too small at unit to have any meeting ahead kind of approaches and one was just do a completely Character level model and then the other one was sort of make use of characters to build bigger things that were then going to put something like into and more words level model so do this one first and the other one Chloe Character level models actually showing an example that last time do you remember so there was a very deep convolutional network from the canola to hell were protected classification at the end and that just start of the big line of characters and build these convolutional layers on top of that like network and classify the documents and so that was sober completely Character level model here's a bit more work on this so people for machine translation have machine translation systems that just read characters and write characters and when people first tried to do that and it's sorted workrite the people thought it might help to build character-level models especially for languages like Chinese but people just one able to build models that works as well as word base models and either that or the newer world play that started to change so people started to have successful Character level decoder around 2015-16 people started the stone look you could can actually do machine translation very well at just a character Level with a few extras work and the weeding and the long-running one from and 2015 on the last slide looking at English to Czech translation and checks a good language to use if you want a motorbike doing things at the Character level because it had those big horrible words with lots of morphology and I think sample I showed you before and I'll show you some more later so people built Word level models for check you know they didn't work great partly because of some of these vocab problems so the sort of Word level state-of-the-art famous 15.7 blue which as you know is much less that will set for for grades and your homework that you know what counts as a good blue score depends on how difficult the language Piri is so you're not doing check so this was sort of the kind of new empty model that we've talked about so I was asleep to seek model with the tension and then it had extra stuff substituting single word translation or by copying stuff from the store so I sort of basically Mt of 2015 15.7 blue and the differences and big but we were able to show look we can build this completely Character level model and so this sort of show the in terms of translation quality today's models were completely viable at capturing the meaning of what's this great resource someone is yes and in another way no I mean this model is controller terrible to try and write that talk about 3-weeks first the train this model and at runtime and also worked very slowly and so the problem with Character level models if you putting them into something like an lstm is just sequences getaway longer I'd say you've got about 7 times as long as sequences as you used to have any since there's not much information the characters you have to do Through Time much further back and so we were running backpropagation through time for 600 steps before we are translating her and say this photo made maybe that was excessive that made the models and very slow able to show that was able to get some of these good effects right so he is he in the chair her 11-year old daughter chinabot said it felt a little bit weird how to speak Czech in Czech speakers speakers OK I don't speak Czech either but we should we can steal this does interesting things that for the second line is the human translation into check which we can use for some guidance on particular in check there's a word for 11 years old and which you can see if that blue word on the second line and you can see that despite 11-year old was for 11-year old it's just able to perfectly produced letter by letter and the Czech word for 11 years old and that works beautifully in contrast for the word level model I'm 11-year old was an unknown word cos that wasn't in the vocabulary and so then had two mechanisms to try and deal with unknown words could either do unigram translation of them or it could just copy them and for whatever reason inside here the best strategy was to copy so that was a complete fail go along for the Character level model another thing that gets right that's really cool and is the name Shani bar it's able to do this transliteration top dimension just perfectly and I turn xender Shani bottova which is exactly what the human translated it as well and say that actually doing some really kind of nice translator things I mean best I can tell from spend a bit of time on Google Translate there she does a pretty good job in the Sims period I this part here start to be different from the human translator it's not a literal translation so this TT and actually translates feel like an the English the human side of the next use the word fionn the check version that they just went was a little bit and we're or strange couple more results from this so here's another system that was built the next year people Jason Lee Calhoun Chillin Thomas Hoffman so they wanted to do new network much more renewal and understanding the meaning of the text on the source side and so they were more using the kind of technologies we saw last time so on the encoder side you started letter sequence have character and bedding then your son of using convolutions of 43 and 5 characters to get representations up here a Max pooling with a stride of 50 you're getting a Mexico representation of pieces of the street of the 345 convolutions leaving light through multiple wires work through a bi-directional and data recurrent unit and that's giving you your source patient play codacide it was sort of the same as Audi code or as just running a character Level sequence model call opposite asters the Czech to English get better score play If You're starving looking at these different numbers will explain the system or in a minute I mean it's sort of seems like the place where they get a lot of value is that using the Character level and decoder give them a lot of value with this very complex model size is giving the most no doubt you at all one more even more recent paper so the visas fellow researchers of Google stay last year and play exploration of doing lstm sequence to sequence all models of comparing word and character based models list of French kisses wingless with just what we were doing and so cases when you have a big model the character model wins for them and the blue model comes out on top interesting thing is he sort of Steel these different effects depending on the morphological complexity of the language so therefore language like really good idea build a good model to use Character level with a game about a blue point of difference there where is stop putting French during most of extreme a tiny bit very little gain from using a character Level model so let me just explain these models so these models and models of different sizes models are using bidirectional lstm in coders and 1 directional lstm decoder tomorrow just has a shower by the external encoder and a 2 layer lstm decoder laurel hedge staircase bidirectional lstm encoders and A4 dipstick lstm decoders and the most complex smile has a 16th DAB directionality and encoders and an eight deepstack decode as this is where it helps to work at Google and probably for your project you don't want to go beyond 3 or 4 yeah so so basically what you're finding a secure making so smaller models you're better off with words big models morphologically which language you clearly start to win from the characters but there is still a loss which is essentially exactly the same lots and that we were suffering from the front 2000 play this is the the time graph the same three models as over here it's just the Exorcist changed the sort of some the total number of lstm layers and so that you know essentially if you're the word level you can find any of these three models and they are fast that you can be translating inside of and not much time but for the Character level model play Kiss much higher to get quite expensive to run the Deep Character level models that's extran sing along Waze sing some models that in some sense still do have words turn 21 word representations out of pieces and their century 10 ways of ways that people have explored doing this on way of doing it is the same we just want to use exactly the same architecture as we use for a word model win it's really going to be words what time is there going to be pieces of words and so those are often called woodpiece models and in particular there's one commonest way of doing at the cord bpe which I'll go through in some detail turn the is to say where we going to come and make a mixture a hybrid so I'm a model is going to work in terms of words some kind of facility where we can construct a representation but otherwise unknown Words by doing things as a character or lower level and I'll show you a bit of that as well BP BP is actually pretty simple idea which has nothing to do with deep learning but the use of BP is sort of become standard and successful and 4 representing pieces of words are you two have February 12th February for actually working with a finite vocabulary play chances by peer and codeine and the name B peer is do with natural language processing on units with Joshua and compression algorithm so this is something like you know compressing your documents with cheese dip I'm so byte pair encoding is that you got a collection of stuff with B and you're looking for the most frequent what's a to b ok I'm going to add that sequence of to buy new element to my dictionary of possible values and that means I can have 257 different values for b so to speak that I can shrink the length of my sequence and I can repeat over and do that again sleep suggested reply this kind of compression algorithm and use it as a way of bring up with words that we use fall and doing a not strictly with B despite the name but instead with characters and character in g common way to do this is with characters and character him G and if you're up with modern times you know that means there's unicode and you can represent all these lovely letters like Canadian and Luke 22 stuff like that but you had a problem with unicode which is you know there are actually a lot of unicode characters theoretically I think there's for 200000 possible unicode characters but anyway if you want to handle bunch of languages which include East Asian languages maybe need something out 20000 characters and that sort of a w so they're actually some people who literally gone back to B and said you know 200000 it's a really big bike every I don't want to do it anything really big vocabulary I don't want to do it anything that large so why don't I actually just algorithms overbite the dim utf-8 in coding characters Take 3 bytes each and so you actually have to all characters issue vaccine words together and several B to the common sequences concretely and how does this work so with so doing this bottom-up clustering of short sequences so we start with the unigram vocabulary which is all of the unicode characters in some days ask what's the most frequent in G have an initial beer by grandpa and we had that 12-hour cabaret restart connect to cinema let me have a text that has been divided into words so we do have word tokens and so we can represent as the dictionary and say here some words with their frequency and so now we look for a common letter sequence and we say 0es 9 times in this day there could we have accounts for the words on the left side start with l vocabulary being all the individual letters a communist leader sequence like es play Let's come together and make that a new thing about February extra thing and elbow cabaret Common sequence of something well actually all of the ZS is a followed by tea so we also have est with frequency 9 and so we can add that 12:00 cavalry and then we ask again what's another common leather sequence let's see there are seven cases of O'Donnell cases of either ELO or aw so we can lamp those and then we can lamp again and make an l o w so free sort of run this we start to build these car letter sequences B like est but also does common words something like that in English will very quickly become together and the unit about vocabulary and so we do that profile so normally what we do is we decided vocabulary size that we want to work with we say ok I want to work with the vocabulary size of 8000 words that will mean my model will be far keep doing this until we have 1000 things now vocabulary and that means out vocabulary will have in all single letters because we started with them and there's all have sequences of Words by tes in the ST4 now no vocabulary but also have Hall words whenever their common so it was like you know that and 2 and with and so on will become part of our vocabulary so then we have a piece of turf Terminus stick longest piece segmentation of words that was outside of word pieces and so is a text returning to word pieces and then we doesn't run it through our Mt system as if we were using words that relate pieces of word the output side we just concatenate them back together as needed we get the automatic word base system to be a very successful system so this idea of using B pairing code inside of really emerged in 2015/2016 and workshop on machine translation which has been the main source of annual competition for empty systems that the several top systems byte pair encoding if you look at last year's competition there's a bit more variety but really and number of the top systems are still using by parent coding that's just been a good way to do things Google machine translation they effectively use them a variant of by parent coding so they don't use exactly the same algorithm they use a slightly different algorithm where they using a language model and they're saying what what rather than just using pure account when maximum the perplexity of my language tomorrow and compos things and repeat over they've done two versions of this model so the first version of Word peace model kind of codeine assume that you have an initial tokenize ation to words and then you just does a word algorithm s version the sentence peace model which you can find at this guitar outside with said problem that if we need the tokenize into words we need to have a tokenizer for every language and that's a lot of work and so maybe instead of there we could character sequence retain white spaces and regard that as something as part of the clamping process did you just build your word pieces with have spacers on one side or the other of them because often things inside a word of the commoner more common clamps and you build those up proven to be quite successful one place where some of you might see this and we get to get to describing in the class really that there's been this recent work which we actually talk about next week and class of building these transformer models in particular and Google has released this model which gives you very good word representation download bird and try and use that what you will find out is it doesn't operate over word the word pieces Hessle large vocabulary not about Cadbury like 8000 words I forget the number that the models have a large vocabulary but they still not accused vocabulary and it's using word pieces so what two words are in the vocabulary that you look at the English model in not only has worth like anything that but it even has worst like Fairfax and 19 pins which aren't that common but it's never left to cover all words it's a game using this word peace idea so if I want to representation of the word Hypatia and that's not in the vocabulary and so I'm making it up of pieces there's an hoa presentacion and then in the book version which is different to the Google and auntie version and the Nun show word pieces are representing with two hashes at start so I hyp etc and this would be my representation of Hypatia so effectively I would victors and for forward pieces and then I have to work out what to do with them the simplest and quite common way as I just average the four of them and then obviously other things you could do you could continental next Paul Willis TM or something to put together representation so models that pieces of words to give you infinite vocabulary and random through and normal sister these to say well we want to work with characters so I can deal with an infinite vocabulary but we're going to sort it incorporate those into a bigger system and a whole bunch of work has done there fairly obvious thing to do I'm so this work in 2014 was one of the early one well we could start with characters a convolution over characters to generate word embeddings and then we can use those word embeddings in a higher level model and this was actually sort of a fixed window model for doing part of speech tagging make sense instead of a convolution you could use lstm so this was I'm work from a year later and they said well we're also going to build up and word representations from characters and the way we going to do it run character-level by lstm concatenate the two final stage that our word representation and then we can put that word representation into a language model which is then a higher level lstm that works along sequence of words play Training you're learning you're play I guess I might be showing the input layer but the input layer your learning a vector for each character play you're doing the same kind of thing we saw before that you're starting with random representations reach character this embedded inside a word sequel your goal is to minimize the perplexity of the higher level lstm the language model and so it Becketts gradients wanting to come up with character vectors such that is it produces good word vectors which produces low and perplexities I'm a slightly more complex trying to do this that's a bit more resend we're again the ideas can we build a good language model all and by standing out from characters and wanting to exploits of related sub words and where words they built for this kind complex model that will go through the stages of here for we start with a word represent does characters we have character and buildings which we built into a convolutional network and then we hit up with Sophie take that one piece of the time character bedding fruit character Yoda delusional layer which then sorted filters over there the sequence of 23 and 4 g of characters so getting representations of parts of words funders show network show them doing Max pooling over time which is effectively sort of like choosing which of these in G this represents the meaning of a word what day do after that is so at that point they've gone output representation for character in Graham's and so then they feed that into a highway network like we talked about a bit last time and then the app for of that then the word level and goes into an lstm network and its lstm network is now Word level lstm network and you're trying to sort of minimise perplexity like for the new language models we saw earlier what could I show with this for the first thing I could show with it is the election again just works well as the language model this isn't that I hadn't told you of about the fact of the matter is you can build this kind of character Level models and train them and they work to a first approximation as well as Word level language models one of the observations that they make is that you can be getting as good result smaller models at the top here are they character-level lstm models and word ones at the models they build and hear a whole bunch of models over this day 3rd and in first time went by perplexed these have been going down gone to 78.4 and their point was well we can build pretty much that's good character model with 78.9 eggs a day that almadel is actually much more like this model he has 52 ml where is Al model works on a character Level has only 19 million about 40% of the size and that seems kind of interesting what's more interesting as the source of peek inside that and see what happens the representation of words when built out of characters and this part is so that's a bit cool so what this is showing is 4 words top while his you Richard trading and it's asking what other words are most similar to it according to the word representations this computer out in the top part is the output of the word level lstm model and that sort of OK Richard comes out of similar Jonathan Robert Neil and thence etc while although living though Hey Son of interesting what happens with the Character level model particular what kind of interesting is like first of all you remember they said have had the character embeddings the went through the convolutional layer and the max pulling that point you are I most similar that basically it's still remembering things about characters the words to while a chilli whole meanwhile and why they all end in LA heaven elsewhere by Cliff Richard had richer Richter that hardens ARD which king was character sequence similarity it's not really doing meaning at all but interestingly when they then put it through the highway layers that the highway layers learning how to Transformers character sequence representations into something that does kept a meaning so if you then say it the highway layers what words then it seems to be working pretty well but I was simulator meanwhile richer the same were the Edward Gerard Edward Carl that sorted now working much more like a word level model in capturing semantic similarity fine decor when I say well what about if we asked about words a tomorrow well if they're not in the vocabulary of the model the word level bottle can't do anything until it's why you get those dishes there and what they wanted the show is the character Level model still works pretty well so if you give it will look with 7 hours in the middle of her that it's correctly deciding and that looks looking are actually the most similar words to that is actually working very nicely and some of the other examples of stimuli computer-aided is seen as most similar the computer guy the computer driven computerized computer again priest someone sensible results picture on visualisations of the units of being learn things that were prefixes that blue thing to character suffixes things that hibernate in the middle of computer guy there and Gray as everything else and so there's some sort of symptoms which is picking out different important parts of words also I guess just another how you can sort of compose together different kinds of what's the make more powerful models that you may also want to think about for your final Project is back to one other example for manual machine Translation system of doing this hybrid architecture that has Word level and Character level I showed you earlier a purely character-level model I mean with book that out of interest to see how well it did but we are sort of really wanting to build a hybrid model because that seem like I've been much more practical to build something that translated relatively quickly and well the idea was with mainly build a word level new machine Translation system that would be able to Character level staff when we had rare and seen words turned out the word pretty and successfully and improving performance so the idea of that model is this run a pretty standard sequence-to-sequence with attention lstm neural machine Translation system and in my pic and in section 4 level deep system but in my picture I said less than 4 to make it easier to see things and we going to run this with a reasonable vocabulary of 16000 words word representations that with feeding into our new machine translation model but for words that end in the vocabulary we going to work out the word representation for them by using a character Level lstm and Converse play When We Started generate words on the other side we have a soft mix with about Cadbury of 16000 it can just generate was like but one of those words is the symbol and if that generates the anchor symbol with an 18-carat representation and feeders Dynasty initial input into a character Level lstm and then we have the Character level lsdm generator character sequence and sort generator can we use that to generate words so postech of a tower stem wires if you wanted to get there change play You Only turn the Character level when the accent to hire opening time there's a determinate piece of text right you know the force and you know the target training time with already decided vocabulary right so we've got the 15999 most common words those in and out of vocabulary so for both the input and the output side we know which words aren't and vocabulary and so if it's not an avocado with running this one if if what was the app does not have a carvery running that one and otherwise we're just not running at all yeah so and a big Delight in explain but is actually important apps related like when we what's that we can back propagate that sort of up here there sort of two losses is a WhatsApp the word level that you know you'd like to win this position give probability 12 generating aren't but really this model will stop maxol say anchors you know probably two or whatever said there's a lost there and then secondarily there's a particular sequence of characters you want to generate and you've also got a the lady she put out the characters I think Ibiza briefly mentioned this commonly the decoders do some kind of being searched to consider different possibilities before deciding the higher anyone over sequence of words and so this was doing a slightly more complex version and that too there's a word level beam search when running up and then also doing a character that will be inserted to consider different possibilities and so if you want it integrate the two of those together essentially this work pretty well this was the winning system and wmt 2015 which used 30 times as much data an ensemble together 3 other systems compared to the days of those provided for the test to my show before they got 18.3 min if you remember our karakuri character-level system got 18.5 building this hybrid system that we're able to build an much better system knows about two-and-a-half bluepoints better than than either this word level or the Character level system so that was kind of nice how many particular that was the state-of-the-art of the time the report if you were paying very close attention that now nowhere near the state-of-the-art because when I showed you that slideway earlier of the Google system you will they have much higher numbers in the 20s but that's what happens as the years go by Jason example that shows he's different systems working and some of the mistakes they make and here's a cherry-picked example and where the hybrid system works perfectly to see you can see some of the defects of things that can go wrong so in this case the level system didn't work here because it just sort of an starting with the Stiff it's free associate completely made up name that doesn't really have anything to do with the source so that one isn't very good Word level system went bang here so you remember when it generates an what system status using a tension so when it wants to generate its has attention back to words in the sauce in with a generator on kid has two strategies they can either do unigram translation of the word that is maximally put the attention on or could copy the word that is maximally putting attention on I'm so in this case it shows to translate the word was next week putting attention on but the word as messali putting a tension on was after rather than diagnosis and so you just get this popo coming out of after after and we completely lost the word in this example sample hybrid system just in that working beautifully and gives you exactly the right translation yeah of course it's not always that good in the real world so he is a different example for thisissouthwales showed before with the 11-year old daughter and alarm for the hybrid model strength of the character model it correctly generates 11 years old at a character Level in its translation that you had this time for whatever reason bang in generating the names translate shiny bark is grey and bad where is the character Level model gets arrived and actually I think this is one of the weaknesses of this hybrid model compared to the Character level model that because of the the Character level generator is kind of this sort of second level Julie character-level model it's able to use the character sequence as conditioning context very effectively where is hybrid model although we feed representation of the word level model in the starting in representation of a character Level model it doesn't have any representations further back than that of w in the word level model and so it's not always do representing of capturing the context allowed to do translation things like names there's just one thing I wanted to mention before the end which is almost practical saying so we start off with word embeddings but now if he took a lot of character Level models so surely just for word embedding should be able to do useful things with them with characters or pieces of words and that something that people start to play with so in this cow and re paper they said well let's train a word to Vic model using exactly the same lost as word of excuses but let's and rather than having word representations let's start with character sequences and run national lstm to work out word representations and will then sort of be effectively training with more complex model we're learning character and beddings and I'll stm premises and that will give us L Word representations and that's an idea that people have continued to play with answering to take for a just wanted to mention these past text embeddings so a couple of years ago and people now Facebook the same commercial vehicle off who did the original word to Vic thought I had a new set of embeddings the fast text and beddings and their goal was to still have a next-generation word to Vic which is sort of an efficient Farm words vector learning library and it was better for real words and languages with lots of morphology in the way they do that was the day service century took the word to take skip-gram model but they admitted it to put him character in G so more precisely this is what they did so word my example words way for some in G sized you represented as a state of Ingram so this is kind of just about like those we call phones I mention right at the Beginning where you have a kind of the boundary symbol suno the beginning of the words of the latest three you have beginning of word why why he re re re end of word as representation and then you'll have an additional 14 just the whole word so you do still have whole word representations in this model so where is represented by 16 you gonna use all 6 of those things in your computation and 30 sort of remember the guts of words of Eric that what you're doing what you're doing these and vector dot product between your contacts representation and your centre word representation so they're going to do exactly the same thing but for the centre word they gonna use all 6 of these Victor corresponding to all six of these representations and they're going to someone what's doing the simple farming operation and that sort of them giving you your representation of similarity size 3 they don't quite do that but there's a hatching trick but I'll leave that are so what they're able to show is that model actually works pretty successfully so the the word similarity scores sibo and then this is the son of new model and use these kind of in g your least for one of the English datasets it doesn't get any better but what they especially knows this is for languages that have more more apology that just getting some fairly clear again 7069 on 275 1666 in the right column so that these wordpiece models to give them a better model of words and and just practically fast free now has sorted word embeddings for about 60 or 70 different languages so sort of a good source of word embeddings multilingual applications I think I am done and see you again next week 