started so welcome to the very final lecture of the class I hope you're surviving the last your Projects so today we're going to be hearing about the future of NLP and deep learning still travelling and today we're going to be having Kevin Clarke who's one of the PSG scenes in the lab in the also one of the head days for the class last year so he's very familiar with the class as a whole takeaway Cavan it's going to be back after B&Q last year are you today to be talking about the future of deep learning NLP obviously trying to forecast the future for deep learning anything in that space is really difficult because the field is changing super quickly as one word reference point Westwood what did Deep learning for NLP look like about 5 years ago a lot of ideas that are now considered to be pretty core techniques when we think of people an LP and didn't even exist back then I'm so things you weren't in this fast like c2c contention mechanism word scale reading comprehension even frameworks such as tensorflow or pie torch and didn't the point I want to make this is that difficult to look into the future and say ok what are things going to be like what do you think we can do that is look at an areas that right now or really for a taking off so areas in which there's been a lot of recent success and kind of project from that that those same areas like will be important in the future I'm going to be mostly focusing and one key idea idea which is the idea of leveraging unlabeled examples when training are NLP systems booking a bit about doing that for machine translation both in improving the quality of translation and even in doing a translation in a unsupervised way have paired sentences with with their translations on you try to wear in a translation model only from modelling go corpus the second thing I'll be talking I will a bit about is opening hours gpt2 and in general this phenomenon of really scaling-up on deep learning models will a bit of this in their backs drunk and tax over presentation but this will be a little bit more in depth I think new developments in and have had some pretty big impacts in terms of more broadly kind of beyond even the technology were using and in particular and it's starting to raise more and more concerns about the social impact of NLP both on in water models can do and also when kind of plans and where people are looking to apply these models and I think I really have some risks associated with it I'm in terms of security also in terms of areas like bias talk about future areas of research on Deezer mostly research areas now that are over the past year have really could have developed into promising areas and I expect they will continue to be important in the future don't wait on this question why is he burning been so successful recently Sonic I'm here there's a specific person and they've got some really complicated and well motivated method for doing the task they care about and then the net person just says coronavirus is that has not been successful recently because it's more theoretically motivated or more sophisticated than previous techniques that actually lot of older statistical methods have more about a theoretical underpinning than some of the tricks we do indeed burning really the thing that makes deep learning so successful in recent years has been ability to scale no nets as we increase the size of the data has been criticised for models they get really big boost in accuracy and other approaches do not if you work to the 80s and 90s and there was actually plenty of research in their own that's going on but it hadn't doesn't have the hype around it that it does now and that seems likely to be because I'm in the past there wasn't the same resources in terms of computers in terms of date only now after we reach sort of an inflection point where we can really take advantage of scale in our deep learning models have we start to see it become a really successful paradigm for machine learning big deep learning success stories and I think you can see kind of this idea play alright so here are three of what are arguably the most famous successes of burning right of image recognition where before people use very highly engineer on features across my images and now known natural material and those method translation has really close the gap between phrase based systems and human Colley translation so this is widely used in things like Google Translate and the quality is actually got a lot better over the past 5 years sample paper round 8 of game playing so this pin work at Atari games there's been alphago and more recently there's been an offer starring opening hours 5 you look at all three of these places underlying the success is really large amounts of data rights for image recognition there is the image that data said which is working knowing images machine translation data sets off and have millions examples for game playing you can actually generate as much training data as you want essentially just by running your agent within the game over and over again so if we if we got the NLP it is quite a bit different for a lot of gas if you work at even pretty core kinda popular tasks to say reading comprehension in English datasets like water in the order of 100000 examples what's the millions or tens of millions of examples that these previous and successes have had benefited from and that's of course only friend there are thousands of other languages I think it's a problem that has a diverse today and the vast majority of data in English when in reality fewer than 10% of the world's population and speak English as their first language problems with small datasets or only compounded if you work at the full spectrum of Wayne Bridge's and that exist so what do we do when were made by the state over we want to take advantage of deep learning scale and train the biggest models we can the popular solution that especially had recent success is using unlabeled data because I'm like labelled data and label data is very easy to acquire a language and you can just go to the internet you can get the books you can get lots of time where is labelled data usually requires and at the we scrubbed sourcing examples in some cases you even require someone who's an expert in something like this text maternity that data and the first part of the talk is going to be applying this idea of leveraging unlabeled data to improve rlp models the task machine translation machine translation data I'm in is true that they do exist quite large datasets for machine translation don't exist because NLP research areas of annotated tax for the purpose of training and models right to exist because in various settings translation is done just because it's useful so for example proceedings of the European Parliament and proceedings of the United Nations some nu citeste translate their articles into many languages so really the machine translation data we used to turn a models or often more of by-products existing cases were translation is wanted rather than kind of a for sampling of the Sword of text we see in the world play Sum 41 it's quite limited in domain right so it's not easy to find translated tweets are not able to work for Twitter add those limitations in terms of the languages that are covered right so someone which is the European languages does Waterford insulation data and brother languages there's much less go into settings where we want to work in a different domain we want to work with a resource language we riveted by the weekend do is pretty easily find unlabeled data so that actually pre-salt problem and maybe not 100% but we can with good accuracy look at some text and decide what language it then I'm trying to classify it's really easy to find data in any way would you care about because you can just go on the web and essentially search for data in that link corpus of monolingual data I'm now going into the first approach out on using unlabeled data to improve machine translation models make a call pre-training in a reminiscence of ideas like Elmo idea is to pretrained by doing language model languages were interested in translating from one to the other will collect large data sets for both of those languages two English tomatoes one each we can use the pre-trained language models as initialization for machine translation encoder will get initialised with the weights of the language model trained on the floor side language initialise with weights turn on the Turkish language this will improve the performance of your model because during this pre-training we hope that our language models will be wearing useful information such as you know the meaning of words or structure of the language and their processing underscan down the line tomorrow will be fine tune it if there are any questions and just in general feel free to ask questions about the stock ST4 here is a plot showing some results English to German translation access is how much training data is in supervised training data on you provide these models with of course they also have large amounts of data for this free training stop you can see that just works pretty well right to go about 2 blue points appointments vs red line about the blue line when doing this free training text surprisingly this game is especially worried when the amount of labelled data is small is a problem with a free training which I want to address which is that in pre-training your these two separate language models and there's never really and the interaction between the two when you're running them on the unlabel corpus here's a simple technique that try to solve this problem and it's called self training it is giving a sentence from a modelling walk Waterstones case I travel to Belgium this sentence have a human provided translation for the centre we can run our machine translation model and we'll get a translation in the target language from machine learning model it won't be perfect we can hope that maybe your mi can still wearing from this kind of noisy weibull example Zoe turn on my sentence and it's machine provider translation as though it were a human provided translation and training r machine learning model as normal I think strange actually as a method when you first see it because it seems really secure this the learn about tomorrow is being trained to produce exactly what it already produces to begin with right because this translation came from a model in the first place so actually in practice this is not a technique that very widely used due to this problem and but it motivates another title translation and this technique is really a very popular solution to that problem and it's a method that has had a lot of success data for translation Roche rather than only having our Translation system that goes from source language language we're also going to train a model that goes from our target language to a source language if at the end of the day we went to French to English tomorrow and we're going to start by actually training and English to French model do something that I like soft labelling so we we take a English sentence we rather English to French tomorrow and translator reference to what we did before is that rushing a switch the source and target site the French sentence is the sort sequence target sequence is original sentence on that came from a modelling okra and overtraining a language that machine Translation system because the other direction so that goes French to English search the why do we think it will work better 1 there's no longer this kind of security to the training because what the models been trained on is the output of a completely different model another thing that I think is pretty crucial here is that the translations the model train to produce the things that the Dakota is actually wearing to generate are never bad translations rights issue up by the target sequence for French to English model I travel to Belgium that originally came from a monolingual purpose literally this makes sense is that if we want to trade a good translation model are we ok to expose it and witty and puts the exposure to the output of a system that English to French and may not be perfect but we don't want to do is exposed to pour target sequences because then it won't wear and how to generate in that language effective way any questions on Dark translation in before I get the results set a stewing we have a word corpus of unlabeled data and we want to be using it to help our translation model play sand clarify the question believe a big corpus of English which includes the sentence I travel to Belgium and we don't know the translations who is still like to use this data yeah that's good question how do you avoid thanks for growing up and producing garbage and feeding garbage to each other is that there is some amount of labelled data here as well unlabeled data you do this but unlabelled data you do standard training and that way you avoid you make sure you can keep the models on track because it's still have to fit to the labelled data another question Redrow the training for two models that is a good question and I think that's basically almost like a hyperparameter you can't I think a pretty common thing to do is first train tomorrow's only on mobile data so don't you back translation over a large corpus can you repeat that process over and over again so each iteration Ukraine on the weibull data we both unlabeled data and now you're more data to work many many kinds of scheduling that would be effective here I have a question good French to English model look English model lookup the original source adjusting on like English to French to English and seeing as I see yeah yeah that's really interesting idea I'm working to talk a little bit about cycle consistency this idea I'm wearing the stock I'm going to go on to the results so so because the method for using unlabeled data to improve translation how old is the deal that the improvements are surprisingly extremely good right so this is for English to German translation and this is France in work by Facebook so they use 5 million labelled sentence Paris also used 230 monolingual sentences sentences about translations either state-of-the-art they get six points improvement which if you compared to most previous research on the screen machine translations are really big gain right so even something like the Invention of the transformer which most people would consider to be a really significant improved over prior work about two-and-a-half 3 points and here without doing any sort of fancy model design just by using way more data we get actually much more improvements an interesting question to think about is supposed we only have her modelling okra so we don't have any sentences that have been human translated we just have sentences into languages the scenario you can sort of imagine is supposed and comes down and stairs talking to you and it's weird what would you eventually be able to you translate what it's saying to English just by having a really large amount of data start with a simpler task than full-on translating when you only have one label sentences translation let's start by only worrying about word to word translation here is giving the word one language - translation but without using any weibull data and the message the method to use to try to solve this task cross-lingual embeddings the goal is to where in a word vectors for words in both languages we like us where bacteria have overnight properties you've already worried about work that you're having but we also one word factors for a particular language what is translation not sure if it's visible in the figure but just visit figure shows a word number of English and I think German words see that the each English word has its course man in German word nearby to its inviting space embeddings like this then it's pretty easy to do word to word translation in English word we find the nearest German word in this joint and bedding space that will give us a translation for the English word method for the key assumption that won't be using to solve this if you run vertovec I still get really different embeddings structure of that embedding space has a lot of regularity to it genital irregularity to help fine and an alignment between us embedding spaces cullimore concrete two sets of word embeddings and Bradley have an English word we have Italian words although the vector space is right now with three different to each other the you can see that they have really some more structure right so you'd imagine distance is a kind of summer distance from a cat and feline in the English embedding space should be pretty similar to the distance between gato and felino in the Italian space motivate an algorithm for winning these cross-lingual embeddings any idea what I'm going to try to do with when was essentially a rotation that we can transform our set of English embeddings so that weather Italian bed in beddings what does means when were the Matrix aw such that if we take that say the word bacteria for cat in English and we x w we end up with the back door for gateau in Spanish or Italian the detail here is that we're going to constrain w2b orthogonal and that means geometrically is just that we was only going to be doing rotation to the vectors in Acts and it's not going to be doing some other weirder transformation this is Uncle also win the w talk about talking about how actually will you and there is actually a bunch of techniques for burning this wmatrix turn around I think it's quite clever is called adversarial training so it works as follows is in addition to trying to wear in this w mate me trying to learn a model that is called a discriminator and what it will do is take a doctor and I'll try to predict actor originally an English word embedding originally in Italian Virgin betting if you think about the diagram were asking a discriminator to do is it's given one of these points and is trying to predict is it basically a red points or an English word originally or is it a blue Point so if we have no w matrix then this is a really easy task for the discriminator because the word embeddings for English and Italian are clearly separated if we were on a w matrix that 16 and a whining all these embeddings on top of each other are discriminated will never do a good job right we can imagine it will never really do better than 50% because given a vector Force a cat it won't know is that the factor for cat that's been transferred myww or is it actually the doctor fegato because in this case those two vectors or lines are there in top of each other so during training you first you alternate between training with discriminator a little bit which means and making sure it is good as possible at distinguishing the English from Italian word when are the w and the goal for training WS2 essentially confuse the discriminator is much as possible a situation where you can't with this machine learning model figure out if a word embedding actually was originally from English or italian word back here and it's the end of the day you have your factors that are kind of online with each other any questions about this approach does a link to a paper with more details was actually can't arrange other tricks you can do but this is kind of a key idea Towers doing word to word and supervise translation you for sentence to sentence translation how to use a standard for the seat the seat model without even an attention mechanism 1 secrecy tomorrow going on here which is that going to use the same encoder and decoder midwest the input output languages see I'm in this example we could give the encoder an English sentence with off to give it a French sentence and it'll have these cross-lingual inviting so have doctor representations for English words and French words which means I can handle serve any input for the decoder information about what language is it supposed to generate and is it going to generate in French or English what is done is by feeding in a special token which year is IFR in brackets to represent French that tell the model ok you should generate in French now only French but you can imagine also feeding this morrow English in brackets and then that will generate English is that you could use this sort of model to generate to go from English to French and you can offer you this model is an autoencoder right so at the bottom it's taking in a French sentences input and it was generating French as output which Siri is just reproducing input sequence so just a small change the sender Tower going to train the speaker seat model it's going to be two training objectives splinter of why there are present in this model and just a few slides and for an hour to see what they are this one is on call the denoising autoencoder the trainer model to do in this case is take a sentence so and here is going to be neither sentence but it could also be a French sentence how to scramble up the words a little bit ask tomorrow to denoise that sentence mean what percent is actually was before with scrambled maybe one idea why training objectives is that since we have an encoder decoder without the attention the encoder is converting the entirety of the sentence into a single vector another one twitter does is ensure that doctor contains all the information about the sentence such that we are able to recover with the original sentence was from the vector produced by the encoder subjective one I'm training objective to is now we're actually going to be trying to do translation as before we're going to be using this back-translation idea we only have unlabel sentences we don't have any human provider translations but we can still do is given a sentence with a French sentence give me a French sentence we can translate to English sing our model in its current state we can ask that model to translate from English or translate English back into French imagine is in a setting the input sequence is going to be someone must in perfect machine learning model to hear the input sequences just I am student has been drop trainer to even with this kind of that input to reproduce the original corpus of of monolingual and French tax you're actually and ask her questions what are the reasons for your words working daddy sit down question so this is going back to earlier when there is a word word translation why would we can strain that wmatrix have your foreigner avoid overfitting in particular is making this assumption that are embedding spaces are so similar that there's actually just a rotation that distinguishes a word doctors in English VS a word batteries in Italian results that don't include that orthogonality constrained and I think it's slightly hurts performance to not have that in there continuing with unsupervised machine translation a training method I didn't quite explain why I would work so so intuition for for this idea on our way to initialise our machine translation model with his cross-lingual embeddings which me the English and French word should using the Shared encoder that means if you think about it at the top we have just a autoencoding objective and we can certainly believe their model can learn this it is a pretty simple task now imagine we're giving our model a French sentences input instead since the embeddings are going to look pretty similar and since the encoder is the same it's pretty likely that the models representation of this French sentence should actually be very similar to the representation of the English sentence the Wonders representation is passed into the Dakota we can hope that will go the Sterling tomorrow I'm already able to have some translation capability another way of thinking about this is that what we want our model to do is to be able to encode a sentence such that representation universal universal presentation about sentence that's not specific to the language a colourful picture that's trying to get at this so autoencoder you're in a back translation example a the target sequences the same essentially means that the vectors for the English sentence in the French sentence or going to be trained to be the same a different or decoder will be generating quotes on his two examples intuition is that what are models trying to wear here is kind of a way of including the information of a sentence in a vector in a way there is language agnostic for questions about unsupervised machine translation the results of this approach the horizontal lines are unsupervised machine translation model time to go up there for supervised machine translation model more and more data surprisingly given a word amount of supervised data the supervised machine translation model better than being supervised machine translation model I'm being supervised machine translation model actually still does quite well around 10000 to 800020 examples it actually does just as well or better than supervise translation promising result because if you think of settings for the risen much labelled examples and the family becomes really nice that you can perform this well without even needing to use a train set everything kind of fun you can do with a unsupervised machine translation models attribute transfer so basically you can send a text that sprayed by any attribute he wants to for example you can go on Twitter will get a hashtag decide which tweets are annoyed in which we serve treat those two portraits different languages unsupervised machine translation model to convert from one to the other and you can see these examples does a pretty good job of sort of minimum wage changing the sentence kind of preserving a lot of that sentence is original cement set the target attribute is changed a little bit of cold water on this idea so are you think it's really exciting and almost kind of mind-blowing that you can do this translation without labelled data certainly right it is really hard to imagine someone giving me a bunch of books in Italian and say ok we're in Italian without you know teaching you how to specifically do the translation is method so promise mostly they have shown promise on languages that are quite closely related it's results and some combination of English to French English to German I met someone and was languages equites in Ware I definitely would pair of things to Turkish where the linguistics in the 2nd digit by different these methods do still work to some extent so they get around 5 points that say but they don't work in your way as well as he doing in the other settings right to restore a huge gap to purely supervised learning Play Sarah probably not you know quote of the stage where and if you could come down and it's no problem with using unsupervised machine Translation system but I still think it's pretty exciting Prague do you think English to Turkish English to map quite a lot better to Latin and I think part of the issue here is that the difficulty in Translation I think is not really at the word level so Ministry is an issue that word existing one language that don't exist in another and I think actually more substantial differences between languages at the level of bikes in tax you know cement x-ray how ideas are expressed Italia to have a relatively similar syntax English and compared to say Turkish and imagine that is probably the biggest obstacle for unsupervised machine translation models we going to this last recent research paper which is basically taking birth and which would you find about correct making a Crossing wall cured white regular is right we have a sequence of sentences in English word in a mask out some of the words Andrea aspirate which is there a transformer model to asexually fill in the blanks dropdown has already been done by Google is training a multilingual Bert play the century is concatenate on whole bunch of corpora in different languages green one model doing using a smashed alarm objective on all that text at 1 tomorrow the new kind of extension to this that has recently been proposed by Facebook is the actually combined this masks Alam training objectives with translation what they do is sometimes give this model a in this case and sequence in English French drop out some of the words and just as before ask the model to fill it in here is that this one much better understand the relation between these two languages because if you're trying to find a fill any English word that's been best way to do it if you have a translation of booking the French side and try to find out word I'm hopefully I'm one hasn't been dropped as well and then you can easily fill in the blank this actually means to Derry a substantial improvements in unsupervised machine translation so just like is used for other toaster and they basically take this crossling station for a unsupervised machine Translation system and the gap your reward games on the order of 10 blue points such that the gap between unsupervised machine translation and the currency provide state-of-the-art is much smaller this is a pretty recent idea by thing about the shows promise in really improving the quality of translation through using unlabeled data I guess yeah I get from the state of bird they are using weibo translation date as well any questions about this that is all I'm going to say about using unlabeled data for translation service talk Isabel what happens if we really scale-up these unsupervised language models make your own min talk about gpt2 which is the new model by opening that essentially a really giant language model and I think it has some interesting implications here's just the sizes of a bunch of different tomorrow a couple years ago that the standards would have lstm medium-sized model was on the order of about 10 million they were premature is just United single wait what's a inlet Elmo and GPT for the original opening a paper before they did this gp42 and we're about 10 times bigger than at gpt2 with about another order of magnitude bigger one kind someone here is that a gpt2 which is a 1.5 billion prime minister's actually has more parameters than a honey bee brain has set up you know honey bees are smallest of animals but they can still fly around in fine next year whatever this isn't really in apples apples comparison right for a synapse anyway and you're not really quite I think that's one kind of interesting milestones what's a in terms of model size that has been surpassed out here is that the scaling a deep learning is really a general trend machine learning beyond NLP service plan is showing a time on the x-axis and the y-axis is 1 scale the amount of paraffin wax used to train this model what this means is that the trend and he's currently is that there is exponential growth u-power I'm withdrawing out of machine learning models what exponential growth continue but certainly there is rapid growth in the size of her models some really amazing results right so he results not from language but revision this is a generative adversarial network has been trained and awhile data and it's been really hard scales the big model kind of the size of Elmo and Bert website instead of Here actually Productions of the model so the photos everything's the model is just kind of hallucinated out of dinner and at least to me they were essentially photo-realistic website that is it work out if you know if you're interested which is this person does not exist.com so if you go there you'll see a very convincing photo of a person and it's not a real photo it's again like a hallucinating image produced by again what's the same really huge models being used for image recognition so this is recent work by Google already training imagenet model with half a billion parameters bigger than the biggest gpt2 here is showing log scaled number of parameters on the x-axis and then accuracy image now and access surprisingly bigger models perform better and it seems actually be a pretty consistent round here which is actresses increasing with a wall above the the model size I want to go into a little bit more detail how is it possible that we can scale at models and train models that such a weird accent one answer is just better hardware in particular there is a growing number of companies that are developing hardware specifically for deep learning even more kind of constrained in the kind of operations they can do with another GPU on but they do those operations even faster hey Google tensor processing unit as one example and they are actually a bunch of other companies working on this idea the other way to scale at models is by taking advantage of parallelism and there's two kinds of pluralism they want to talk about briefly the one is data parallelism each of your website GPUs with a copy of the model you do is split the mini-batch that you're training on across use different models what's a 16 GB use in each other and see about size of 32 aggregate the gradients of these 16 if you do back and 16GB used and you end up with effectively about size of 512 train models what kind of pillow is in bed growing in importance as model terrorism eventually so big that they can't even fit on a single GPU and they can't even do a bath size of 1 in this case we need to split up the model across multiple compute computer unit and that's what's done for models kind of the size of a what's a GPT to their new framework such as match tensorflow which are basically designed to make this sort of model parallelism easier ok so energy PT 2 I know you ready for this little bear in the contextualized embedding but I'm only going to the more tap here essentially it's a really large transformer language model so there's nothing really kind of novel here in terms of new training algorithms or in terms of the functions or anything like that the thing that makes the different from firework is that it's just really really big trained on a correspondingly huge amount of taxes training 40 gb larger than previous language models have been trained on when you have that size of dataset the only way to get them much tax is essentially to go to the web so one thing open a I put it quite a bit of effort into you when they're developing the snaps ensure that text was pretty high quality play that in a can of interesting way they would get ready which is this website where people can vote on links and then they said if I there's probably further decently Close probably your reasonable tax there for a model to learn ok so we have this super huge language model like gpt2 and the question of what can you actually do with it well I do shift tomorrow you can do English modelling with it and the one thing I'm interesting is that you can run this language tomorrow on existing benchmarks for burning which model get state of your capacity on the Adventure the training data for The Venture say evaluate your own language tomorrow on the Penn treebank you first train on the Penn treebank and then you've alleyway on this holdout set in this case Siri just by virtue of having seen so much text and being such a word model and outperforms all these are there fireworks even those not seeing that data another bunch of different language modelling benchmarks other interesting experiments that open and I ran with this language model zero shot learning this means trying to do a task without ever training on it so you can do this as a language model is by designing a prompt you feed into the language tomorrow and then have it just generate from there and hopefully generate something relevant find a song reading comprehension what you can do is take the contacts paragraph the question to it a poem which is away I guess of telling tomorrow ok you should be producing answer to this question January perhaps it'll generate something that is actually answering the question and it is paying attention to the contacts simile for summarisation you can give you a call then to your Dr and prostamol will produce the submarine you can even do translation where you give the model something list of known in which the French translation for a primate to tell her that it should be doing translation and then you give it a thought sequenceequal run and perhaps generate the sequence in the target language ok so it's a hero the results look like I the x-axis is worth is long scale model size and the y-axis is accuracy find basically correspond ring works on his tasks so for most of his time who is quite a bit below of course is the difference for existing systems are trained specifically to do whatever has there been evaluated on where gpt-2 is only trying to do English modelling and as in language modelling it's sort of picking up on his other tasks simple English to French machine translation not as well as standard and supervised machine translation which is the dotted line play It's daughters daughters quite well one thing have interesting is the trendline right for almost all these tasks performance is getting much better is tomorrow increases in size interesting one of these tasks is machine translation to the question is how could it be doing machine translation when over giving is a bunch of web pages and web pages are almost all in English magically picked up a little bit of machine translation right so it's not a great model but it can store your do a decent job and some kisses if you name corpus of English occasionally within within a corporation see examples of translations right so you see a French idiom ministry inspirational quote from someone who's French in the translation English I'm kind of amazingly I think this big model enough of use examples that it actually starts to wear and how to generate French I wasn't really sure of an intended part of its training interesting thing to do a bit more into is its ability to do question answering so simple baseline for question answering gets about 1% accuracy gpt-2 ml by 4% accuracy so this isn't like you know super amazing we sort question it's so pretty interesting in that if you look at interest the models most confident about you can see that it's sort of his word some facts about the world right so it's worried that Charles Darwin wrote origin of species in the history of Anarchy if you want to get kind of world knowledge into an RV system you need something like a big database of facts even though this is still kind of very early stages and that I'm just a huge gap between 4% accuracy and the 70% or so that a state-of-the-art open-domain question answering systems can do 8 a.m. I can pick up some world knowledge just by reading a lot of text and without kind of explicitly having that knowledge put into the model any questions by the way on gpt2 so far someone question that's interesting thing about it is what happens if tomorrow is get even bigger the Rotunda three scientific thing of drawing from lines in PowerPoint and see where we meet holds at about 1 trillion metres we get to human level reading comprehension performance if that's true would be really astonishing actually do it back that I want to prove Ian primary model would be attainable and I don't know 10 years or so play the train isn't so if you look at some frustration for example it seems like performance is already took down so I think this will be really interesting thing kind of going forward I'm looking at the future of Anarchy is how the scaling will change the way and LPS approached the other interesting thing about gpt2 was its reaction from the media and also from other researchers the real cause of a lot of the controversy about it was the semen from open the eye they said that we're not going to release or full English model because it's too dangerous in our language models too good the media really enjoyed this and you know so that machine learning is going to break the internet interesting reaction from our researchers right so it's some kind of tongue-in-cheek responses you know training model and Miss is it too dangerous for me to release we don't really great work but we can't really say it is too dangerous that you just have to trust us on this working at Moore Cardiff reasoned debate about this issue I'm you still see articles arguing besides so visit you are articles from the gradient which is a sort of machine learning newsletter are you doing precisely opposite sides of this issue should it be released or not I guess I can briefly go over a few arguments for against debate about this I don't wanna go to deep into a controversial issue there is a long list of kind of things people said about this right so here's why you should Ruiz that is this model really that special there's nothing you going on here it's just 10 times bigger than previous models instead even if this one isn't released you know in 5 years everybody can train model disco you work at image recognition or images in speech data it ready as possible to synthesise hi we can been saying take images and fix peach what makes the thing different from other systems and speaking about the systems right Photoshopped existed for a long time that we can already convincingly fake images where to adjust I'm wearing that you shouldn't always trust what's in an image because it may have been altered in some way you can say ok photoshopping sis by you cancel the scale on Photoshop and third mass producing fake content-aware you can with the sort of model and they point of the danger of fake news fake reviews in general with AstroTurf in which means basically creating fake user content that supporting of you you want other people to hold something that's already done I'm pretty wide webuy.com companies and governments is what evidence for this but there of course hiring people to write all these comments and news articles that say and we don't want to make a job in easier by producing a machine that could potentially do this I'm not really going to take a side here the store had to beat about this I think the main takeaway here is that has a community on people in machine learning and NLP don't really have a handle on this virus or caught by surprise by opening eyes decision here means that you know the release and figuring out that needs to be done I'm what exactly is responsible to release publicly what kind of research problems should we be working on and so on any questions about in general something arising from the state the question of bbml people by the people making you through decisions or is there a need for more interdisciplinary science we work at play computer security people from social sciences are Express and ethics to look at these decisions cpt2 with definitely one example of where suddenly it seems like our NLP technology has a water pitfalls right where they could be used in a malicious way or they could cause damage what's trend is only going to increase you work at Kinder people working on increasingly people working on really high Stakes applications of NLP and there is often have really big vacations and especially if you think from the angle about bias in fairness so let's go over a couple examples of this why so Samsung areas where this is happening is people looking and ok to work at judicial decision so for example this person I get there or not hiring decisions right so you look at someone's resume you want and I'll be on it and then make a decision automatically should be this as it may not send you some sort of screen can you take the dry your task will be grated by machine a prism also look at that's you know a sometimes very important part of your life when it's when's the task that affects your acceptance into a school what's a is awesome some good size of using machine learning and kinds of can't we can pretty quickly evaluate machine learning system and search out does it have some kind of bias just by running out on a bunch of data and see what it does also can't even more importantly we can fix this kind of problem if it arises right so it's probably easier to fix a machine learning system the screens present days than it is just to fix having no 5000 executives that are highly successful something right so so in this way there is a sore a positive angle and using machine learning and he's high stakes on the other hand it's been pretty well known and I know you are Alexa and bias in fairness that machine learning often reflects bias in the data said it can even amplify biased in the dataset turn up on your feedback loop where a biased algorithm actually worried that the creation of more by Estate is problems with only compound and get worse all of the high-impact decisions that slide there are examples where things have gone awry right so Amazon had some AI that was working as recruiting through Lana Turner to be sexist some kind of early pilots of using AI in the justice system and there's also had in some cases really bad results Marrakech vibrating it's not really a great you know and up your system right so she was an example of an essay that a automatic grading system used by the theory test gives a very high score it's a kind of a salad of big fancy words and that's enough to convince the model that this is a great essay I want to talk about where you can see those really some risks and some pitfalls with using a Nokia technology just chat I think chatbots do you have a side where they can be very beneficial robot is one example is this company that has this chat but you can talk to if you're not feeling too great and not Friday I don't know cheer you up so it's about you know could be a really nice piece of technology that helps people but on the other hand it's a big risk so so one example is Microsoft Research had a chatbot trained on tweets and it started quickly saying racist things not to be poor highlight that as an appeals becoming more effective people seeing opportunities to use it in an increasingly high-stakes decisions and although you know there's some nice there's an appeal to that there's also a lot of risk 4 questions on the social impact of NLP last part of this lecture is working more future research and in particular I think a lot of the current research trends are kind of reactions to birr the question is what is solvent and what we work on next so here are results on the glue benchmark that is completely immobile 10 natural language understanding tasks are you to get an average score across the 10 tasks the two are so the right into Ramos model non supervised trained machine learning systems effect instead use offence senior architectural tiles composite tension we got about 5 points the gains from respect different improves results by about 17 points and we end up being actually what's the human performance on these tasks implication of this that people are wondering about it is this kind of the death of Architecture engineering olive you go work on the default final project have seen a whole bunch of fancy picture showing different architectures for something squad water papers.co proposed some kind of attention mechanism or something like that you need to do any of that right you just trainer transformer and you give enough data and actually you're doing great on squad you know maybe these architecture enhance men's are not necessarily the key thing that will drive progress in improving results on his tasks the prospective research or you can sing a researcher will say ok I can spend six months designing a fancy new architecture for squad if I do a good job maybe improve result by 1 iPhone increasing the size of the model three acts which is the difference between there like a base size model and a large model Grand Prix results by 5 F1 points suggest me just a re-prioritise which avenues of research we presume because architecture engineering is in providing kind of gains for a time investment two-way leveraging unlabeled data is if you're going to M2 board I think it needs the top 20 entrance or allbart plus something issue I think it has raised is that we need her tasks right but has almost all squad if you do find it by getting close to human performance so there's been a growth in new data sets that are more challenging and a couple ways in which and they can be more challenging so one is doing reading comprehension and one word documents are doing it across more than one document what area is looking at coming up with harder questions that require multihop reasoning sweet news me and you have to string together multiple supporting facts from different places to produce the correct answer another area situation question in string within a dialogue small detail with the construction of Reading comprehension datasets that is actually really affected the difficulty of the task and that is whether when you create these datasets questions about a passage can be seen at passenger not it's much easier to come up with a question that when you see the passage and if you come up with a question without seeing the car Javier instabook question the problem with looking at the passage is that first of all it's not realistic right so if I'm asking a question you know I'm not going to have usually the paragraph that answers that question sitting in front of me on top of that area included easy questions right so if you're a mechanical Turk and you're paid to write as many questions as possible and then you'll see an article that says I don't know your Abraham Lincoln was the 16th president of the United States what you going to write as your question you can write who was the 16th president United States You Knocking write something more interesting as hardly answer is one way in which cards were status of changed people are now making sure questions are third independent other contacts go for a couple new dataset in the science of one is called quack which dance requesting in string in context the status there is a teacher and a student the teacher Caesar Wikipedia article the student was so worried about this Wikipedia article and the goal is to train a machine learning model that as a teacher maybe in the future at the thought of technology would be useful for education for Cannock adding adding some automation that makes this difficult is that a questions depend on the entire history of the conversation example if you work on the left here at the example the three question is was he the star on poorly you can't answer that question and less you work back over here in the dialogue and realised that the subject of this conversation is Daffy Duck because the same as more challenging you can see those that there's a much bigger gap the human performance right so if you trained some Burt with an extension there is also like 15 F1 points worth in human performance 108a that ankle hot pack you a it is designed instead for Monday reasoning so essentially in order to answer a question you have to work at multiple documents you have to look at different factions documents and perform some influence on to get what the correct answer is this is a mi heart attack again there's a much bigger gap between human performance questions on new datasets tasks for an obby I'm gonna rapidfire go through a couple more areas in the last minute of this talk so that has learning I think is really growing importance of course you have Alexa on this rights and Marks and Spencer much time on wine point of interest is that if you work at performance on the screw benchmarks that is benchmark for natural language understanding all the top that are now actually it's a passing bird in performance is taking bird and training in a multitask way I think another interesting motivation for multitask learning is that if you are training birth you have a really really worried tomorrow and one way to make more efficient use of that model is training it to do many things at once another area that definitely important and everything will be important going in the future is dealing with Will resource settings and I'm using a really broad definition of resources right so they couldn't compute power you know Bert is great but it also takes huge amount of computer run out there's not realistic to say if you're going what's a mobile an app for mobile device immoral the size of bird as I already know where we first language is this an area that I think is pretty underrepresented in NLP research right now because my status esta in English but I do think there's a really no words number of people that in order to benefit from NLP technology will need to have technologies that work well in a lot of different languages especially those of how much training data speaking of what were my training data I think in general is an interesting area please machine learning actually people are working a lot on this as well I'm so thermosoftening a term often used is few shot learning essentially means be able to training machinery model that only sees that's a fiver can examples motivation we're distinction between how our existing machine learning systems word and how humans learn is that humans can generalize very quickly from fibrous examples with your training on your own that you normally need you know thousands of examples of perhaps even tens of thousands hundreds of thousands examples to get something that works turn off the TV being a pretty important area in the future I want to go in November that is interpreting and understanding model it's a really there's two aspects of the if I have a machine learning model and make a prediction I'd like to be able to know why did it make that predictions so get some rational get some explanation I would especially be important in an area like Healthcare right so if your doctor and you're making a decision it's probably not good enough for your machine learning model to say patient has disease acts you really wanted to say patient has Aziz Oxford his reason you with the doctor can we machines thinking a gas to come up with a diagnosis the other area of interpreting understanding models is more of a scientific question right is we know things like Bert work really well we want to know why do they work well what aspects of language do they model and what things don't they model and that my me to ideas of improving there is does models so I'm here is a couple of slides on the main approach for a value answering visa scientific questions what does a machine learning model is you have a model for his birth it takes into a sequence of words output sequence of vectors what ask does it know for example the part of speech of word so&so does in its Dr representations does a capture something about syntax asking this question is train another classifier on top of train to do website speech tagging but we only back prop into that diagnostic classifier it's the output of birth that sequence of vectors as a fixed input and we'll sort of probing those factors to see do they contain information about part of speech that there's s diagnostic classifier on top and candy code on to get the correct way bus so it's kind of quite a few concerns here when is if you make your diagnostic Pacifier too complicated you can just sort the cost of the task call on itself and I can basically ignore whatever representations were produced by Burt I'm so so the kind of thing right now it's use a single softmax wear on top of birth and to give his decisions and there's been a whole Match Attax propose for evaluating essentially the linguistic knowledge of these models speech tagging you could do more semantic tasks like a relation extraction or something like a reference and this is a pretty active area work here is just one a plot showing some of the results approach so here we doing is re-adding diagnostic classifiers to different ways of birth we are seeing which where is Alberto more useful for a particular task and I'm something else interesting comes out of this which is that the different where is of birds seem to be corresponding fairly well with notions of different where is linguistics dependency parsing which is a syntactic task consider yourself Media mobile task in understanding at sentence the medium where is a bird so where is kind of 63 or something or the ones best at dependency person fermented cask like sentiment analysis and what you trying to oneself properly before sentence the very last where is the first or the one that seem to include the most information about about this phenomenon I'm waiting for the talk I just have 15 here Knighton kind of the Academic Resources contact with severe even talking about and I've been industry and really there's rapid progress there and I want to point you to areas where I think there's especially the word interesting using NLP technology how are you I'm still things like chatbots writers the Alexa prize where they actually investing a lot of money in having groups figure out how to improve chit-chat dialogue so I think a lot of potential for customer service right so improving basically automated system that all you know book you a flight to help you cancel subscription or anything like that That's Amore there's water potential Healthcare one is understanding the records of someone who the help of diagnosis is I think another equally important area is actually pursing biomedical papers so the number of biomedical papers that are being written is really insane on its way wider than the number of computer science papers are being written if you were a doctor if your research in medicine you might want to look up something very specific right you might want to know what is the effect of this particular drug on this particular Gene or cell with this particular Gene no good way right now I'm searching for you hundreds of thousands of papers to find if someone has it has done this experiment and have results for this particular combination of things and so automated reading of all this biomedical literature and clever what about you progress in the last 5 years due to Deep learning in and ok in the last year we see another really kind of increasing the capability vs thinks using unlabeled as matters like and I'm the other kind think about is that NLP systems are starting to be at a place where they can have big social impact that makes some issues like bias and security very important thank you good luck finishing all your projects 