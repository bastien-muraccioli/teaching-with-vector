so I'm delighted to introduce our mouths have invited speakers and so we can have two and five minutes because I'm today so starting off how are we going to have a cifas one who's going to be talking about self-attention for generative models and in particular Bob introduce some of the work on Transformers that he has loan.for along with his colleague and then as a sort of addition and talking about the applications of this work a couple of people in the classroom the interest and music application so this will be your one chance of course to see music applications of teeth whitening ok so thanks and it's actually here to make the class lessons also issues the highlights hi everyone this is a bus speaker no pressure so hopefully disabled as well the talk is going to be about self attention is model as a cistern consumer machine learning to apply to various tasks what's starting point always ask question structure my dataset were the cemeteries in my data setting and is there a model that exists that that's a very good that there has been inducted by system model these properties are existing my dataset hopefully over the course of this dyslexia and I will convince you that self-attention indeed us have some has the booty models of inductive biases that potentially could be useful for the problems that you cared the stock is going to be learning representations primarily of for variable-length data will be images available and and always care about this problem because deep learning representation learning and learning representations is this is an important Factor in achieving critical success are the models of choice primary workhorse for indoraptor this point of in recurrent neural networks people here are from the liver tolerance is find the primary focus of indigo Netflix and some of them or some variants that explicitly add multiplicative interactions like a list and also they also have mechanisms that allow for better breathing transfer and some recent variants like recurrent units of simplification their kind of the current landscape and typically how to return your networks learn or a stranger a sentence imagine in a particular sequence and position at the time step to produce a continuous representation that's a summarisation of of everything that they've actually punched through in the in the room of large data arrow Models is this quite it's quite beneficial actually reading Oliver Selfridge MIT and sorry read them 2D political pandemonium I would recommend everybody to read and has this fascinating note that you know she give me more parallel computational just add more data and make it slower so you can consume more data and recurrence recurrent sort of just by construction position because you have to have to wait until you're waiting for a particular time point to produce the representation please raise Your Hands hopefully the ground attention and again and now because we actually producing is representations resource summarising you know if you want to pass information if you want a passport reference information inside of fixed size factors that could potentially be Wilder V successful in language or Express doesn't have a very clear explicit with a model hierarchy which is Which is something that's very important Devon Devon it is an excellent work yourself attention that actually surmounted some of these difficulties or learning difficulties basically as a compositional sequence models where you have these limited receptive field convolutions that that again consume the sentence now not sequentially is representation Square they produce representations of the available in sequences the truly paralyse because you can apply this conversation simultaneously at every position is solaris to build to paralyze the dependencies only in the number of letters you can get you can that you can get this local dependencies efficiently because there's a single application of a convolution can consume all the information inside its local receptive now if you want to have these really long-distance interactions while you don't have to pass through your number of steps you still because these because these receptacles are local you you might need something like Linda and something like a dilated convolutions so they still need the number of layers that are needed or so a function of the length of the of the orchestra will be developing and actually pushed a lot of research like way Barnet EN4 examples of classics of success story of convolutional sequence models even b attention has been incontinence continence base unit memory retrieval mechanism in its contents because you have that I tend to always content that you encounter and then just decide what to what was what information absorbed based on customer of this content is to every position in the memory bakery called mechanism in India machine translation of the question of the assholes like white wine or just use attention for representations now framework of this representation mechanism essentially now imagine you have you want to represent a word represent the word represent you would construct its new representation and then you attend or you compare yourself you can pay your content in the beginning I could just get one in bed you can get accountant with all your words volume bearings and based on this based on these compatibility of these comparisons you produce you produce a weighted combination of your entire neighbour a combination you you summarise all the information just like a way expressing yourself in certain times a weighted combination of your entire neighborhood that's what attention does four letters to basically sort of computer new features for you no is going to be a bad how I'll be the self-attention actually help us in text generation like what kind of devices are actually useful and the empirically show that indeed the movie Little in text generation and this is going to be rubbish in translation for the other work also that we calculator now with this with the song mechanism we get a car are pears a position in any position every position simultaneous the number for S is not to me just by virtue of like it's construction you have a soft Maxi-Cosi is a deity and multiple interactions and again I'm not going to be able to explain why but it's interesting like you see these models like even even the even pixel pixelcnn or when it was actually morning image of the explicitly had to add these multiplicative interaction inside the model 22 basically the ordinance and attention just like construction gets this because you're you're multiplying the attention probabilities with your with your activations it's to build a paralysed why because you can just do a tension with Matt most especially the Variant that by using our paper another question is convolutional sequence models have been very successful in in general tasks for text can we actually do the same as she's the same with as a primary workforce for representation suggest two text and Islington distance up to the Transformers in a lot of great work on using self-attention primarily for classification within open self-attention within the confines Ladbrokes perhaps the closest is memory networks by Weston-super-Mare the actually had a version of recurrent attention but they didn't have but they didn't actually didn't show it to work on conditional modelling like the translation and their mechanism was using server fixed them using a fixed query at every step leave something to be desired this this question is that actually going to work undone large-scale machine translation systems or large-scale text generation systems the combination of attention attention work this is and put it together in the transformer model so how does this look like greetings attention attention primary for completing representations of your input imagineering English to German translation to have your words attention is communication environment so you just change the order of your positions you change the order of your words and and it's not going to affect the actual output so not in order maintain order of the position representations and there's two kinds of betrayed in the paper this is fantastic sinusoids with no she invented and we also use learning representations which are very plain vanilla and Sophie have some input looks as follows so we have a self attention there then just free computer or presentation reposition simultaneously using my pension then there will be four layer and the author of residual a residual connections and I'll sort of give you a glimpse of what is residual connections might be bringing that is between every and The Entity of a skip connection that just add the activation cristobal offer self-attention in FIFA does essentially repeat inside the sort of standard encoder-decoder architecture on the Dakota side be mimic of language model using self attention and the widowmaker language model using self-attention estate in post causality by just masking out the positions that you can look at so basically it can't look forward to look forward to look at itself because we actually shift the end so it's not copy at one point when early on it was in heart new coping with record models with 19 sticking copy really well which is the positive sign I think overall The Dakota side we have a potential are followed by encoder-decoder attention where we actually attend to the last layer of the encoder and a few Ford liar and this repeated a few times and at the end of the standard cross entropy loss and em search staring at the carpet and the particular variant of the of the attention mechanism and the use we went we went for simplicity and speed so how do you actually computer tension so imagine you want to represent position E2 we're going to firstly linearly transformed into Lily transform every position in your neighborhood or the server position of the input this is the end destinationskin actually thought it's features and I'll talk about it later on so it's like it's basically a bilinear form you're protecting these vectors into a space for doctors a good we're just a dog park has a good proxy for similarity as soon as your socks off next computer convex combination and I'll based on this convex combination you're going to e2orr in terms of this complex combination of all the actors of all these positions and before doing before doing the convex combination you can do a linear transformation to produce values and then we use second linear transformation just to fix this information and passing through passing through a feed for layer and this is can be expressed basically into into into my multiplications and the square factors just to make sure that the is.tax don't blow up it's just a skilling fact and then why is this particular why is this attractive what is this really fast you can do this very quickly and a GPU and sent me to do a simultaneously for all positions with just two metals and a soft Max it's 6 active the same except being posted causality by just adding into the logic so it it's just that you just get 0 probably Nando's postcards led.by by adding Deezer highly negative values on the extension on the extension leads so ok so attention is really attention because this friend of attention is involved to invoice to matrix multiplications it's quadratic in the length of your secrets and now what's the computational profile of ions are convolutions their quadratic in the you can just think of a convolution just Black Mirror but it just applying a linear transformation on top of it actually become very attractive this becomes very very attractive venue dimension is play some machine translation now we will talk like a cylinder when when this is not true and get her to do what we have to make as a model but for short sequences or sequences with your legs does a dimension dominus length attention zaveri has a very favorable competition profile as you can see it about 4 times faster than an ironing faster than a convolutional model where that you have a cuddle of flight filter with three do something so it language typically want to know the liquid what to write filter because you actually have different linear transformations based on Let it Roll out of distances like this this linear transformation on the word who on the concert we can have this concept the phone and then pick up different information from the embedding of the word I and the slimmer transformation transformation can pick up different information pulling a transformation can pick a different different information from boot when you have a single attention layer because they just a complex combination we have the same linear transformation everywhere all it's available to just is just mixing proportions you can pick up different pieces of information from different places we had one attention layer something like that feature detector on mostly because I particularly it might try to invite because it carries with it a linear transformations it's protecting them in the space that which starts caring maybe about syntaxes predicting the space with starts going but who or what then we can have another attention layer for attention head for what did what in another tension head for all of this can actually be done in parallel and that's actually and exactly what we do and proficiency instead of actually having these dimensions operating in a large space we just we just reduce the dimensionality of all these heads and the operate these attention layers in Paris or Bridgend can you actually combination of heads cigarettes and which you can actually exactly simulator convolution probably with more parameters I think they should be as simple as that if you cancer headwear function of positions you could probably just simulator convolution with other with a lot of planets in in the limiter can actually simulator convolute also if you cannot we can continue to enjoy the benefits of parallelism but we didn't read the number of Staff Nexus because each and then carries with it the soft Mac's but I might have lost it and change it because we instead of actually having his head's operated very large dimensions they are putting in very small dimensions so I'll ring you play the sun on machine translation be able to automatically outperform previous results on English German English French translation to have a pretty standard setup 2000 word vocabulary workpiece encodings ww2 2014 2013the deaths and some of these results were much stronger given our previous ensemble models also we had some very favorable results and the India vs Australia the stepping back a bit I'm not claiming that driver architecture that is better Express series and lstm in there's there's there's the serums that the model any function all we did was just building architecture that was good for as you the St Regis train this architecturally well because the gradient Dynamics inattention or a simple sentence as a linear combination I think it's I think that's actually favorable but hopefully as as we go on but what I also like to point out the video explicit explicitly model of all past connection all pairwise connections and it has had the advantage of very clear modelling very clear relationship directly between between any two words other inductive devices that is not just architectures that that are good for that are good and active by Sister Sed so frameworks a lot of our work was initially posted intensity 10 sorry maybe that might change in the future with the arrival of Jack's is in there's a framework also from Amazon cos there's also fair sick does the convolution sequence-to-sequence Torquay from Facebook that the problem I'm actually not sure if it is a transformer implementation tomorrow as well what does residual cell residual connections between little connections that go from here here here here here like between every pair of letters and it's interesting so what we do is we just add the position information and the input to the model inject position information everywhere we 7-days residual connections and he looks terrible detention distribution centre in the middle of the potential distribution you actually basically it's been unable to pick his diagnosis should have a very strong diagonal and so what has happened with these residuals were carrying this position information that every layer and because these subsequent had no notion of position they were in your heart to actually attend this is the important potential recipients of being egg ok so then we actually continued continued to serve the residuals will be added position information back in every love injected position information back in and we didn't recover the accuracy but we did get some of this sort of diagnose Focus Back in soon as they're certainly it definitely moving this position information to the model there's something this position information for that was so that you're not been able to sort model but long and short short term relationships along long and short distance relationships with with the tension is beneficial for for text generation what kind of an inductive Isis actually appear an images of something that because of the seat possibly seeing images of music is this it's very similar to each other you have these motifs are repeating in different scales so for example as another artificial but beautiful example self-similarity where you this texture of these little objects just repeat these images of different pieces of the image of a similar to each other but they might have different scales again in music has a motif that repeats they could have it could have expensive time between in between so so retweet centred after the Civil Justice question can self-attention help person modelling other objects like images paprika store a standard autoregressive image modeling the probabilistic modelling not against because when it was very easy had a language model and most of this is just like language more than images training of maximum likelihood allows you to sort of measure measure how well you're doing on on the Underhill doubts as you diversity so hopefully you're covering all possible to this point is we had a dentist that also been there are there been good work on using the current models like the salon and pixelcnn that getting some very good compression rate sure the argument was because because you want symmetry because you won't like the other face you want you want 1-year deserve not with the other if you had a large receptive field can you get with the tension at a lower computational cost then it should benefit in it and then it'll be quite beneficial for for images for images and you wouldn't need many layers like you doing contributions to actually get dependencies between this far away pixels so seem like so fat and she would have been was already a good computational but it was actually interesting to see however even modelled naturally models of somebody and people use of somebody animation-direction like you know this is really cool work by afros where they actually see ok in the training said what are these patches that are really that are very similar to me and based on the patches that are really similar to me I'm going to fill up the information so it's like actually doing images generation really classic were called non-local means denoising where they wanted to know is this sort of this patch and they say I'm going to based on my similarity between all other patches in my image and go to compute sum function of content-based similarity and based on the similarity I'm going to pull information exploring expect an images of very similar this is also been applied in some recent importance of attention mechanism just replaces wearing a patch this notion of content-based similarity between these elements and then based on this content is somebody constructs a convex combination that essentially brings these things together so it's a very nice it was it was quite it was very pleasant to see that this is a different way of doing on local mean stop transformer architecture and replaced words there's some there some architecture adjustments to do so this is basically the kind of it was very similar to the original work and hear the position representations instead of being you know one-dimensional do because you're going to deliver presentations quotation is a very very favourable computational profile dominus linked with which is absolutely untrue for absolutely untrue for images 30274 32/33 when you flatten them and have your flat number 32 you get 372 positions sanity for image solution I mean you could come listen to basically little local windows and get translation like variance you said ok same strategy there's a lot of spatial locality in images we will still have a better computational profile if you're if you're receptive field is still smaller than your dimension you can afford you can actually still do much more long distance computation than a standard convolution because your because your quadratic in increase Ireland to be on the dimension we still had a favourable computational profile I need it was essentially had two kinds of rasterization national restoration where are reserve single which was sending or to the into a larger memory straight fashion along along along the Rose another for my best position for extended to the National locality maybe actually produce the image in blocks and within each block we had a restoration scheme images for malaria was very similar to the opposition representations along with the same with the personal attention betrayed but super-resolution an unconditional and conditional image generation the Keeper Mara I and the cup and a few other offers strong and my present better perplexity than existing models the pixel snail is actually another model that used to export contributions on self attention and are they are profundus on on on on on bits for dimension flexity because it's a probabilistic not as a probabilistic models like basically language model of images in and it just and your end of factorisation of your language model does depends on how your last price in the in the in the 10th position when first rows and columns in The Telegraph position we went clockwise and inside each block the rest YouTube perplexities I'm good at again level right I mean this is I think probably generation goodnight rescan 2019 there's a paper binaural that actually use a self attention and gets very very good quality images but what what be observed was we were getting structured objects fairly well like the second rules what's the cars in northern asked something else videos their cars here and civil and the last drawer is another vehicles like so it's actually structure structure on just easier to catch is there a camouflage Justin does match Enda Byrne super-resolution of super-resolution is interesting because of our conditioning information when you have a lot of condition information that the sort of possible if you break you you actually quite a few of the modes there's only a few options you can have the output and super are super resolution results so much better be able to till orientation in structure than previous clothes at different temperatures and quantify this reduction human evaluators flashing emergency as real as false and we were able to be able to fool humans like 4 times better than previous results on Super resolution you are not these results are like I guess the list latest can result from NVIDIA a joke but I mean this is starting later than getting so hopefully we'll catch up but but the point here is that this is an interesting inductive bias for my natural inductive bias for images for playing and classification and other such tasks just out of curiosity and asking how good is maximum allowed does maximum likelihood one does the model actually capture something to stretch on the diversity with maximum likelihood sugared eversleep by work completion completion because as soon as you lock done after missed the goal truth you actually setting off a lot of the possible modes so much easier time stamp the first first what is supplied to the model that the right the right must confess is gold and we were able to generate different samples was really interesting is a third row so the rightmost column is the rightmost column is good dispersed the actually there's a sort of a glimpse of a suggestion of a what's the name of the human in some of these in some of these images which is interesting like an indus casually the data teaches at Decatur some structure about the world cute and I guess it's also shows that you know there's an entire object this chair that the model has completely refused to imagine so there is a lot of difficulty talk about 2 the way you explained to Samsung gallery ashish feather introduction self-similarity and images there's also a lot of similarity in a music so we can imagine transformer being a good model for we're gonna show how we can add more to to the surface tension to think more about can a relational information and how that could help music generation I want to clarify what is the representation that we're working with right now so and Alice goes to language you can think about those text and somebody is reading out a text so they at their own intonations to and then you have sound waves coming out at sea Samoa kind of kind of generation where you say the composer has an idea right down the score and then performer perform and then you get so focus on today as mostly give the score but it's actually a performance representation where pianos were used and professional amateur musicians were performing on the pianos so we have the recorded information of their playing in particular at least I'm stuck modelling music as this sequence show process what is being alcohol are ok turn there's no line advanced the clock by the smarter than there's no laugh and also there is dynamics information so when you turn on your first day like cows national modelling a music as Catalan are recurrent neural networks because as a suit introduced and talked about the impression that needs to have it like a long sequence has to be and vetted into like a fixed length vector and that becomes hard when music you have repetition the distance so I'm going to show you samples from from the ions from a transformer and then from music has one that has the runs retention and has that you hear that difference and then I'll go and to Howie weather modification to do on top of that call my mother here are this task is kind of the image completion task so we give it an initial teeth and then we asked the model to continuations that we fed recognised Chopin a 2-piece and already gonna ask the ironing to do a continuation like in the beginning I was trying to repeat it but very fast at the wonder and it's a different ideas because direct me to go back to what happened in the car just look at the kind of blurry version and that blurry vision becomes more and more blurry what the Transformers a detail is at these models are trained on half the links that you're hearing so we can asking them to generalize beyond the like that it's train on and you can see this form deteriorate be on that multi pretty consistent you get you get that idea initially I was able to do this repetition really well copy very well beyond that was trained on it can I do know how to cope with like longer contacts the last one is from the music that's what I think that kind of the relational information and you can just see visually how it's very consistent and kind of repeating these larger music transformer the self-similarity that we talked about the motif is also there we primed the model with MOT and this is actually a sample and conditions sample from the model so nothing there was no priming that the moment had to create its own no teeth and then do considerations from there and here if we can look at it and analyse ab you see a lot of a repetition with gaps in between self-attention structure you see the model looking at the relevant parts even if it was not immediately preceding what are coloured shaded out is where the motif and you can see the different colours is different attention home and they're kind of focusing Mungo's a great at sections and we also have a visualisation that can you as the music is being played a what no it was attending to as it was predicting that no generator from self-attention is no 20 level event event love also it's quite low level so when you look at it it's a little bit overwhelming it has like multiple heads in a lot of things moving but there is kind of you structure a moment where you were Tennessee more of this clean sections how how do we do that play from Canada the regular attention mechanism we know it's a weighted average of the past history and the nice thing is however far it is we have direct access to it so if we know there are penalties that occurred in an early on in the police were you able to based things similar to be able to refill it also becomes all the past becomes kind of a bag of words like there is no structure of which came before after so there is the position of dinosaurs that is she's talking about that basically in this in the indexes into a Santa slides are moving at the front seat positions which have a very similar cross-section and to those multiple sinus in contrast evolutions you can have the suffix filter that's moving around that I can't resolve relative distance like one before 2 before kind of rigid structure that allows you do bring in the formation brakes Passat you can imagine relative attention warheads play to be some combination of these so on one hand you can access the history very directly on the other hand you also know how you relate to this history capturing 3 calculate translational invariance and I think one of the reasons why and the beginning and prime examples that you heard that the was able to generate the link that was trained on at a very coherent way is that it's able to have to rely on this translational invariance to to carry the relational information forward take a closer look at how how how how this works is the regular transformer you have you compare all the queries and keys so you get kind of this Square matrix you can think of it as a case of similarity matrix so it's a Square does is to add an additional term that thinks about whenever you are comparing two things how far are you a pie do I do I care about things that are 2 Steps Away or 3 stops away or maybe care about things that are occurring at periodical distance information gathered that influences the similarity between positions particular at this extra term is based on the distance so you want to gal them better that did the Craigie distances on the underlords PlayStation this has shown a lot of improvement in listen to German translation in Translation the sequences are usually quite sure it's only a sentence maybe 50 Words 100 words but the music examples that you've heard or in the range of 2000 times of so it's like 2000 tokens need to be able to fit in memory goblin because the original formulation relied on 3D tensor that's why is this is the case it's because for every pair you look up with there was thinking computer the relative distance and then you look up an embedding that corresponds to that distance so feel like this there's a link by link lyl make sure if you need like settings for each of the positions and that's a d is the 3D you can actually just directly x the queries and betting distances hello can I have in a different order because now you have the previous order by relative distance that you need the craziest ordered by absolute by absolute so what we could do is just do a series of stewing to to put it into the right decoration and just a quick contrast to show the difference of memory requirements challenges is in being able to skill about being able to be more memory efficient so that you can borrow long as sequences ibises play when were sample if we have time but if we don't have time we can go ahead this isn't maybe I won about one minute sample and I thanks Anna attention has been mechanism for music Southampton machine translation one really interesting consequence of one really interesting consequence of roles of attention in an images evolutions achieved convolutions of sheep translation equiv so if you have litter you daughter this feature that you're computing at this red dot it doesn't depend on whether so much of the dog is in the air and the larger image of independent is absolute location this winter is going to be the same activation so you had got a conclusion of this nice translation the Queens with relative positions of relative attention you get exactly the same effect because you don't have any once you just remove this notion of absolute position that you're injecting into the mod remove that then your attention competition because it actually includes a couple of actually actually working on images and themes and its instruction show better results this satisfies this achieve translation equivalent to the great property for images so there's a lot of it seems like this might be an interesting direction to pursue if you want to push self-attention images for herself supervisor Princess of Wales learning the gender of modelling work that I talked about before I in itself just having probably stick models of images get the best model of an images Google Search and I pick up an original just give it to you but I can see the model images of you so because if you want to do something like self-supervised learning we just be training model on overrun a lot of and label that you transfer so hopefully this is going to helping this is going to be a part of that machinery interesting another interesting structure their drill to attention lousy the model is this is kind of a graph so imagine you had the similarity graph where are these relatives or other notion of companies in the US is not enough fruit I'll take these two forms and you can just imagine relative attention just modelling this just been able to model or being able to yourself being able to impose these different oceans of similar cleaning between oven elements are so if you have if you have back problems then drop some potential might be good for a position paper by battaglia or from deepmind talks about drugs of attention and how it can be used within graphs Wiley on gratitude that might be interesting to connect some excellent work done on passing your networks and it's quite funny so if you look at it if you look at the message passing function what is saying is we actually just passing messages between Paris and no it's ok just think of self attention is imposing a fully can say goodbye you're passing messages between you're passing messages between nose and message-passing message passing your network to exactly that they are passing messages between ones as well and how are they different will the only way that mean mathematically they're only different and that message passing forcing the messages to be between pairs of notice but just because of the softmax function where you get interaction between all the notes self-attention is like a message passing mechanism with the interaction between all notes so they're they're not too far mathematically and also the the message this is an interesting concept or multiple cars are similar to multi-head attention the Normans it's like a run keep copies of the message passing your networks in parallel so there's a lot of similarity between existing you disconnect to the existing before this connection's or came in later play blueberry kind of connected these both strands message passing in the thesaurus summarise is itself attention have been able to help us model is this constant Catholic positions in a Mitsubishi going to be quite useful in in in sequence modelling having unbounded memory night having to pack information in finite in and sort of finite model in a fixed amount of space we're on in Arkansas remember recently goes with the sequences is it is how to computationally struggled to paralyze you can you can count a lot of data which is useful for data sets self-similarity very natural thing very natural phenomena if you're dealing with images or music 0 circles with added Dimension of being able to my list of timing in music variance accessory naturally the grass this part everything that talk to farmers about reactivate your research now using these self-attention models for for less autoregressive generation to notice a generation time note of the decoder Masters causal you could look into the future so when you when you're generating was still generating sequentially left to right on the target site so nny why is generation Hardwell because you are putting multimodal if you had different translate English to German there are multiple ways and an end your your second word that you're translating will depend on the first word for example if you if you first the first then that's going to change the second word to predict and it just predicted them independently they can imagine you can have all sorts of permutations of these which will be incorrect where we actually break Moses this or we make decisions is just equal generation once we commit to word that that makes a decision and then that nails done what's the next Word document there's some work on an active research area and you can kind of categories some of these papers like another address of transformer of the fastest paper fast decoding the first people to us a better understanding of other countries are included into the group where they're actually doing the decision-making in a little space that's been it's either being learnt using word alignment of fatalities or this being learnt make you do the decision making in Leyton space and then you once you've made the decision to Leyton space you soon they're all your outfits are actually conditionally independent given that you've made this decision so that's how we actually speed up there's also there's another another paper the second one is paper refinement is also black white powder coating paper by mitchelston essentially just rang multiple models like and rescore using more using a faster model in score expensive mod speed it up the potential has been beneficial transferred running GPT from open the and examples of Vincent Morgan actually scaling like artefact optimizer distance piece of paper by Rohan Anil in your arms singer tensorflow which actually the train several orders of magnitude larger than the original models that beat are you working this large data retrieval probably going to memorise or memorize a lot of things that your friend has just been or tomorrow again that you do that universal Transformers sort of recurrent neural networks and actually can't very nicely as if you this new papers by actually probably actually sugar into the cell mechanism just learnt how to counter like if you can learn about the end the end with a lithium so then universal Transformers brings back recurrence in-depth inside the transformer Wikipedia paper simultaneously with the user's local attention transformer acceleracers combines recurrence with some potential to disrupt attention in Chelmsford this or summarise history by using recurrent acute using speech but I don't know if there's been some really big success stories of sulphur tension in speech shoes that have a large positions to my to-do self-attention number self supervision as if it works it would be it would be it would be very beneficial we would need large label datasets intertransverse becoming becoming is becoming NLP with Burton somebody's at the models understanding out here's what actually happened search for me and a couple of operators multitask Lilian surrounding this self attention is an interesting area of research that I'd like to pursue play 