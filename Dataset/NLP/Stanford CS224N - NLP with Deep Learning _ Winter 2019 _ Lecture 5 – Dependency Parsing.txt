play let's get started again welcome back to tn3 of cs224n ok sorry we've got a bit of a change of pace today after week to so this week and week 3 we're actually going to have some human language and so this lecture has no partial derivative signs in and so will be moving away from side of working out the so take the Cowardly is of doing your networks and backpropagation and the sort of met heavy week 2 and so then this week what we actually wants well in today's like chili one look as well what kind of structures do human language sentences have and how we can build models that kind of structure for sentences that we say I'm going to explain and motivate a bit about and structure of human language sentences so that's kind of like linguistics in 20-minutes or something and then going particularly focusing on dependency grammar and then going to present a method for doing dependency structure dependency g of Passing called transition based dependency parsing and then talk about fancy puzzles I'm going onto a couple of announcements so assignment 2 was due 1 minutes ago so I hope everyone succeeded in getting assignment to out of the way if you still working on at to make sure to make use of the office hours and get help for their coming out just today is assignment 3 assignment 3 basically about this lecture I'm so in a stuffing and 3 what you're doing is building a new or dependency parser and so we hope that you can put together what you learnt about me all networks last week and the contents of the day and jump straight riding to Building and you'll dependency parser the other thing that happens in the Silent 3 is that we start using a deep learning framework pytorch sofa doing assignment 3 instructions and this is in the PDF to the assignment is to install pytorch as a python package and start using that so we attempted to make assignment 3 sort of beer highly scaffold tutorial where you can start learn how to do things and pytorch I just writing a few lines of code at a time hopefully that works out for people and if you have any issues with that well obviously even sent a message has come to office hours I mean and one other thing you could think of doing is this sort of a one-hour introduction to pytorch on the pytorch site where you down where you directed for installing pytorch and you could also look at that that was maybe helpful mentions yeah so I'm final project some you know we're going to sort of focus on those more week 5 but it's not bad to be thinking about things you could do if you want your custom final project you're selling carrier to come and talk to me or the two we have under the sort of office hours page on the website a listing of the expertise of some of the different tiers I miss my office hours yesterday I'm going to have a shortened office hour tomorrow from 1 to 20 that's at the same time as the PS2 24-in for sale so you can kind of come for any reason you want but it might be especially good to come to me if you want to talk about and final projects sleeping and start talking about the structure of sentences and Soph 12 explain something about human language sentence structure and how people think about that structure and what kind of goals then people and natural language processing have of building structured wonder stand the meaning of sentences examples I'm going to give today I in English and because that's the language if you're all expected have some confidence in this really isn't meant to be sort of facts about English this is meant to be sort of ideas and I can think about the structure of human language sentences that are applied to all sorts of languages so in general different ways that linguists have thought about the structure of sentences are some relations to them one of them is called I'm free structure or free structure grammar and if you vaguely remember from cs103 if you did that when you spent her that electron context-free grammars and phrase structure grammar using the tools of context-free grammars to put structures over sentences and so first of all going to just briefly introduces that you've seen that actually the main tool that we're going to use in this class in for Simon three is the deport dependency structures over and sentences so then go about that phrase structure is the say that sentences units for progressively NES so we start off with words that kept etc and then we going to put them in a bigger units that we call Fraser as like the cuddly cat by the door and then you can keep onto buying those up and even bigger phrases like the company cat by the door so how does this work well so the idea of heard and Mrs olive the way linguist thinks is to say well he is this language which you know might be what happened or some other language what kind of structure does it have and Wolf we could look at lots of sentences of the language and south of linguist is going to think well I can see like that had a dog that dog I kept etc so sort of seems like there's one word class here which Lingus often referred to as determinants and they also fed with articles sometimes in English and there's another word class hear of nouns and Soph what I took up to this pattern here it seems like we can make this unit and that I see all over the place in language which is made of a followed by now and so I write I'm a free structure grammar roll text Free Grammar role of I can have a noun phrase that goes through a determiner and a noun you know that's not the only thing that I can see so I can also see samples in my language of the large card or a barking dog or the cuddly care the cuddly dog so that seems that I need to put a bit more stuffed into my grandma's so maybe I can save for my grammar that a noun phrase goes through the terminal and then option where you can put in an adjective and then you can have a noun around a little bit further and I can find it samples like the cat in a crate or a barking dog by the door and I can see lots of Sims does like this entire want to put those into my grandma but they're putting I know there's something special here are some other things things look a lot like the things I start off with so it seems like a phrase with the same expansion potential that nested inside this bigger phrase because these ones can also be expanded right I could have something like the green door something in here in some way so maybe I could say that a noun phrase goes to a determiner of an adjective and something else for charcoal prepositional phrase and then I'm going to write a second rule saying that a prepositional phrase goes to a preposition please word Tia buy a noun phrase and so then I'm really you reusing my noun phrase that I defined up here and so then I could immediately generate other stuff I can sort of say the cat by the store on deed I could say the cat large crate cat5 large crowd on the table or something like that because once I can have a prepositional phrase including noun phrase and a noun phrase include a prepositional phrase I've already got something that I can kind of recursively go back and forth between noun phrases and I can make infinitely big sentences light something like yeah add table the door keep on going and make big sentences and I could say well I've got fitted on this slide but I've gone an Elsa this according to my grandma way that's a noun phrase goes to determiner now prepositional phrase the prepositional phrase goes to a preposition and a noun phrase and it's noun phrase goes to the terminal adjective prepositional phrase preposition another noun phrase and I keep on going and I can produce big synthesis step kind of then continues on because and then start seeing more bits of Grammar so I could say while I can now talk to the cab and so if I want to capture this talking to a cat here or that now means I got a verb cos works like talk and walk and verbs and then talk to the cat seems like after that could become a prepositional phrase and I could ride another rule saying that a verb phrase goes to a verb followed by prepositional phrase and then I can make more bigger sentences like that 4 sentences of the language and start building up this is these context Free Grammar rules to describe the structure of the language what we was do and different languages have different structures so like in this little grandma I've had in general in English and what you do what you find is the prepositional phrases follow the verb that if you go to a different language like Chinese what you find is the prepositional phrase that come before the verb and so we could say ok there a different rules the Chinese and like it start by the context Free Grammar for them beauty I'm a Celebrity idea context-free grammars and you know dominant approach to linguistic structure that if you go onto linguistics past and linguistics department people make these kind of phrase structure grammar trees but just to be contrary no it's not actually just to be contrary it's because this alternative approach has been very dominant in computational linguistics what I'm going to show you instead is the viewpoint of dependency structure so the idea of dependency structure is rather than having these sort of phrasal categories like noun phrases and prepositional phrases and things like that we going to the Ripley represent the structure of sentences by saying how words arguments or modifiers of other words in a recursive function which is another way of saying how their dependence on other words so we have a sentence look in the large crate in the kitchen by the door and if we want to we can give these words word classes so we can still say this is a verb and this is a preposition and this is as a terminal and it's as an adjective down but to represent the structure what we going to say is well luckier is the this whole sentence so that's where things start and then well where we going to look large crate for that is a dependent of luck well will then we have for the crate it's got some modified that's a large crate so that's a dependent of prey isn't the large prayed that the dependence of Craig and in the system of dependencies I'm going to show you we've got is kind modifier of crate in the large crate I can come back to that well the discrete has his own modification because it's a crazy in the kitchen so we have Shan play fire crate in the kitchen in the kitchen the the dependence of Christ then we have this next bit by the door and I'll discuss and minute well what does the by the door modifying it still modifying the crate at st. a crate by the door ok so that by the door is also a dependent of trade and then we've got of dependencies and coming off of it so that's in the straps you get may be drawn a little bit more neatly when I didn't know in advance like this sensory call these things at dependency structure and so crucially what we doing here deadweight samples example what we doing is saying words modify other words and so houses to serve understands have a different parts of the sentence relate to each other overall you know let me just say he might one of what he do we need sentence structure you know the way seems to work when you're talking to your friends is that you just lever something and I understand what you're saying and what goes on be on there and it's sort of not really accessible to consciousness I have machines that interpret language correctly we sorted need to understand the structure of these sentences because unless we know what words that arguments in modifiers have other words we can actually work out what sentence does mean and I'll show some examples of that is to help things go wrong immediately because actually a lot of the time they're different possible interpretations you can have general alcohol is in a up until now it's sort of looked at the meaning of words right we did word vectors and we found out words that there were similar meaning and things like that and you can get somewhere and human languages with just saying words I mean you can say hi just words the way human beings can express ideas and explain 80 things to each other if you can put together words to express more complex meanings and then you can do that over and over again recursively the build up more and more complex meaning so that by the time you're reading the morning newspaper you know most sentences assorted 2030 words long and they're saying some complex meaning like you know overnights in at Republicans resolved that they would not do believable and you understand that flawlessly I just so putting together those meaning to words and so we need to have the know what is connected to what in order to be able to do that ways of seeing that's important is seeing ok so here is a newspaper article send his a cop kills man with knife this has two meanings in the two meanings depend on well what you decide depends on what near what modifies what so what is the two meanings step the guy right the meaning one is the cop steps a guy so what we've got IKEA is we've got the cops the Killing so this is what will say is the subject of kill was the cops and I just called the San Jose cop here and Wild is what they kill which show the the man is an object of Killing then one person is that the cops use in knife to kill the person and so that's then that this is modifier in complex recovered instrumental modifier to say that the cops the killing people with a knife possible analysis ok then is a second meaningless sentence can have the secondary they can have ok s Heaney the sentence can have is it the man has a knife in that case what we want to say as well you know it's this word man and this man has noun modifier is sort of saying something that the man possesses and in this dependencies the same and it's a man with a knife and so the interpretations of these sentences that you can get the PIN on putting different structures over these sentences in terms of who is what is modifying what there's another one that's just like that one scientists can't Wales from space this sentence has two possible structures right that we have the scientists are the subject that accounting and Wales are the object and will one possibility is that this is how they're doing the counting but they're counting the Wales from space using something like a satellite and but the other possibility is that these parts of the Spain this is the subject and this is the object quail from space with she know we could have analysed as a noun phrase goes to on an opp in our constituency grammar vs dependency grammar is saying all this is now and modifier and the tyre Wales from space and the sign to turn up as in the bottom example writer obviously what you want is this one is correct and this one is here wrong answer this choice is referred to as a prepositional phrase attachment ambiguity and it's one of the most common ambiguities in the passing of English right so he is out prepositional phrase from space and so in general when you have prepositional phrases and before it you have verbs and noun phrases or nouns that the prepositional phrase can modify either the things that come beforehand this is a crucial way in which human languages a different from programming languages write in programming languages we have had rules as the hell you meet to interpret things that dangle afterwards right so in programming languages you have an else is always construed with the closest what you are use parentheses or indentation or something like that I get a different type on cos you have to use indentations everything something like see your similar language dried if you haven't used braces to indicate it's just the Terminator clearly the else goes with the closest that's not how human languages are human languages this prepositional phrase can go with anything proceeding and the hero is assumed to be smart enough to work out the right one and you know that such a large part of why human communication is so efficient write like we can do such a good job at communicating with each other because most of the time we don't have to say very much and it's really smart person on the other end and who can interpret the words that we say in the right way so if you have artificial intelligence and smart computers within start to need to build language understanding devices who can also and work on that basis that they can just decide and what would be the right thing for from space to modify and if we have that working really well we can then apply it best to program languages and you could just not putting any braces in your programming languages and the compiler would work out what you mean ok so this is prepositional phrase attachment seems maybe not dead hard there but you know it gets worse I mean this isn't as fun an example that it's a real example of a sentence and from the Wall Street Journal actually the board approved this acquisition by Royal Trust Co Ltd of Toronto 2017 $27 this year at its monthly meeting orange sentence but what is the structure of this sentence well you know we've got a verb here and we've got exactly the same subject and for this noun object coming after what happens after that will hear you've got a prepositional phrase here it's got a prepositional phrase we just got to see for prepositional phrases in a row and so well do a safe for each of these prepositional phrases what they modify in standing off there are only two choices the verb and noun proceeding as before that is going to get more complicated we go in because love there's another noun here and another noun here and another down here once we start getting further in there be more possibility is ok so let's see if we can work it out so by Royal Trust Co Ltd what's that modifying acquisition does not the board approved by Royal Trust Co Ltd it's an acquisition by Royal Trust Co Ltd ok so this one is a dependent of the acquisition ok now E12 of Toronto and we have three choices that could be this this or this ok so of Toronto is modifying it's acquisition of Toronto is there another guess first one is modified royal Trust M Ward Tesco Ltd of Toronto so this of Toronto is a dependent of royal Trust Co Ltd and royal Trust Co Ltd right that's against this noun phrase so it can also have modified by prepositional phrase ok for $27 this year is modifying patient right so now we leap right back up now elite right back and it's now the acquisition that's been modified and then finally we have had its monthly meeting is modifying trying to prove it's a proved at its monthly meeting stop that one on the wrong way round the piano sorry it's been down this way wrong so that we've got this pattern of how things are modifying so actually you know once you start having a lot of things that have choices like this you start if I want to put in now so on to this sentence of to work out the the right structure I have to potentially consider an exponential number of possible structures because I got this situation with the first prepositional phrase there were two places that could have modified but the second prepositional phrase there are three places that could have modified for the fourth one there five places I could have multiplied like a factorial it's not quite as bad as a factorial because normally wants you to lift back that kind of clothes off the ones in the middle and so further prepositional phrases have to be at least as far back in turn to what they modify and so if you get into this sort of combinatorial staff the number of analyses you get when you get multiple prepositional phrases is this sequence called the Catalan numbers and Burnett still an exponential series and it's sort of one that turns up in water places when they're tree like context so if any of you are doing or have done cs228 Lacey and triangular the graphical models new asked how many triangulations there are it's sort of like making a tree of variables and that again gives you the number of them is the Catalan series ok but the point is we end up with a lot of ambiguities prepositional phrase attachment lot of those going on the only kind of these I want to tell you about a few others and apple veteran and longtime Nursery executive Fred Gregory appointed to board this sentence ambiguous one of the different regions the other possibility is that there's a shadow veteran and there's a long time Nessa executive Fred Gregory and have been appointed to the board and you can start to indicate the structure of their using our dependency so we can either ok Fred Gregory and then this person is adalbert trend and Time Master executive or we can say well we doing appointment of that's ridiculous Gregory and so we can represent by the tendencies and these two different structures that's um1 and very funny again so so he is a funny example that illustrates the same ambiguity effectively so he is President first physical Dr No Heart cognitive issues so there is an external coordination word here but effectively in languages only English you can use kind of just, and sort of list intonation to effectively act as if it was an end on or right so here we have again two possibilities that either we have issues and dependencies series of issues is that there are no issues so that's actually a determiner pictures and then it's sort of like no heart or cognitive issues so half is another dependent it's of a noun compound heart issues and so is referred as in independency and then it's half or cognitive so that Harrow cognitive is a conjoined phrase this no hard or cognitive issues but there's another possibility and which is that the coronation is at the top level that we have no heart and cognitive issues and at that point we have the cognitive as an adjective modify of the issues and the no heart the determiner is just a modifier of hard and then these are being conjoined together so loaded dependency of issues that's 11 funny ones it's ok so what the person who wrote this intended to have is that there we here we've gone adjective modifier and accurate he said he intended reading was but first is an adjectival modifier of first 10 and it's first-hand experience so the first hand is a modify of experience and the job is also a modify of experience and then we have the same kind of subject on that one this sentence different reading where you change the modification relationships and you have it's the first experience it's goes like this ok example mutilated body washes up on Rio beach to be used for Olympic beach volleyball what are the two ends of the two readings that you can get to this one play ok so we've got this big phrase that I won't and try and put a structure over to be used for Olympic beach volleyball and then you know this is sort of like a prepositional phrase attachment ambiguity but this time instead of it's a prepositional phrase that's being a test we've now got this being verb phrase we call it and I said that when you sort of got most of the sentence but without any subject to her that's for the verb phrase to be used for Olympic beach volleyball which might be an infinitive form sometimes it's in participle form like being used for beach volleyball and really those kind of verb phrases that sort of just like prepositional phrases whenever they appear towards the right end of sentences they can modify various things like verbs or nouns so here possibility he said is to be used for Olympics beach volleyball and what the right answer is meant to be is the dad is a dependent of the Rio by modifier of the Rio beach funny reading is that instead of that we can have here's another noun phrase mute 40 minutes of mutilated body that's going to be used and so then this would be a noun phrase modifier ok knowing the rights sentences is important to understand interpretations you're meant to get and interpretations you're not meant to get ok you know I see you think funny examples for the obvious reason but you know this is sort of a central to all the things that we'd like to get out of language most of the time so you know this is Spectre the kind of boring stuff that we often work with reading through biomedical Research articles and trying to extract facts about protein-protein interactions from them or something like that so you know this is how the results demonstrated that interact with me play with class A iamkb well navigation if we want to get out for the protein-protein interaction and facts you know what we have this Casey does interacting with these other proteins over there and the way we can do that is looking at patterns and now dependency analysis and so that we can sort of see this repeated pattern where you have the noun subject here fire and then it's going to be these things to the beneath of the satay and it's conjoined things k&k by other things that interacts with so we can kind of think of these two things as essentially patterns this sorry it should also be involved with we can kind of think of these two things that sort of patterns and dependencies that we could look for to find examples of in Protein interactions and appear in biomedical text general idea what we want to do and say the tour we want to do it with his these dependency grammar since I've shown you some dependency grammars I just wanted to sort of motivate dependency grammar a bit more and formerly and fully right so dependency grammar the forest syntactic structure is is that you have relations between lexical items the sort of binary symmetric relations which we draw is Arrows cos they're buying asymmetry and we call the pin and seas and I'll sort of two ways common ways of writing them and I sort of shown both now one way as you said put the words in a line and that makes it let's see the whole sentence neutralise probably pieros above thumb and the other way is you sort of more represented as the tree where you put the head of the whole sentence of the top submitted and then you say the submitted our bills were and brown back and then you say and the dependence of each of those so who's on ports in immigration so the pins of bills and worse and need words of the pin of some is head and you're giving this kind of tree structure so in addition to the arrows commonly what we do is report and type on each arrow which says what grammatical relations holding them between them so is this the subject of the sentences of the object of the verb is a conjunction things like that we have a system of dependency labels so for the assignment what we going to do is use universal dependencies for child show you more a little bit more of in a minute and if you think man has started fascinating I want to learn all about these lingua structures and there's a universal dependency side and that you can go off and walk out and learn all about them but if you don't think that's fascinating and what we're doing for this class we never going to make use of these labels always doing is making use of the arrows and for the arrows you should be able to interpret things like prepositional phrases as the what their modify just in terms of where the prepositional phrase as a connected and whether that's right or wrong yeah so formally when we have this time of dependency grandma we stood of drawing these errors and we should have referred other thing at this end as the head of the dependence say and the thing at this same as the dependent of the dependency and as in these examples and normal expectation and what are parcels are going to do is the dependencies for maytree so it's the connected acyclic single rooted graph at the end of the day the penalty Grandma has an enormously long history so basically the famous first linguist that human beings know about this pan and a who wrote in the 5th century before the common era and tried to describe the structure of Sound scared and a lot of what pan and he did was working at things about all of the morphology of sanskrit that I'm not going to moment but beyond that he start trying to describe the structure of sanskrit sentences and the notation was sort of different but essentially the mechanism a used for describing the structure of sanskrit was dependencies of sort of working out these modifies of what relationships like we've been looking at if you look at kind of the history of humankind and most of attempts to structure of human languages essentially dependency grammars and so two parts of the first Millennium there's a ton of work by Arabic grammarians and essentially what they use is also kind of basically a dependency grandma so compared to that you know the idea of context-free grammars and phrase structure grammar is incredibly incredibly new I mean you can basically and totally day that there was this guy Wells in 1947 who first proposed the idea having these constituents and phrase structure grammar and where it then became really famous through the work of trotsky and which love him or hate him is by far the most famous linguist and also various Lee contributed the computer science the hierarchy two people remember that 103 yeah ok the Chomsky hierarchy the Chelsea hierarchy was not invented to torture beginning computer science students the chance he hierarchy was invented because he wanted to make arguments as to what the complexity of human languages was yeah so in modern work there's this guy Lucian and he sort of formalize the kind of version of dependency grammar that I've been showing you and so we sorted off and took that his work and you know it tell me an employee show and computational linguistics some of the earliest passing work in US computational linguistics was dependency grammar and I won't go on about then more now just one two little things and two note I mean if you somehow start looking at other papers where they're dependency grammar consistent on which way to have the arrows point there are solid two ways of thinking about this that you can either think ok I'm going to start at the head to the dependent or you can say I'm going to start the dependent and say what it is and you'll find both of them and the way we're going to do it in this class is to do at the way 10 year did which was you start at the head and pointed to the dependent Planet Rock sculpture of the outstanding issues so really the dependence of is and discussion ok but we go from here to the pendants and usually it's convenient to serve in addition to the sentence to so have a fake root node that points to the head of the whole sentence and so we use that as well fancy passes and D build any kind of language Finders including kind of constituency grammar passes the central tool in recent recent word kind of main 25 years has been this idea of three banks and the idea three banks's to say we are going to get human being round and put for medical structure over sentences and so here's some examples I'm showing you from universal dependencies way here is some English sentences I think Miramar was the famous trainer or something and some human being has set and put a dependency structure over this sentence and all the rest and with the name universal dependencies this is just an aside universal payments is actually Project I've been strongly involved with but precisely what the gold universal dependencies wildest say what would like to do is have a uniform parallel system of dependency description which could be used for any human language so if you go to the universal dependencies website it's not only about English you can find universal dependency and our seas of your friend showed German or finish or Indonesian lots of languages of course there is an even more language is universal dependencies analyses also if you have a big calling to say I'm going to build a Swahili universal dependencies and 3 banking and you can get in touch this is the idea of 3 bag the sparkly tree bags something that people thought of immediately this is an idea that quite a long time to develop right third add thinking about grammars of languages even in modern times in the 50s and people started building puzzles for languages early 1960s and so there was decades of work in the 60s 70s 80s and no one had three banks the way people did this work is that they wrote grammars that they either Road grammars like the one I did for constituency of noun phrase goes determiner optional adjective noun noun goes to go for the equivalent kind of grammars in the dependency for mad and grammars and then train head puzzles that could possibly sentences into things human being writer grammar official because if you write rol-lite noun phrase goes to Terminal operative noun I mean Drive the huge number of Fraser's watching infinite number phrases so that you know this is a structure of the of cat the dog dog a large dogs almost things we saw at the Beginning so it's really efficient your Kettering what's of stuff with one roll turned out the factors that wasn't such a good idea and it turned out to be much better to have these kind of three bags of play structures over saying that says bit more subtle was the why that is because it sounds like pretty menial work and building three bags in some sense idiots no turns out to be much more useful I mean someone huge three bags of very reusable that effectively what they was in 60s 70s and 80s was that every different person who said about building a parser invented their own notation for grammar rules which got more and more complex and it was only used by the parser and nobody else's parser and so there was no sharing and reuse for the work that was done by human beings have a tree bank it's reusable for all sorts of purposes that lots of people build houses farmer other people used as well like linguists now often use three banks to find examples of different constructions this sorry just became necessary 131 as a do machine learning so if we wanted do machine learning we won't have data that we can build models on and a particular Lauder Foundation learning spiders how common are different structures and sorry 10 about the commonest and the frequency of things another big thing which is well lots of sentences are ambiguous and what we want to do is find the right structure for sentences in a bowl you do is have a grammar you have no way of telling what is the right structure for ambiguous sentences or you can do is say hey that sentence with for prepositional phrases after it and that I showed you earlier has 14 different passes let me show you all of them once you have examples you can say this is the right structure for this sentence in contour should be building a machine learning model which will recover that structure and if you don't that you're wrong me Banks so how we gonna do build dependency parsers how we want models and that can kind of ketchup what's the right has and just speak about abstractly you know the son of different things that we can pay attention to SO16 that we can pay attention to is the actual words right discussion of the issues that a reasonable thing so the Hub issues as a dependent of discuss and where is Neo discussion about standing that sounds weird to probably don't want that dependency there's a question of how far apart words are most dependencies are fairly short distance they're not all of them are and there's a question of what's in between how much does a semicolon in between it probably is in a dependency across her and the other issue is sort of how many arguments do things take so here we have was completed if you see the word was completed you sort of expect that there will be a subject before out of something was completed and it be wrong if there was and so you expecting an argument on that side but on the other hand it won't have a object after he won't say and the discussion was completed the Goat and that's not a good sentence right so you won't have an object after so there's some information of that sort and we want to have a dependency parser be able to make use of that structure so effectively what we do when we build a dependency parser is we going to say the each word is is going to be the dependent of some other word or the route give here is actually the head of the sentence so it's the dependent of rude is a dependent of give is a dependent walk and Sophie each word and we want to choose and what is the dependent of and we want to do it in such a way the dependencies form at 3 so that means it would be a bad idea if we made a cycle so if we should have said bootstrapping a dependent of we had things sorted move around so this goes to hear but then talk to the pendant van as all on a cycle that's bad news we don't want and cycle through on the tree final issue which is we don't want things play the we want allow dependencies to cross or not an example of this so most of the time dependencies don't cross each other sometimes they do and this example here is actually an instance for that so I'll give a talk tomorrow I'm on bootstrapping so sing a torque that the object is being given is tomorrow orgasm modifier that on bootstrapping so we actually have another dependency here that crosses that doesn't happen at an English but it happens sometimes and some structures like that and so this is the question of what we say is that the part of a sentence is projective there no Crossing dependencies and there's none projective if there are Crossing dependencies and most of the timing wishes projective in its positive sentences that occasionally not and when is not is when you can't have these constituents Adelaide to the end of the centre and try to cut a said I'll give it took on bootstrapping tomorrow and then it'll be ever projected path but if you want to you can kind of July that extra modifier and say I'll give a talk tomorrow and bootstrapping and then the past becomes none projective there ways of dependency parsing that basically what I'm going to tell you about today is this one called transition base or deterministic dependency parsing and this is the one that's just been and normally influential in practical deployment surpassing so when Google goes off and pause every webpage what they're using is a transition based parser so this was a motion of parsing that was mainly popularized by the sky why can NIA Frazier Swedish computational linguist to do is inspired by ship produced parsing so probably and you know cs103 or compile was class or something he saw a little bit of shift reduce parsing and this is sort of like a shift reduce parser apart from when we reduce we build the pin and C's instead of constituent and this has a lot of very technical description that doesn't help your tool to look at in terms of understanding what is positive and has a formal description of the play shift reduce piles of a daughter doesn't help your Nuttall I'm so instead we can look at this example that hopefully help you know what I want to do is has the sentence I ate fish and your family what I have is I have a way I start there are three actions I can take and I have a finished condition for formal pass pass and so he is what I do so I have a step which is on this side and I have a buffer the stack is what I have built and the buffer is all the words in the sentence I haven't dealt with you the past and that's the sort of instruction here by putting route my route from my whole sentence onto my steak and my buffer is the whole sentence and I haven't found any dependencies yet then take us the ship things onto the stairs to do the equivalent of a reduce where I build dependencies or dependency because I only have root on the stack so the only thing I can do a shift so I can shift I onto the stack I could have this point say let's build a dependency eyes of the pin that rude but that would be the wrong analysis because really sentences 8 Dyna clever boy and I shift again and now I have eight on the steak and so at this point I'm in a position where hey what I'm going to do is reductions that build structure because lock I have I ate here and I want to be able to say that is the subject of dependency of 8 and I will do that bio I doing a reduction and so what I'm going to do is the left Ark reduction with says look I'm going to trade the second from top thing on the stake as a dependent of the thing that's on top of the stack and so I do that and so when I do that I create the second from the head thing as a subject depending of eight and I'll leave the head on the stack 8 but I saw this dependencies of dependencies I've built I'm so could immediately reduce again and say Oasis of dependent sentences actually I ate face so what I want to do is say it's still fish on the buffer they want I should first US shift again have route 8 fish in my sentence and then I'll be able to say look I want to know build the thing on the top of the stack as a right the pin and of the thing that's second from top of the stack and select preferred was a right Ark move and so I say right Ark and so I do reduction where I've generated anew dependency and I take the two things that are on top of the steak and say fish is a dependent of eight and so therefore I just keep the head I always just keep the head on the stack and the and I generate this new app point I'm in the same position I want to say that this 8 is a right dependent of my route I'm going to do right arm make this extra dependency here play my finished condition having successfully passed the sentence is my buffer is empty and I just have fruit left on my step sorry I've sent back here that my buffer is empty as my finish condition has the sentence well you know I actually had different choices of Winter Park winter shift and winter a juice and I just miraculously made the right choice citypoint one thing you could do at this point is say well you could have explored every choice and happened and gone different houses and I could have that's what I done I would have explored this exponential size three of different possible parsers and if that was what I was doing I won't be able to parse efficiently and that's not what people did in the 60s 70s and 80s and clever people on the 60s said enjoy your company search here we can come up with clever dynamic programming algorithms and you can relatively efficiently explore the space of all possible causes and that was so the MainStay of Passing and those decades what team did Freya came along he said yeah that's true but hey I put a clever idea now it's the 2009 Omar schein learning so what I could do instead is say I'm at a particular position in the past and I'm going to build a machine learning classifier and that machine learning classifier is going to tell me the next thing to do it's going to tell me whether the shoe alright are so if we're only just saw talking about how to build the arrows there just three actions just left or right heart if we also won't put label from the dependencies and we have a different labels and there are then sort of two are + 1 actions because she's subject or left-hand object or something like that actions and so you going to build a classifier with machine learning somehow which will predict the Right Action give me free show the son of slightly surprising fan play you could predict action to take accuracy so in the simplest action of this there's absolutely no search you just run a classifier disturbing it says what you should do next shift and your shift and it says what you should do is left are you left dark and you run that through and he's proved to be shown empirically that even doing that you could pass sentences with high accuracy now if you want to do some searching around you and do a bit better but it's not necessary and we're not going to do it for our you're doing this just sort of run classifier predict action run classifier predict action we then get this one before result which your little explain a bit honest you assignment 3 is that what we built is a linear time by the because we're going to be sunny a sentence where you drill the amount of work for each word and that was sort of an enormous breakthrough because although people in the 60s and come up with these dynamic programming algorithms dynamic programming algorithms first sentences or always cubic or worse and that's not very good at trump as the whole web where's if you have something that linear-time that's really getting you places this is the conventional way in which this was done was you know we have a stare already built some structure of working out some things to pin and of something we have a bottle of words that we don't deal with that we want to predict the next action so they can finish your way to do this is the same we want to have features and well the kind of features you wanted was so they usually some kind of conjunction and multiple things so that if the top word of the step is and something else is true that the second touch stop word of the Stick is has and it's part of speech is verb then maybe that's an indicator of do some action to hear these very complex binary indicator features and you'd build binary indicator features and you'd feed them into some big logistic regression or support vector machine or something like that and you would build parses work pretty well but you were still have had these sort of very complex hand engineer binary features so in the last electro I want to show you and what people have done in the neural dependency parsing world but before I do that let me just explain how you valuate dependency parsers and that's actually very simple right so what you do is well because the human Road at down but there's a correct dependency parsers for a sentence he saw the video like to like this and so these are the correct arcs and to evaluate their dependency parser with simply going to say which acts are correct so there is a gold arcs so there's a goal dark from and she saw subject old r 0 to the root of the sentence is the gold axe as we going to propose some acts as the what is the head of each word and with simply going to count up how many of them individually and there are two ways we can do that we can either as we're going to do ignore the labels and that's in referred to as the unlabeled attachment score so here in my example my dependency parser got most of the arcs tried but it got this one wrong so I say my unlabeled attachment score is 80% or we could also look at the labels and then it my parcel wasn't very good at getting the labels write-on any games 40% and so we can just count up him season how many we get correct and that's our accuracy and in the assignment you're meant to builder dependency parser with a certain accuracy at the number now is saying something or something the joint to get to skip so now I want to explain to you just a bit about new or dependency parser motivated side Minton to already that the conventional model and had these sort of indicator features of top of the stack is the word Gordon the second thing on the stick is the verb has or on the top of the stairs some other word and the second top is of some part of speech and that part of speech is already vagina the dependency of another part of speech people hand these features problems with that these features were very sparse each of these features matches very few things match some configurations but not others are the features tend to be incomplete and they're a lot of them they'll come and Liam millions of features and sort turned out the deck she computing these features was just expensive so that you had some configuration on your step and then you wanted to know which of these features for that stick and buffer configuration and so you had the computer features format and a turned out that convinced all dependency parsers spend most of their time computing features then went into the machine learning model rather than doing the son of shifting and reducing I've just a pure pozza operation seemed like it left open the possibility that will what if we could get rid of all of this stuff a new network directly on the second buffer configuration then maybe that would allow us to build a dependency parser which was faster and suffered lists from issues of sparseness and the conventional dependency parser what's a project that done teaching and me try to do in 2014 I missed the build and you're all dependency parser and you effectively what we found is that that's exactly what you could do so here sort of a few steps Here and cevizli same you as and L as I'm so much was working neither is parser that Iris ring before and it got us on this day the of 89.8 everybody loves dirt and the reason I loved it is a kippah head 469 sentences a second I've been other people that worked out different more complex ways of doing parsing with so-called graph-based dependency parsing so this is another famous dependency parser from the 90s so it was actually you know accurate but it was a bit more accurate being two orders of magnitude slower and your people have worked on top of that so he is on an even more complex graph based parser and fungi 2002 little bit more accurate again that's gone even slower ok so what we were able to show is that using the idea of instead using a neural network to make the Decisions of a walking need for style shift reduce parser we could produce something that was executive the very best puzzles available head that time and it's strictly one over here with a fraction behind on uas no it was not only just as fast as near as pazza it was actually faster than the first puzzler because we don't have to spend as much time on feature computation a surprising result right it's not do anything we had to do matrix multipliers and annual network and it turned do the Matrix and multiplies and more quickly in the feature computation that he was doing even though at the end of the day it was probably looking at weights that went into a support vector machine when does Call into the secret was were going to make use of distributed representations like before ready seen four words word we're going to represented as a word embedding like we've always already seen we are going to make use of word vectors and use them as they represent the starting representations of words and their past very interesting distributed representations it seems like maybe you should only have distributed representations of words and maybe it also be good time to tribute representations and other things so he had parts of speech like either nouns and verbs and adjectives and so on well some of those parts of speech have more to do with each other than others I mean in particular NLP work uses fine grain parts of speed so you don't only have a part of speech like noun or verb you have parts of speech like singular noun vs plural noun and you have different parts of speech for you no work kind of the different forms of verbs given different parts of speech well so there's so titze parts of speech label for Cannock plasters so maybe we can have distributed representations of speech that represent their similarity by not what we going to do that why not just keep on going and say the dependency labels they also have a distributed representation and so we built a representation that did that so the idea we have in our stair the top positions of the State systems of the Buffer and for each of those positions we have a word and a part of speech and if we've already built structure as here we kind of know about at the pen and see there's already been billed and so he's got a triple for each position and we're going to convert all of those into a distributed representation learning and we gonna use those distributed representations and to parser the starting from the next lecture forward we going to sort out using more complex forms of neural models but this model result of a very simple straightforward way we said we could just use exactly the same the same path as a structure that need for a used right doing those shifts and left alexandrite acts and the only part we going to turn into a neural network is so going to have the decision of what to do next being controlled by on your network so our new network is just a very simple classifier of the kind that we're talking about last week the configuration we create an input layer which means we sort of taking the stuff in these boxes and turn and looking up a Victor representation for each one and cans calculating them together to produce a input representation that sort of similar to when we're making those were window classifiers and we can chat later that bunch of stuff together so that gives us in our input layer so from there we put things through a hidden layer just like last week we do WX + b and then put it through a real you aw nonlinearities hidden layer and then on top of their with simply going to stick a soft next output layer so I x another matrix adding another bias term and then it goes into the soft eggs which is going to give a probability over our actions the weather shift left or right or the corresponding one with labels and then we gonna use the same kind of cross entropy loss to say how good a dog do we do at guessing the action that we should have taken according to the three bank part of the sentence step of the shift reduce parser when making a decision on what to do next and we do it by this classifier and we gain a loss to the extent that we don't give probability one to the Right Action and so that's what we did using the treebank we trained up out parser and able to predict the sentences the coughing the coating was that this had all the good things in the first parser that you know by having at use these didn't representations that meant that we could get graded accuracy and speed than nearest parser the same time so here's some results on there and I mean I already showed you some earlier results right so this was showing the fur you know we're Out Performing is earlier parses basically doing this work Google his papers here by weisen and door and they said this is pretty cool and maybe we can get the numbers even better if we make our new network and bigger and deeper and we spend a lot more time tuning out hyperparameters all these things help when you're building your own networks and when you're doing your final project sometimes the answers and making the results there is to make it bigger deep and spend more time during the hyperparameters they can really help so you know rather than just saying what's the best next action do that one in the peace over you allow yourself to do a little bit of search you sort of table let's consider to actions and explore what happens it's always on screen on how to build this cheese and if they don't want or agreement document good question which I haven't addressed humans don't always agree to recently I can't agree on the made by one is the humans and sort of Miss upright costume in work is doing this at perfect and the other one is they genuinely think that they should be different structures so you know it varies depending on the circumstances and so on if you just get humans to pass in and says and say well what is the agreement and what they produced you know maybe your any games something like 92% that you know if you then do an adjudication phase and you say differences a lot of them where you know one of person as effectively say rui gift wasn't paying attention of whatever and so then what's the residual raise in which people actually disagree about possible parcels I think that sort of more and 3% the Sunley are cases not include some of the prepositional phrase attachment ambiguities sometime attachments for the same clothes that was not really cleared which one is right even though there are lots of other circumstances where one of them is very clearly wrong room to do better I mean at the unlabeled attachment score it's actually stand get pretty going to do that beam search for final thing that they did was that we're not going to talk about here is a storm or global influence to make sure it's sensible and so lead to Google developing these models that they gave silly names to especially the party mix fat face and model of parsing and so yeah so that's in that sort of pushed up the numbers even further so that they were getting close to 95% unlabelled accuracy score from these models and this work is Kai people like to optimise how this works so long in the intervening 2 years of the numbers of stargazing Abyss higher again but you know so this is actually lead to a sudden you have sorted better passes because so effectively this was the 90s is your of puzzles that was sort of we're around 90% and then going into this sort of new generation transition based dependency parsers we sort of have gone down that we have that era and we now down to throw about a 5% yeah I'm basically out of time now but you know there is fear of the work including Stanford and another student teams has some sort of more recent work is more accurate than 95% still going on but I think I better stop here today and that's me or dependency parsing 