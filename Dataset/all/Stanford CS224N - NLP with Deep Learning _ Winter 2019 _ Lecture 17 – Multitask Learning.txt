very pleased to have a self S I'm invited speaker Richard social who's the chief Scientist had Salesforce and Richard actually also has a lot more connection to this class because I'm for several years and Richard was involved either as instructor or instructor in teaching just material at Stanford and so he sort of knows the course have pretty well and so today he's going to be talking about some of the challenges and recent work enjoying multi-task learning in natural language processing so welcome thank you everybody about the insured caldbeck LLP give a big shout out to Brian McCann his the first offer of this paper in a lot of people in the last plague 34 years and most people were like this is too much preprocessing because you're trying to do 10 different that's the weather decathlon wording comes in he really stuck to it did all the preprocessing and all the things that you know like talking isation it turns out a lot of different datasets have a different conception of word is wasn't two words or one word and things like that in that changes how you write all your validation script and all of that so Brian is a banana researcher in the group and has helped us a lot on the optimisation side of this and some inside directory searches done a lot work instead of helpful infinite alarm I'm going to tell you a couple of different lines of reasoning that letter to at this idea of multicast learning and the first on result of trying to take a step back and looking at the field in I know it's not like diameter of a circle class but basically pre-2010 most natural language processing had kind of these very hand designed features and we basically just had learning kind of learnt weights in the optimisation procedure for the human designed features and so in 2010 another sort of started to work and learning for future learning so everything was backpropagation to them and actually learn those representations having a state with a lot of sleep architecture engineering for specific past and you seen this already have like an er model ever question answering model never translation model and we basically now each of these Communities test at least convert stone is probably some kind of no network but you're still a lot of different kinds of architectures of these new networks that you're working on for each different say ok will probably do that for another couple of years making good progress but what sort of next on the research site and what actually love about this custom matches that you go from like maybe not knowing much about it ok all to you can basically understand research papers as they come out now this is one of those so why not continue to work in this multitask resume and some ways if you like the community little bit like the stew dog where we can have randomly restart can you project can you play me that's if you have a lot of training data and you define a specific dataset dataset you start architect engineer new model to hill climb on a particular metric or leaderboard or a publication in-store product or whatever it is as long as you did or said his roughly a good representative set of 1000 times the number of outputs that you have your probably get it into resume reading the 88 and 90% accuracy or one where you're basically doing pretty ok and of course imagenet 1000 classes and computer vision 2020 where's 1000 images so if you have roughly images you do pretty well and then machine translation ideally I have many more by hundreds of thousands of words many millions of examples of each of the word in words in the context of the caveat is machine translation doesn't work but it works well enough to have a disinfectant even the best human translators using that sort of a pre translation and then clean it up autofill me that in this regime get to some more generally I features we need to have some kind of more continuous learning of a single model does it keep restarting every project we never going to get to single-molecule in composites more more the complexity of natural language when I say we start from Random your first no that's not quite true because we do have some things that we pre train near me what doctors and computer vision with even more things in some ways that play ideal for Anna page division you would be kind of crazy but not use some kind of convolution neural network that is free training has been fully trained on some kind of tasks that unit project in try to classify objects with Logic pensioner what about the things some ways that the whole community could get behind a very quickly because reasonably well because they were sort of a single blocking task in computer vision if you can't even tell apart a dog from a cat from house doesn't make sense to think of even larger vision projects and an NLP we had a lot of success factors to know a lot of those now and started to sort of just a small window based approach to go to back and Glass contact factors that were trained on machine translation but basically instead of just having a single set of words worksheet pretrained sound lsdm that came on top of those were back there do we train that with also extra Brian McKenzie paper and contextual factors was with machine translation and an Elmo panels replace machine station smiling mature even more training data and it still tells you what in some ways a more complex version of distributional properties that we had in simple word actors and tomorrow but also kind of trying to predict words in her context free training a lot more layers and lock ethernet success of a certain set of weights why not try to pretrained the entire model including your output your softmax your pointing mechanism eating a completely pretrained model something in return of the goal that we have are we should have asked ourselves why hasn't happened why are really you know the first to think about like trying to try and the entirety of the model the encoders and decoders and outputs everything anything part of it is that Anarchy requires lot of different kinds of reasoning you've seen many of them already you have logical reason they like 55125 leave they still keep on the room when you cannot logically answer that question and I have lots of different kinds of linguistic and emotional reasoning sentiment analysis this is a typical Nicolas Cage movie in then you need to know that that's probably negative review types of reasoning and so on and so I think partly because of that complexity in the beginning of field didn't make much progress separated in 18 in some cases can of artificially separated into all these separate ass the recognition in Paris speech tagging in semantic role labelling and so on ways and some kind of sorry but you know it made what I sent to the time allowed us to make a lot of progress but basically we started chasing his bench Martin always different Communities kind of started going off in their own ways and your even have some Communities take me to general question answering and there's literally workshops and general question answering and when I asked the organizers can I ask you model with the CM of this week I like go to the shop down the hall the question why can't you answer it in the general question answering workshop so a lot of people then say well if you want to work in more general stuff it has to be in unsupervised kind of task and the future will not be supervised I don't think NLP play The Answer revising we won't solve it completely because in the end playing which has a lot of supervision for people thing for for systems also ent the child in the jungle probably develop a free good visual Cortex by itself but it won't about Language by itself and also like I think if you just allow it I have to talk to one another is make very little sense for them to try to come up with as in a fish and other communication protocol as humans have with mental processing of language Samsung computers human language I could just communicate in much more efficient ways with one fully clear we need a lot of supervision in an LP basically all of this has led us to trying to think about a unified for lots of different animal Peter questions just Qantas unified model to decide how to transfer knowledge and not have it should have been manually assign like the most cases when you decide your project you say all I know ignition speech tagging help each other because once you know something is a noun then it's more likely that it's also an entity we want to basically allow for the single unified model to know itself how to do domain adaptation and how to share the weight play then leads to a lot of Transylvania and learning capabilities ng33 get to this sort of hard goal of having a singer then we'll be able able to more easily adapted to meet ass and will be also able to deploy in production work quickly it's now days do you want to build a little stroll detector and connected to your sprinkler system have downloaded some off-the-shelf software and basically kind of application if you tried to do a pretty complex language completing your language your website and do something else afterwards so you're also when you actually try to deploy and use discounted clothes in companies you realise that there are lots of different kinds of Gru open the chatbot team in the translation team social sentiment analysis team they all use different models and they all deploy different models and they all have to build the water over here of the rounded corner of an eye model basically it was sort of what we had with the start I think that once we have this unified model off to be a first step to being able to them continually learn this and just have a cinema that just gets better and better over time starts to capture more and more but the complexity of Langley questions around cylinder motivation it's out of the question how do we actually make that happen I have a first set down and look at the formats of all the time experiencing assassinated nop Topaz a feeling general and I think they can broadly classified be classified into three different no thanks I can any or ask specific sentimental in a specific context want to classify if a word is positive or negative and then text classification just the single label for the entire piece of text and then sequence-to-sequence lot of different problems falling to that love these three particular task machine translation summarisation question answering because they are immediately used for explain somebody over why you need to semantic role labelling apart and you met you understanding me to leave whites useful to do summarisation translation in in Windows 10 translates into better product people being able to communicate better and more efficiently of language that can please let us to think about this what I call Three equivalent super task of NLP and basically they are linked modelling question and answering in language modelling basically trying to beginning predict the next word for the work done can you use to restore free train these days but really if you ask a question and then you try to predict the next couple of words also language modelling if you're able to predict the next couple word after Christmas what were the names entities in this sentence and then you just generating or Dresden was a location person and W you can kind of cast almost all these task into English modelling similarly question answering you can ask any kind of question what is consolation what's a summary on and then right now cannot tricky cos there are no really good dialogue data sets out there and a lot of times he wants an interaction you have to run user studies and most of the existing and lpt pretty short one step dialogue wanted an entity attack set title bit overkill and because of that would basically converged on Question answering as our main formalism where is now in overview of the 10 different tasks that we have and we cast all of them as question answering have the format of the training dataset eventually also the way we formulate the test that you'll see basically for every single pass Akon document review Wikipedia article between the longer document question about it anyone in January enhancer I'm curious if you can think of any task in AP that couldn't be formulated in this kind of structure let's go over some of these the first one is the standard bastard you're off Amelia with now the squad cancer is essentially afraid somewhere in the contacts the second one is something that you would never see you in most general question answering that is having a context of the single sentence asking what is the translation from English into German again a sequence of words but in this case and we carried them differently here this is blue because all these words are basically not in the contacts and not in the question just generate them with stamps off Mac answer this question what is a summary can see that those two in some ways is artificial to make them into a natural language question you could just say Translate rice and it's just like one last token internet half of these task and makes sense cos the question also has actually different for every example so this one here is natural language inference and I should cover it also our we want to ask with her two sentences until tricky charger or there some neutral relationship between them seen a lot of centum it's important we actually ask is the sentence positive and negative vs just what is the sentiment add what why it is important that you see her in green this answer here actually comes from a word in the question the.way eventually do 0 shot learning where we asking you a question that was never asked before for a new set of labels medically in some cases is still actually works in roll you know ask where you can ask question I get a story happy or sad will give us an entry and go with never given a train day said of a bunch of happy and sad stories zero-shot classification did you get to in some cases if you formulate your questions in a way that the answer is 2 now we have to medigrow labelling here so experienced can we have a 0 shock relation extraction who is the illustrator cycle of the werewolf will Stefan de Lux de tracking what is the current state in in a dialogue in the context just keeps on growing with dialogue we also have a sequel we can sequel translation task for not translating into another translating into a sequel database query forecasters know the lot of data out there that is stored in databases exited without having to ask somebody who knows how to program sequel it will make that data available to lot more people so they can analyse it and Analytics and saw winogradsky mass and anaphora resolution all this kind of common sense reasoning resolution trying to understand in this who is Cena word like when giving her or a giant and then based on this context to count get up so the question is different for every single play Spotify the question is when I asked is the sentence part of negative will it sometimes eventually accidentally switched to a different one of the task and we actually have a slight on that and the answer is it's surprisingly good at knowing doing the task and where to get the answer words from don't make more sense in a couple of lights is about the question answering formalism generation in the questionnaire as well tell me a story question so can we do text generation like tell me a story from the random kind of or in this kind of formula don't have that as a task it's really hard to be late I'll tell you some random stuff in is that a good story or not doesn't have to come up with a lot of sort of evaluation metrics would you actually are doing for some of the dialogue systems and in case of dialogue widest while they equivalent because the contacts can just keep on growing in every time the user set something you basically tried to then answer in a dialogue in so I think you could very easily use this to generate text you busy just asked what is you know what's a good ending of the story and use maybe start to contact with Lightroom auto generate more words in in the pollution levels pregnancy training set an alarm for 4 a.m. amazing leafio question and it's important will have a bunch of slide so maybe we'll go we'll continue and we'll get to that question and a lot of detail that sort of why were doing so these are basically the 10th again this is the actual so if you have casting in this format are you can just take the Open Source Code and run it and it'll work in so when you can analyse and think about what we've done here in some ways we've taken the task in your head but it's not given to the model the model is just giving him an internet and output y and almost all these systems z-rex including a task in the input set of inputs to the Marlowe old is midday supervisor earn the question it's kind of our task definition for each of these different ass and more has to figure out itself went to ask the question that we can also forgot itself went to transfer knowledge from this other task just the answer so in some ways it's metal supervised learning and I'm quite excited because once you allow the tasks to be given to the model as input and decide itself how to go about solving that particular task in that you can learn a lot more powerful models etc that ok how do we now solve this problem the simplest ways you could just say what I have a big if statement now the classifying the beginning and I classify if this is a machine translation task then running machine social model and in general that would still be just like one big Python model with a bunch of statements it's not to go because we wouldn't get to any of the transfer learning and capabilities that were hoping for at the model to have the capability to internal sin make decisions itself basically all those considerations and all those thoughts let us to this model so be quite go and see a little bit more details I just likes of give you that high-level overview and then you started to you start a question about and that context generate the answer one word at a time pointing to the contacts in your head pointers already pointing to a question word for choosing a word from an external vocabulary with your standards off Mac Spotify and we'll have a pointer switch mechanism that will kind of chews how much to wait each of these mechanism let's begin to look into this model fortunately in somebody's this kind of just taking the best of consorcio Dr techniques and putting them together in a way that the channel is well enough you can look at all the codes on booking.com thousands of stars and and South combined can you know basically run everything in this Germans with just one we get all the datasets and everything thing you can really explore let's let's live yoghurt into the details of what this model so that's in some ways again it just takes all the best ingredients from most of which you've already learnt about and put some together in a reasonable embeddings eventually will we have updated the embeddings to Coven bedding we are working better for optimum to Burton bearings for it some point we kind of have to move on to other things but basically you have a fixed set of vectors and that teleport datasets are much smaller than other as you know from squad if you actually back-propagating to the word back cos you just do really well on your training dataset but then you won't generalise because most of the test documents include words you've never seen before so if you change all the way back it won't work so what time and what channel is 20 words embedding if you don't have Word actors France in words with lots of character in government buildings fighting through a simple linear layer bidirectional lstm until at DPD point so you skip to hire layers and a shared between the contact centre questions so they set of weights we have a core tension layer or basically our products between all the hidden states of those two sequences skip connections to circumvent those as well so now you have kind of contacts or question dependent contact your representations or representations of that context then we feed those in search and former layers Transformers 4 all the things will having no lc-ms or any of that at fortunately still very here and very hard to optimise and there's a lot of chicory with the learning rates and we could just not get them to perform really well unlock this 10 different task transformer layer 14 men at work in Audrey's around one task but the only other transformer network that work well on the second task at Lake have two layers to have one network with the same number players it just wouldn't work on it up to task anymore unfortunately as nice as they are because they nicely paralyzer bungee fused they weren't yet robust enough to be used to this what is lcm before and after the transformer layer standard sort of other aggressive decoder given the last date that we generate the next one 3.0 mechanisms are very similar to the point of making my no but now on top of these very contextualised representation this encoder basically Laurence 20.2 question words context words based in the hidden city also standard sock Max and then we just basically have the waited some connect some of these three different distributions I think these are mostly standard components that you've already so seen all the details for the other questions would have to go work a word and it's always either a word from the contacts to work on the question or work on the sofa data preprocessing the thing is different priestess but we basically have to normalise everything to have the same talking Alsatian and all of that Indian curry just represented by directional by direction of so left to right or right to left for that stop whatever sets are we using I mentioned that that was a big headache in the beginning we definitely wanted to include a lot of the sequence sequence past that we felt like our very high level in mutiny useful and in some ways what is also shows you instead don't have to work as much on some of the intermediate representations in an LP anymore and you can just directly goal for the enter that the real uses my care about and have his enter did really do quite well add myself to one person inside I don't want you no say we don't need are certainly still task that you do need it for about this kind of surprising it you can just go directly to translation or summarisation without having intermediate representations that were so very specifically hand design we had the three really interesting and Hard task question answering machine translation summarisation they actually also have the 3 biggest data sets of all of these we had all of these 10 days stats where are in several cases especially for translation you could actually find much larger the size were normal people that don't work in gigantic companies with huge GPU infrastructures could still run experiments so University I can still running on basically if you have just a single view a week or so our to run if you have multiple GPUs in one large EWS machine you can canephron exterminator 2 34 translation right you could get a lot more data than iwslt each of these Communities and datasets and tassazione metric where she tried to in the beginning with that a lot of discussion about how we should define the measure of success make sense to have a normal iPhone store for basically all the different task but then we basically realised that these different community of different metrics for reason unfortunately at least all of these metrics are from 0 to 100 in theory for some practice you're really ever see a Translation system with 194 really high roofed forest play go from 0 to 100 and so basically in metrics for each of his community we're going to set them up when we first talked about this we have the lot of discussion with others also the translation is so much more important because it's much bigger and it's much more useful tasks and he still silly like fun and resolution photographs emax which only have a couple 127 weighted translation more and then literally 5 questions later sounds like why don't you wait phone and resolution more that is a really hard task that captures sort of commonsense reasoning and you know the complexity of language and semantics and like this takes the statistical pattern matching Italian translation and I would like you to talk to that guy hopefully he will just all agree that it's reasonable to Sunday have to tackle when you run experiments in the complexity that you have in machine learning and people talk about like having their skewed distribution translation retard 1200000 examples in university must they only have a couple of hundred train that touch that you don't just completely ignore the smaller dataset will get her some of the optimisation trickery spent several months on in a bit but I'm first one and I'll give you the first set of experiments so as you can see from all the numbers there is a lot of experiments that we're and you can get to this and so we'll walk through this by carefully I think hopefully you'll get some of yours also for for a PlayStation store experiments that you might want to run in your in your experiments in in your problem final project so what are we looking at here so basically on the left side we have single task performance so here each number comes from it's different model that was trained find just one task raw is Colin here is the same architecture turn on the right side here we basically have for each Colin is basically the same architecture and the same exact model we have four different models in here we have 40 different models time again is the same architecture and so the simplest first column here is just a standard sequence sequence model with very few building with us nothing should meet you at 3 bi-directional HDMI connections all the standard good well soon stuff for sequence-to-sequence models then we added self attention this the sort of basically then we have this covert attention layer of the other products that we mentioned begin edit the question pointer so having the ability to point about this it will begin to some of the details to the details person ever you can think of some questions so let's analyse what's going on the table cos they're lotto number you want to carefully analyse my first observation was even even this is not quite what we went right through on a single model but even discount shows as well you can ever single architecture that actually does real when we are in some cases actually had gotten state-of-the-art results so we can sequel for instance this architecture had the best model to translate natural English indiscretions into secret I said I was really not like a priority fast and when we designed the model and thought about how to generate words in front of mechanisms context of sequel words and we have to question what's the translation equal and then somewhat surprisingly trust this particular architecture had the service garage on secret generation and bunch of folks in that community cannot take that more quickly and that's unfortunately doesn't have that many other save your numbers just why it's harder thank you much harder task service all of the cases using the multitask models having a single model for all the 10th ass performance at Perth nothing really reading papers because papers have the strong selection by us to only publish positive result the most transfer learning and multitask money paper in outside of the actual model consideration of like well that's only combine pasta we know we'll work well with one another and if they don't work in her performance and just exclude them from March 3rd don't see many negative task results in the literature and a few papers in there that study basically the opposite side of cancer moaning and that is interference and catastrophic forgetting so interferences when you train two different interfere with one another next you heard each other's perform forgetting if you train continually first train at 1 on the 2nd what's the thing first class will be completely forgotten work well on the second class with a training neural networks out in the sequential way one task and then another Headingley we found dead things aren't actually catastrophically being forgotten in this matter training sequentially in you add a little bit of the original g first class it comes back very very quickly so I live performances really bad you can get to the really good performance very very quickly and very few iterations so but it's one of the many interesting titbits that we found the course of this that we have been published yet so focusing on the Transformers Theory basically find Transformers do help the original sequence-to-sequence models or so carefully and you can buy them with some Ella bi-directional stems and someone they were very helpful and proof across the bunch of different data sets and some place is quite significantly probation is question answering and semantic role labelling actually can predict each other's performance quite well that one works well the other works well and and vice versa if they don't work well both of them don't work very well what do you think of both of those tasks have different questions for a train example so the question pointing is super important we actually have in some cases twice the performance even for this kind of surprising Trust a simple classification task or you could just have a standard soft next but instead of saying you have a soft No7 tell men's contradiction so one you just basically point to the word entailment in the question also the case for winogradsky master also benefited a lot from this point mechanism can we explain why when is the help so much I think party is the whole architecture has gotten better at pointing and part of the reason we actually do very poorly in Translation which is the only task that hurt in the first experiment casting as that is the only task that now has to generate is Ultron a completely separate off next where's the rest of the architecture that really really good things to answer questions and so but in some ways I think destination but I don't think it's all of it I think we still need to figure out more wiretaps now I multitask learning is the most helpful when it comes to 0 shot and I'm actually very excited about that so this is your shop relation extraction we have different kinds of relations that you might want extract and you might have never seen like the student teacher relationship that you're trying to identify in a certain context or a product company relationship or something like that that one actually hot and almost twice as high in terms of the accuracy when you learnt it with everything else so these are questions it's never seen before relations that is never seen before and got twice as good and benefit a lot especially from kinds of questions in some ways we have to give a low credit to squad 2 what is a dataset can push people to thinking about pointers as a mechanism to generate answers and pointers we cannot see them like as a given and they don't get them much credit but they allow you to predict you've never seen before a training to generate words you've never seen before 20 time which is actually quite quite amazing now observation so here is you had an oracle that would tell you exactly which tasks you are currently in you would be perfectly kind of separating these independent models maybe they're all the same architecture but there's still 10 different models then you would actually still do slightly better than the first version of this multitask learning model and that is largely because we chose to include a bunch of different tasks that have nothing to do with one another and we wanted to Communities start thinking about tackling catastrophic interfere play Like a new language or you learn how to understand a social media on Twitter you don't replace all your language you have one drink and keeps getting smarter you keep learning new skills even when the skills that are new to you are very very different from all skill so somewhere our lives too hard and now we're thinking ok maybe if you want publishing I should pay for multitask learning we just look at all the tasks that to help each other can I have groups of past and then I can very quickly publish some some nice day of your papers here we're still quite significantly away in the deca score between 10 different models now this of course is kind of an organ score that's why we put in parentheses because you don't actually have this orca some cases it's quite easy to build an almost perfect classifier so you know separating what is the summary based on that question and what is the translation from English to German you can do with on a Samsung Active adds a question answering insulation extraction and question answering as a semantic role labelling those are actually easily confused and terms of how to generate the answers and you wouldn't quite know which into which model to wrote The something sometimes this can of theoretical now I mentioned that we have this project this complexity in the optimisation strategy and this is one of the many sorts of problems said don't get that much coverage haven't dataset is easy to lose track and basically overpower the smaller dataset pass the first simplest training for a reaction tried it on a different range training strategies but in the end is fully joined one work quite well but actually on the any questions on this result so far can to mention that if you tell you which is better with my training a model on what tasks seem interested in it confused in your squad and and all through the quest the other two types of problems that are also classed as question answering confused denziloe the others it was able to complete very perfectly do it but then you basically as soon as you Triton until to home and get a deca score if your Pacifier is even like 90% accurate multiply this by .9 and you get thing so hard that it's not competitive anymore so it is actually hard if you try to just build that whole system and keep adding sort of if then statements to make that into sort of a single system the Model what kind of tablets indicator of who is we did in this case because we only trained each model separately on it we wanted to do what we want to do it now in some cases you could tell so the question is even in their multitask setting you could have like an extra kind of talking to say now you're doing summarisation so that's another in is whether you have a summarisation token or you ask what is the sunrise in actually I don't think makes that big of a difference it's just now you can query this model in very natural language rather than having to know cannabis petrol token to decorate them actually in a couple slides dead the mole is not confused when it comes to how to generate the answer so for everyone the time clearly how to generate the words to get to the right to get a reasonably accurate they're only trained on that dataset so the squad number here's just a single model it is only seeing spot training prisoners surprisingly even in the case here where we had this particular model I mean if you just have the standard sequence of sequences just generates you know also with the soft Mac's that label so netsense is quite similar but yeah it was actually better able to just point rejection letters for awhile into thinking about me whichever project were just a point to all the things turn off Mike's pacifiers forever when you then try to do translation also how what are you .2 between Airdrie and just have a line then it can be very large and your point a lot of different like you may have like like terms of thousands of potential candidates so that's like a single unifying model for all the things but you could point to a lot of different like a lot of the pass you could actually point interesting side project from this year play in the hole turn on so we the question is how sensitive were the task if we were to through the different task patient kind of trickery on how to train it but we never said this task only Meadowside 0.5% so we didn't do that is it but you could definitely even more architecture engineering and tech there is holfield I don't think you got to write architectural new connection combined reinforcement learning and you say the action space for the reinforcement learning agent or trying to have a couple of different models of me on that site maybe you want to have like 18 layer and like a memory layer and an lstm layer and may be bi-directional you basically figure out all of these decisions so I think would be phenomenal to try to apply new architecture surge not too what's usually been done with his we are not doing much that's ok Sandra just do it slightly better with it as an architect research by we actually try to find a single architecture for multicast morning which we don't know the problem of courses heading to this all this number start a lot of compute I'm in a lot of fiddling around with I can I can only give use of an idea of like how often would we get like as remitting result in this task but I needed this morning same model same set of hyperparameters everything to get to good performance needed a much higher learning rate can you try to combine those two tasks only together and you like a how do you choose your money right now you Tuesday in oh you choose the task only rate from the past that is no bigger than the smaller tasks just doesn't work well at all because it needed this tile anyway the higher learning rate that the small task and small data said turn the light orange overfitting doesn't work well you there neither to work like there's a lot of complexity in trying to do most athletic that's why I just had such an interesting I think we should challenge 47 results we'll get better we we have we have had some ideas already on and how to improve them how do we actually train this whole thing we have tried a lot of different things but in the end this very simple fully joined range treasury actually works the best and that is you basically take a minute the different task and you do mini Badge from their past so basically just going through all the 10th I go through them turns out that that does not work quite as well and if you look into optimisation strategies and no nuts are actually a couple of papers on so-called curriculum you start with the training model with simple proper simple instances of your problem so in translation for instance you start training with very short sentences and then you go to larger and larger sentence it's a long and now it turns out for multitask learning the opposite you want to do anti curriculum learning start with the hardest task and iterate on those for a while and then you add the simple task later on creating is intuitive because when you this very gigantic and Powerful model on a very simple past you just need to classify everything to be positive and negative you train all these weights and you arrive in local Optima that are quite deep and very specific to just generally used to work and if you don't try to get out of it out of this local optimum but that very simple task and then try to generate all these other kinds of words and point to different you know work is never seen before Archer come out of their local at the moment and it's of my intuition of why it actually makes more sense to say let's start required and machine translation in the couple of his heart attack Mulberry general-purpose has to generate a lot of different things creator softmax words that have two point to all kinds of different words of the able to parse all kinds of different Wikipedia couple times finished at the sort of pre-training stage curriculum then you move on and add sort of the simpler smartass so with that relatively simple change that the take us a lot of different experiments to get to actually clothes store closing that gap and now will only sort of 14 away is there still a big gap in the biggest nuisance and issue that we had was with a translator get all of these most things are kind of similar get slightly better and the sort of the toss up but then and roughly similar but translation was rebuilt almost only half the performance in the multitask learning setup and that is because translation was the only task that had a very large soft next vocabulary of words that were in no other most of the other task actually were doing really well with point My interpretation of this was dead the intermediate layers all these representations that we learnt with bidirectional lstm some Transformers they got really good pointed to by credenhill representations that the answer module can point to a very accurately is 1 classed as like I don't point so much anything I basically just generate other words in different vocabulary and so there's kidding representations become less useful for that and so one of the insights in letter one of the ways of trying to improve this now one of the interesting issues that we had is when we improved the model the most single model for Alton Towers you said well but now yourself to go back and Run 10 more experiments on all the Single task to have a proper comparison right because if you tune the thing you care about the new stop tuning the thing you want to show you can do better than than that's not fair so you always want to give as much Chelsea and then focused and determined time to your bass lines and so in some cases we actually set something but then remove different models in our model in some cases like the principles and even more so the gap between larger than the opposite of what we want to show but in general is better for most of the Architecture of all so basically we started with this fully joint training and we have this sort of set of single models that we could in theory with some oracle count just some up in their stores to get the decor so the gap started at 23 we basically centre Kirkham training which lower to get the 15 excited making good progress then we switched from glove and use cold so contextual factors which actually entries to gap lot against everything that better but the 10 separate model then the one singer model that does the 10th ass so the gap bigger but everybody's performance increase so it was still overall a good thing we basically figured especially this machine translation is so we shouldn't just free training squad but we also should include machine translation in this free training and Beginnings for the model doesn't just start learning to point help this to reduce the gap between the 10 separate models oracle and single model to about 5 points and then we bases that ok translation still not that good we just keep oversampling so every time we go through one of these round-robin mini-batch machine translation allowed us to then reduce the gap to just a single point so now we started 7 months ago at 586 and now the single oracle with 10 different models if you worked on the map get 6 and 18 the Rebecca's and tuning in edinger what more sensation in the still not as good as we would like it to be but now as several of the other task benefited the Bunch an hour find Becca score away from having a single model that does as well as 10 different months basically and even more experiments and some ways you could burn millions of dollars on DWS cost here because most of the time we kept the hyperparameters of these different models to say you could also say what maybe this multitask modelled except Himalayas or maybe 19 or later so maybe five more letters and maybe they should be 1000 you know why the dimensions and you could basically run alarm experiments may be hopefully eventually the community leaders that and then we can move move towards that but we figured out pretty close so we moved onto to some other things but basically let someone else's of what happened in this project and this is kind of I think something that I would encourage you all to do as well like you you can chase the numbers for Wylam in some ways you should always be sceptical about your evaluations and in some cases you seen we've seen the penalty community people like basically just optimise blue scores for translation for years and then somebody came out with a paper turns out blue metrics and human evaluations on how good of a translation is this context that correlate just spent years of our lives to network and polishing a bunch of papers and so in some ways all of these metrics have flaws in a roofs course summarisation is a super subject tasking and summarisation for instance when you analyse the errors often realised that were doctors have problems too so for instance they were back there for Jason John and Jeremy all kind of the same right there all that similar distributions similar contacts windows and so on and so weird pictures of names are very similar PlayStation errors you realised oh well you know this article about Jeremy being kidnap me like one word is off and then I call the rest is correct but it's pretty important woodpeckers have like issues for civilisation that are pretty fundamental and I don't think anybody is tackling really well right now so all of these networks have issues I would argue that combined into 10 actually makes it less problematic and more meaningful then looking at each one separately because now you can't use the idiosyncrasies of one particular evaluation metric to just get like your score little bit higher if you just tune with that particular thing in mind it will hurt some the other task and you won't get to the sort of general elke mobile Devon now let's do some analysis of this model and counting comes to one the question that was asked no too kind of generate the right word for the White House in here we busy looked at the distributions of how often the model generated words in these different with the three different mechanisms softmax for capillary context pointers pointers as you can see in the majority of cases of knows exactly how to generate 204 semantic role labelling in squad sequel summarisation that basically uses the context pointer so just points into the context documents and we know for squad that is basically how the data set was generated so that's the only thing that that really makes a lot of sense what kind of colours that in some cases like summarisation it sometimes creates new words or in a bit weren't in the context document 1.20 short relation extraction also sometimes uses the section of vocabulary in some cases to contact point or so for the most part this model doesn't is not confused cured on a task given the scratch and formalism rather then the format of this is the Task just do this particular task you might argue ok I'm not then impressed by you not having the performance by the same with 1 vs 10 separate models in though it's nice if you want to deploy reds like and uses less RAM and all that remains the same size 1/10 excited about this more like the next couple of results and namely through this transfer learning to Meditation in ricability is so here we chose two data sets that were included in the original ten trained a pretrained model on this versus a random model rent in the year again they're the same architecture and preaching means the entirety of the model was preacher encoders decoder in the softmax everything and to add a task were another iwslt language perineum translating from English to Czech recognition task you know very well so sound is dead in Converters much more quickly in the beginning and then there's still a significant but not checking the gap so there's free training on this completely separate kinds of tasks help and that's pretty fighting especially sort of the cricket conversion cycle learning more quickly whatever new task you you come up with which also means in some cases you can get away with last training data on this new on his new task what the meaning of patient is kind of the simple form of transfer learning where you basically just have a type of distribution for your words we mention we have the same for tenantry bank for sentiment analysis and then we analyse this on different datasets in the Amazon product reviews in the Elk restaurant reviews and out the box without any training the mall just got 8% accuracy on both of those datasets that is pretty exciting to see bet you didn't have to train anything just cannot work out the box downloaded from get up and run it so I was slightly different it didn't quite work as well it's another natural language inference dataset but has very tribution different kinds of domains that these questions are asked over here the out-of-the-box attitude 62 but then once you find unit and similar to these experiments your continue to actually train on the status Mercedes 7 litre still 2% gain over randomly initialised how much less do you get away with that we can get away with we didn't and somewhere explain when you're basically like I like everything all these models will still do better with more training data to just kind of a party cannot say like that sort of results right we say well with 110 we might get to 50 and the other model might get only 242 something like that actually also need to analysis to do cancel let's have some fun go to train Newcastle do you can just add it make it into the format contactmusic question simple like Sainsbury type format and then you added you train the picture Mario Celtic and downloaded pretrained model and just added so it's a great question so how does this compare to other pre-trained representations like so in some way counter smaller does everything but when you actually read the paper you realise well it's a separate module for use different Astrid if you want to have a classification task you have a little token in the beginning and you have a different top layer if you want to do a sequence labelling task you have a different top layer she want to do if you can't extraction task you have a different top layer so but isn't actually a single model for all of these different on all the results there's a lot of extra tuning for each of the datasets and task different learning rate for this task different sized or different sets of bird and son so Russell superstar me like me this is it just run everything on first and then we looked into all the details in your someone to fight in the beginning and then the more details to let excited he became is this thing like side of the answer because it is not a single and some ways play better to four pre-training so instead of coal we can have kind of birth at the very beginning everything will get slightly better but you still need to have a lot of the the other sort of modelling architecture on top of it and then the setting is to really get to see the art resolve there's a lot of very specific tuning windows last top players so if you try to unify the Soviet Union you lose a lot of the good performance of Earth and so it's not quite the sort of all just used it and you'll just have saved your numbers and all things I can hardly go out like talk about a lot more but who makes sense to think about some of the ideas from bird like basically the past Line tomorrow morning I'll be very likely the tasks that helps the most for the other task and we should include that nice to have a class tomorrow right now how to do line smiling is very very large and benefits even more from billions and billions of words find the make and model is current question answering model to cover tension mechanism of the question with like in increasing the large contacts so you have to split it also like reasonably well only for like as 500 words or so she wanted to the summarisation and basically after original documents to only 500 words and then try to summarise it a lot of Devil in the details that they didn't have to figure out cos they said well we'll just sorted just like woodpeckers we can take them in and it would be a lot of other stuff that is passed Pacific with those letters are with the bird architecture RBC amazing and we are looking into trying to use a DS on it but unfortunately it wasn't just for the Silver Bullet assault multi-task learning you considered stuff of college plasterers therapist so did we consider powered housing assembly sun some ways with this free training strategy here that's kind of what we did buy basically focusing on this really hard TAS what about the gap in the end was improved by really waiting for like for the task at the very end until after you're gone through over 7 playing all of these really hard cast it's basically most exciting thing so I think you can also do a lot more work in this Direction I mention it's all question pointer and therefore their name in the beginning into we basically just try to play around that a little bit and found that in some cases it actually parametrically works so here we tried a sentence John had a party but no one came and he was all alone what is a story sabra happy the model could a funeral generate a random German words or some words or it's just said you know whatever it actually pointed to of all the words I couldn't find it to in the context of the question is pointed to set stop just one small sample and you could do a lot more you could try to come up with a very large 0 chart kind of classification data you have to be quite creative Italia you can just say or would you take all these reviews and label them as this in a positive negative I think we we need to do more work in that direction somebody who hopefully create azure short cannot pass dataset that is not just your short for distributions or something completely different output we tried a couple and it doesn't always work where you can be adversarial about it you can he looks most similar to is the sentiment positive or negative is this is this sentence possible naked if there was the formalism we had for sentiment analysis and so you could the question more more different eventually or can I get ripped benefited from the word factors of sad being floated negative and an understanding love to all these correlations and deep representation instead there are other sort of sad words in this context or whatever it is and so it was able to point to this but you can be adversarial it doesn't always work but even the fact it is short classification based on what factors for new kinds of questions and we tried a couple of other things like Brian David talking nobody clapped was praying every upset and also got so there couple couple of examples where we're at least they're starting work in a couple of hours that we will try but what I'm what I would be most excited about this eventually actually trying to have a task that combines two different task to so I didn't train it doesn't happen with tomorrow but in theory if you asked what is it that you can Samurai play putting it into German why can't you ask to model for German summary at work eventually they'll be even more amazing but it doesn't work right now because we never asked it sorted for this compositional task this conversation pass questions but yet another interesting line of research that I think it's one from this alright so I hope I could show you that this framework is an interesting a new benchmark for generalized in ok you think it's a reasonably good framework for tackling a bunch of the really hard questions in the field or general language understanding in answering of course learning domain adaptation little bit for the sentiment and sli vs sharing I think it's clear everybody loves weight sharing you want to share as many Weight as possible would like to start it Cove and Albert basically share more more different deeper layers will be great if we can unify that last bit also ensure basically the entirety of the networks and then eventually hopefully get to see returning now there's a bunch of red work the original papers over 100 net of people to other other lines of work but this is actually there at least some of the models and papers that influence us the most in an hour thinking in modelling comes from the two instructors of the class and so hopefully we can then also think about what what's next after all this architecture Engineering in I think one potential answer to that is single multi-task learning for more generalized outcome play 