everyone start again so a couple of announcements first of all thanks to everyone who filled in our quarter survey dissipation in that you're my two little Pac-Man figures for the Pac-Man figures thinks means that almost everyone thinks the lectures at the right pace the most the don't have pre-match evenly divided if we go for help challenging was assignment 3 slightly more people photos two e's and two hard so I guess we're sitting about rectifying that with the sign of four and five the whole bunch of other questions and we've been trying to absorb over feedback I mean one of the questions was what people wanted most from the remaining witches I guess the good news here is really weird very good at predicting what people wanted dead or else everybody just looked hidden this syllabus and write down what it said was a hit in the syllabus but I guess the most popular for answers to topics that they wanted in the remaining lectures were Transformers and Bert both of which are going to be covered this week and question answering which we talked about last week and then text generalization and summarisation and you guys get my back next week to talk about that they also a lot of people to answer this question the different way as to what kind of style was Star people emphasise new research on the latest updates from the field I guess we'll get some of that today as well some people are more interesting successful applications industry or try and do a bit of their calling you and your architecture what's a what is the most popular one I made that at least a few people and wish that we were teaching more linguistic stuff something that makes you feel good out the way things from those with cs224n with this deep learning I mean the truth of the matter is that sort of seem like in the early part of a course there's so much to cover with neural network backpropagation different architectures and so on the reality is that we teach rather less linguistics two in the class I mean the last 4 weeks of the class we really do try and cover some more linguistics tough topics so look forward to that announcements ok so it made a couple of deadline changes and first play a number of people mentioned that they think Simon 5 as a bit ta ring people one extra day to do a silent five the weather today is not a tan but you know this sort of this complex balance here because on the other hand we don't really want to undermine time that people have available for final timer for people who hasn't yet started assignment 5 and we do really encourage you to get on the way on first Direction we decided that the project milestone was really too late if we are going to be able to give you feedback on at that you could usefully make use of so I'm moving the project milestones date 2 days earlier project proposals in our plan hope is to get them back to everybody on Friday yeah so lot of things moving and finally another announcements I guess on this Thursday as our first invited speaker and so if you're in person student you're meant to be here and if you're not able to be here you should know about a reaction paragraph policy and I actually stuck up on the piazza pin post about reaction pieces in attendance an example of a reaction piece and from a pass plaster make a little bit more concrete the idea is what we hoping for something that is in a tonne of work you can just ride 100 150 words a few sentences but want you to pick out a specific thing that was interesting and ride a couple of sentences about what it was and what your thoughts are about it I not just some very generic statement of this was electro about Transformers he talked about Transformers and it was interesting that is not what we want and put the reaction T's went for today so for today is it what I want to talk about is the exciting recent work about contextual word representations I mean it was going to say I was wanting to say all this is the most exciting thing in deep learning NLP in the last 5-years completely wrong cos really this is the most exciting thing in deep learning that happened in 2018 I mean I guess things move very quickly running at the moment had a sort of I don't think is released fair to say that you know it's it's got 5 years of Life exciting things happened last year and we'll talk about that we'll talk about early start Elmo transformer Architects and briefly and then go on to talk about the bush model that's been quite prominent lately backwards a bit first stop where we've been and where we are now and why we might want something more so up until now we still have just head one representation for words which is what we learnt at the beginning of class can you change a word vector 40-in that's what you use in your model and you could do that with algorithms like word to bed global plastics that I mentioned last week progression of ideas in deep learning when looking for an old p or the general just the Resurgence of neural networks that NLP came about sort of at the beginning of this decade these trained unsupervised over a large amount of tea lately seen as the secret sauce and they were the thing that transformed new network from NLP to something that didn't really work to something that worked great so this is actually an old suede of mine so this is a slide I guess I first made for a 2012 ACL tutorial and then till of used and lectures 2013/2014 so this was sort of the picture in those years this is like it tutor tagging and named entity recognition values quite a bit today and you know the top line was throwing a state of the ass which was a traditional categorical feature based classifier of the kind that dominated nop in the 2000 stage in their performance what did the next line showed is that if you took the same day the said and you trained a supervised neural network on a and said how good is your performer free was it wasn't great and has very high Numbers always for various reasons I text the more indicative one to look at it these named entity recognition numbers so you know this was solid and units upright the reason why last dick head everybody used feature based CRF250M kind of classifiers percent better than a new network why would anybody but then what had happened was people come up with this idea that we could do unsupervised pre-training of word representations to come up with word vectors for word stays this was very hard to do the face because of a kind of algorithms and the kind of machines that were available right so Colburn Western 2011 since 7 weeks training the unsupervised word representations at the end of the day the only 100 dimensional and word representations but this was the miracle breakthrough right you put in this miracle breakthrough of unsupervised word representations and now then you'll never this going to 88.87 so it's almost as good as the feature based classifier and then like any good engineers they did some hacking with some extra features gazettes and stuff like that and they got a system that was in Whiteley better than the feature-based system I was told about picture that I'm having these turn on Super in and supervised men of word representation that was sold the big breakthrough and the Secret sauce that gave all the new networks competitors Funny Thing Happened which was after people at side of head some of these initial breakthrough which were all about unsupervised methods for free training it was the same envision this was the envision where your building restricted boltzmann machines and doing complicated unsupervised pre-training techniques on them as well after people have kind of discovered that and start to get good on it people sorry well actually we have some new technologies for nonlinearities regularization and things like that and if we keep using those same technologies we can just go back to good old supervised learning and shockingly it works way better now inside neural networks and surface side of Go-Ahead to what I'll call the study of 2014 to 2000 team picture the picture is actually very different so the picture actually going to show this is from the chin remaining Newall dependency parser that we talked about weeks ago the picture there was you despite the fact that this dependency parser is being trained on a pretty small corpus and million words of supervised a that you can just initialise it with random word vectors trainer dependency parser into a first approximation that just works fine you can get 7% accuracy English dependency parser it is the case that instead you can use pre-trained word embeddings and you do a bit better do about 1% better and so this was sort of order which was yeah these free train on 25 word and buildings are useful because you can train them from a lot more day there and they can know about a much larger vocabulary means a useful I help with real words and things like that and they give you a percent but there's definitely no longer the sort of night and day thing to make new networks work that we used to believe EPA Tier 2 narrative to just sort of say one more unknown words with words vectors just in case as useful for some people building question answering systems 4 unknown word why clay is you got your supervised training day there you define a vocab which might be words for the care five times or more in your supervised training day that and you treat everything else as an ankh and so you're also trained one victor far it has some problems which you have no way to distinguish different uncle 48-in to the o meaning and I Tend To Be problematic for question answering system play to fix that's what we talked about last week is safe words made out of characters I can use character representations to learn Word vectors for the words and you can certainly do that you might want to try that that add some complexity but especially for things like question answering systems there are couple of other things that you can do that were considerably better and have been explored in this paper by dinga Adele and from 2017 the first one is to say well blood test time and counting new words stop provide word embeddings have a much bigger vocabulary than your actual system does so anytime you come across a word that isn't in your vocab that is in the pre-trained word embeddings get the word vector of that word and start using her that will be a much more useful thing to use possible tip that if you see something that still an unknown word rather than trainers and you just assign it on the spot and random word vector and so this has the effect that each word does get a unique identity which means if you see the same word in a question and the potential answer they will match together beautifully in an accurate way which are not going with just and matching and those can be kind of useful ideas to try in digression so up until now gonna head this representation words we ran word to beckon we got a word vector for each word it's worked pretty well bed head big problems so what were the big problems yes is anyone selling a word can have so typically you have one letters which has a whole bunch of meaning so words have a tonne of sensors and yet so that's the biggest the most obvious problem that we collecting the other all the meanings of the word so we talked about a bit more one solution that was you could distinguish word sensors and to have different word vectors for them and I then said something about RC could think of this word vector as a solvent mixture of them and maybe a model could separated but it seems like we might want to take that more seriously and one way take that more seriously is we could start to say well traditional list of words sensors of themselves a crude approximation actually want to know is the sense of a word inside a particular context of use and what I mean by that is you know we distinguish different senses of a word writer for the word star there's the astronomical since and is the and they're clearly differ if we didn't go that's what I'm calling the Hollywood saying cycle then say why the minute there are movie stars and their rock stars in their eyes Stars In Their country stars now all those different sensors some context I wanna rather than would be a verb you know it's very hard if you're trying to work she and numerous instances of a word as to which one counts different all the same so really you sorted one know what a word means in a context mission of the word vectors witches we haven't really talked about in his list of years is also something that we might want to fix and at least one of the models we discussed today take some a man there and that is we've just so have one victor for word dimensions of a word so words can have different meanings O'Sullivan real samanda synthetic behaviour like different parts of speech upper medical behaviour so on some since arrived and arrival their semantics almost the same but different parts of speech ones are a verb and ones and down so they can kind of appearing quite different places and you know what to do different things with them in the dependency parser and even another dimension so words also have staring connotation differences so you can probably think of lots of different words for a bathroom and a lot of first words all means so medically the same but have rather different registers and connotations as to when they are appropriate to use English words on that basis as well kind of solution things we want to solve without new contextual word embeddings I've set up until now just had these sectors words that have one victor think about it what's wrong I mean maybe we never had a problem or any right with salted 6 classed as ago because if you remember back so when we started talking about neural language models well what didn't your language model do at the bottom you've fed into a Word vectors but then you ran across that one or more recurrent layers something I can lstm layer and was calculating these representations each word and it is hidden States as a bit ambivalent they use for prediction they use him style and output States and so on many ways you can think representations are actually representations of a word in context about what happened with the question answering systems has exactly how they were used right we're an LSD and backwards and forwards over a question and the passage and then we say ok those are a good representation of a word meaning and context WhatsApp matching them with attention functions etc so sort of seem like with already invented the way to have specific representations of words effectively you know content of this lecture is sorted basically no more complex than their did it took of while but sort of people woke up and started to know this Ronnie my language tomorrow you generator context-specific representation of words or you just took those contacts presentation of words they're be useful for doing other things with them you know the few more details that really the summary of the entire of this lecture first things to do that was a paper that met Peters Road in 2017 and the year before last and the sofa predecessor to the Southern modern and versions text sensitive word embedding so co-authors he came up with a paper called Kegel an actually already had all the main ideas so wanted was ok we want to do better tasks such as named entity recognition and what would like to do is know about the meaning of a word in context but you know standardly if we doing named entity recognition we just trainers on half a million words of supervised a that and that's not much of a source of information to be learning about the meaning of words and context can we adopt the semi-supervised approach and so that's what we do so we start off with a tonne of unlabeled a there thunder and label day that we can train a conventional word embedding model like word today but we can all start the same time train and your language models something like by sdm language model for step 2 when we using a supervised a there step 3 and safe then when we wasn't one as supervised Paris to be to take her up the tab what we going to do is say well for the input words New York said we can not only use the word and bedding which is context independent but we can use that train recurrent when with moral and also run it over this input and then will generate hidden state wireless tmlanguage tomorrow and we can also feed those in his features into staging model and those features will let work better I can picture that man's this through in much greater detail assuming that we have trained language model and autobahn supervised a data what we want to do is we want to do named entity recognition for New York is located sing we do is say let's just run New York is located separately trained neural language model so we run it through a foreign language tomorrow we've run it to a backward language model we get from there hidden state representation and for each word we can catenate the forward and backward ones and that's going to give us a concatenated language model in bedding which will use these features in our named entity recogniser named entity recognizer itself that we're going to train same sentence look up a word of exile token and bedding for head we can use what we weren't about with Character level CNN and our interns and we can build a character Level representation for which we also concatenate presentacion do we feed these representations lstm layer set the output of the display ostm layer as well as it's normal output we can concatenate with each output what we get from our tomorrow so each of these things for terms of Paris take one that stayed up from the first BIOS DM lair and it's concatenated with something from the new language model and select concatenated representation is there second layer of by Alice gym and then from the output of there we do the usual kind of soft next classification where within giving tags like beginning of location in the location stay New York as the location and then is will get an other tag say it's not a location singers CIMA representations that we get from BIOS TMZ useful we just going to feed them into supervise models as we train them and the idea is that will give us a better features of word some kind of representation of their meaning what's to learn and better named entity recognises that what it for everyday is slide earlier for this fight with me to remind you what a name did he recognise his I hope you remember there something where we going to label into these four things like person location day organisation doing this work a little bit of a history so the most famous named entity recognition day the said is this connacht 2003 and day the said which actually exist in multiple languages but whenever people say Connell 2003 and don't mention language they mean the English version of that's the way the world were so on this day the said yes so sorry been around for whenever 15 properly now so Indian originally competition rights of this in 2003 with the original Bake Off and microphone we took place in I think he got 3rd or 4th place or something and F1 score with a 6 people who won World from IBM research labs and they got 88 almost 89 but a difference between these two things is our system was a single single clean machine learning model categorical where is the IBM one was not only an ensemble machine learning models plus gazetteers it also fit in the app output of two other old systems that IBM people a train years ago on different day that I wasn't work for the complex system and another system from Stanford so and this was a classic standard in the ass system that is widely used to this within using a conditional random field model dominated the turn off the 2004 the 2010 for doing any app and it was so turn on hue sleep better than the 2003 system I'm here was sort of the best ever build categorical CRF system but rather only using the training days of The Bill the model is this system did it through in Wikipedia and other network bed Ark and that got you about 90.8 F1 actually I'm one sort of bioscem style model known and used an nop that was when people were able to build training training day that systems that work to what better because essentially you're going from the same day they are from this system to that system so you're getting about 4% gain on at because of dates of Wikipedia and things like that and so this man Hovey system is pretty well known getting about 91 to 1 we then go to this tag alarm system M and Co have a system that sort of similar to the man Hovey system that is a little bit worse but the point is that this by Ice-T use it sorry using the new language model is just a useful which sort of takes the results and then give some the best in your system that was then available so that sort of proof they still contextual word representations really had some power and started to be useful space at the top because I will get back to more this later details on their language model and some of the details are beautiful have a bi-directional language model not unidirectional is just thought of a big language model to get much in the way of games and trainers Langwith model over much more days her it doesn't work if you just sort of training over your supervised training data that was around with Cove but I think I'll skip that ok so then the next year that paedos in a different set of colleague came up with an improved system and called Elmo and effectively this was the breakthrough system that this was so system that everybody noticed and said well these contextual word Victor's a great everyone should be using them not traditional old word text increase volume YouTube Elsie and Anya train because this piece over here a big neural language model is trained first something I forgot to say so thank you for the question play some quiet free Traders this was train first but the main reason why people think of this is pre-training is after you've trained frozen so this is just something that you can run with parameters which will give you a vector which is your contextual word representation each position and then that's just gonna be used in this system so when you're training this system there's no gradient flying back into this new language model was changing and updating it it's just sort of the sense when people talk about pre-training it's sort of normally a model that you trained somewhere else and that you're using to give features but isn't following the model that you're now training pause reconstruction unsupervised in the sense of this is a language model your training at to predict the next word so he rewards1 Decay what is the K-pop one word do a cross entropy loss which position I mean gone through tag in some detail I mean what is the difference between tiger lamb and Elmo is kind of small it sorted in the details that mean to a first approximation they do exactly the same again but a little bit better and so made since the last time I mean what are the things that different and they do the bidirectional language model a bit differently to try and come up with the compare all the be easy for people to use another task don't have the beefiest computer hardware in the world and so they decided to the spent with having word representations altogether and just use character CNN to build word representations that listens the number premises you have to store the big matrices you have to use a expanded the hidden dimension to 4096 but then they project it down to 512 dimensions with a sort of feedforward projection layer that's a fairly common technique to again reduce the parameterization of the model so that you have a water permit is going in your current Direction but you need much smaller matrices for including and the important the next level sync the layers they now use a residual connection and I do a bit of premature tying so sort of all in the little details there are they interesting things that they did which was an important innovation of Elmo so we should get this bed so integral and what was fed play pre-trained element to the main model was level of the new language model steak and that was completely stand and they return those days that you might players of new language model that you got the top level is your son daily meaning of the sentence in Lower layers for processing that lead up to the idea that maybe it would be useful to actually use all I'll be in your language tomorrow so maybe not just the top layer but always will be kind of useful so there are these kind of complex equations and but essentially the point of it over here is we go secure position word 7 and the language model we going to take the hidden stays at each level of a tomorrow stay give one a white for that level we going to sort of some names so this is so the weighted average of the Himalayas at each position and that will be used as our basic representation and so they found that gave quite a bit of extra usefulness and different task would prefer different layers there's one of the big here which is they learn a global scale in fact the gamma for particular task Nelson to control at the Santos word embeddings might be really useful and father task they might not be so useful so you just have learning a specific usefulness for the entire task so turn new version of language tomorrow this is allowing this idea of well maybe this sort of more synthetic meanings of a word and more semantic meaning of a word possibly those could be represented different layers of your new language model and then for different tasks you can differentially wipe them that's the basic model so you run your BIOS gym before locations of each word and then the generic it all my recipe was well frozen language tomorrow you want to feed it into some supervised model depending on what the task was next sort of saying the PayPal how you do this maybe depends on the task time concatenated to the intermediate layer just as the tag LM dad that might be fine but you know it might also be useful to make use of these Elmo representations when producing output so if you're doing something like generation system you might just sort of feeding the Old my representation again before you sort of do the soft next to find me out but they still have left at flexible as to help with you but the general picture you know was kind of Microsoft before indeed and reusing the same picture that you calculate an Elmo representation from preposition as a weighted average and then you're still can't catch anything that to the hidden state of your supervisor system and generating your output One Way or Another go to do this and the with the little improvements that gave them about Extra 0.3% in named entity recognition the sounds like not very much and you might this why the excitement and you know in some sense that's right because side of The Expendables and interesting idea here really that come up with her for the table in paper which gave a match bet again but you know why everyone got really excited was that in the Elmo paper they then showed this isn't something that you can do one often proven named into the recognizer you can take these Elmo representations and use them for pretty much any in Old Peter very useful and give good games people got excited and worse because of the day that's in this table so here we're taking a whole bunch of very different paths to the squad question answering and there's natural language in French Lower Way playing disco reference recognition doing sentiment analysis a wide range of different NLP task have a previous state-of-the-art system their own baseline which is similar to the previous state of the art actually bit worse from the current state of the art because it's Whatever simply cleaner system that they came up with but then they could say in each case this system an ad Elmo vectors into the kid and representations in the middle and have those help you in all cases at giving your about a 3% or so gain absolute which was in huge increase which and all cases was moving the performance well above the previous and stay the system so you know this sort of them made it seem like magic pixie dust because you know in the stakes of NLP conference land you know a lot of people use to trying to come up the next year that 1% better on one task and writing it up and that's the big breakthrough this for the year to get their new paper out and the idea that this just wildest creating context-sensitive word representations need any task and I'll give you around 3% and take you past the stallion this seemed like was really great stuff and people got very excited about this one there and best paper award at the Deco 2018 conference as I say bakery mentioned so the model they actually use what in a deep stack there actually two layers and feisty end they do show this interesting resolved that the lower level better captures low-level syntex word properties and is most useful things like part-of-speech tagging syntactic dependencies any are the top layer of the was better for high-level semantic search for useful for things like sentiments medical labelling and question answering that seemed interesting that's interesting to see how that pans out more if you had sorted more layers to play with done ahead nothing else that I just thought I should mention a little bit by another piece of work that came out around the same time a few months later maybe so maybe not same time in what's 19 was this work on universal language model fine-tuning the text classification are more own fit by Howard and ruder and essentially this head the same general idea of saying well what we want to do is transfer learning where we can learn a big language model language tomorrow and then for a Tag task which might be recognition but here's text classification we can transfer this language model information and help us to do better with the toss posted in architecture to do that and so there architecture was you have a big previse corpus train annual language morro Bay is the deeper your language model with three hidden layers you then fine-tune your new language tomorrow on the actual domain that you're interested in working in for this was sort of an extra stage that they did and then finally you now location objectives so what they going to be doing is making text classifier um2 internet from a language model into a text classifier faded differently and something's foreshadows the later work and Transformers so rather than just feeding speeches from this into a completely different network they keep using the same network but they introduce a different objectives at the top so one thing you could do with this network is used to predict the next word at the language model and sort this point they freeze the parameters of that soft next at the top that's why it's shown and black instead they can stick on a different prediction you that were predicting stuff for a particular task predicting positive or negative sentiment in a text classification task or something like that Sunday more they sort of reusing the same network but sticking on the top of that a different layer to do the new classification task they will still interest in something small that A1 GPU model of research a lot of details if your new models to maximize performance if you're interesting that you could look up some the details about Dad able to show a game was making use of this language model pre-training was a very effective way to improve performance this time for text classification so these a text classification datasets IMDb is the sentiment and trick is the topical text classification and again they are proceeding systems that other people have developed and they're showing that by making use of this language model pre-training they're able to significantly improve on the state-of-the-art that is it so that lowers going showed another interesting result which and what you would expect or hope from doing this kind of transfer learning that what they were able to show us if you can train this new language tomorrow on a big amount of day that that means you'll then be able to do well on your supervised playing on Pretty Little Daisy play here this is ever rate So Low is good and so what date and he is the number of training examples which is being done on a log scale and so the blue line is if you're just trying a text classifier from scratch on supervised a there so you need a one of day there to start to do pretty well and but if you're making use of learning and fun sign language tomorrow you can get to the choice of doing pretty well with Wales examples of centrally and Order of magnitude list training examples will give you the same amount of performance between these two lines correspond to the extra and that they had in the middle of days and which is whether you're doing this sort of extra fine-tuning on your target domain and your process and they found that to be pretty helpful free cursor one big part of what has happened since then is it effectively people said this is a good idea and maybe I'll become a really really good idea if we just make things why bigger I'm fed I'm with something that you could train in one GPU de sounds appealing for cs224n final projects remember that but well then the people had open AI we could build a a free train line with model and trainer on a much larger amount of data on a much larger amount of computer and use about 240 GPU days and that will get a lot better what Google said well we could train and model 50 days which means maybe about double the amount of computation had to figure out exactly and that might be able to do exciting things and that was the Bert model and it did and then if you're following along these things I'm just last week and the open AI people said well we can go much bigger again and we can train and model for approximately 2000 tpu version 3 days and it will be able to do much bigger again a bit much better again does the GP2 gpt2 language model and which open AI released last week end directions very impressive results and when they're showing the if you're sort of building a really really huge language model over a very large amount of days and then use a language model go off and generate some text teach you a topic that are connected just doing a great job of producing takes so the way this was being done was it human is Right Here couple of sentences in a shocking finding scientist discovered a herd of unicorns living in a remote previously Alex for belly in the Andes mountains so within using a new language model on that so that gives us context and then say generate more text and it starts to generate the scientists name the population after they distinctive horn over to unicorn these four horns still have white unicorn for a previously unknown to science it produces remarkably good and picked examples that I showed in the tech news and it produces extremely good text yeah so I think one should be a little bit cautious about so some of it random output sexy at nearly as good but never left sexy dramatic how good language models are becoming once you're trying them on long contacts as we can do with modern models on vast amounts of data the open AI people decide this language model was so good that they weren't going to release it to the world and which then got transformed into headlines of Elon Musk open AI builds artificial intelligence so powerful it must be kept locked up for the good of humanity with the suitable always turn up at these moment down the bottom of the screen I guess that is it leading even Elon Musk to be wanting to clarify and say that it's not actually really that is directing what's happening at open AI anymore moving ride along so part of the story here is just a scaling thing that these things have been getting bigger and bigger than the other part of the story is that all three of these as in systems that use a transformer architecture and transformer architectures of not only being very powerful but allowed scaling too much bigger sizes so do understand some of the rest of these we should learn more about Transformers and so I'm sort of going to do that pics of orders invited speaker coming Thursday and is one of the authors of the transform of paper and it's going to talk about Transformers I think what I'm going to do is say a little bit about Transformers quickly but not really too well on all the details and the hope that it's a bit of an introduction and you can find out more on Thursday about the details and then took some more about the birth model before finishing PlayStation 4 Transformers is essentially want things to go faster so we can build bigger model plus we mentioned for these lsdm on general any of the current models is the fact that there are current you have to generate sort of one him stay that time chugging through and that means you just can't do the same kind of parallel computation love that you can do and things a convolutional neural networks another Ham we discovered that even though and this gated recurrent unit tmg I use a great get really great performance out of these recurrent models we found that we wanted to problem with them these long sequence length and we can improve things by adding 8-inch and mechanisms that lead to the idea of tension work so great chicken just use attention and we can actually get rid of the recurrent part of the model altogether accident leads to the idea of these transformer architectures and the original paper on this is actually called attention Is All You Need which reflects this idea of we gonna keep the attention part and we get it going to get rid of the recurrent park and will be able to build a great model sewing the initial work what they're doing is machine translation kind of like the new machine translation with attention we describe but what they wanting to do is build a in a complex decoder that works non recurrently and disable to translate sentences well by making you attention distributions I wanted to say the little bit more quickly about there and hopefully will get more this on Thursday 1st in The Resource if you want to home and learn more about home architecture there's this really great by Sasha Rush called the annotated transformer that goes through the entire transformer paper accompanied by pytorch code in the Jupiter notebook and so that can actually be a really useful thing but I'll go through a little bit of the basics now of how we do things idea is that they're going to use a tension everywhere to calculate things for about the different kinds of attention of the thought of multiplicative bilinear attention in the Lidl work out of attention by the attention is just dot products between two things Cardiff do the Morgan they do them more complicated version of Dr between two things where they have things that are looking up her assume to be key value pairs keys and values and so you're calculating the similarity as a dot product between a query and the key and then based on that you're going to be using the vector for the corresponding value you shouldn't here for what we calculating is looking using the 4th May key similarities and using that to give the weightings of attention based waiting over the corresponding values basic attention model sing it that way to little bit of complexity but sorted for the simplest part for their and Coda actually all of the query keys and values are exactly the same they are the words that they using as this source language so add some complexity that isn't really there they're a couple of other things to save do one thing with a note is that are you sure qtk variances the dimension gets large so that they sort of do some normalisation by the size of the hidden state to mention but I'll leave that out as well for details right in the encoder everything is just out word vectors there the queries the keys and the values only use the tension everywhere in the system second new idea is Dennis Grey Dad if you only have one attention distribution but you can only 10 do things one way maybe for various uses of be great if you could attend from one position to various thing so if you're thinking about synthetic dependency parsers if your word you might want to attend to your him word that you might also on the 8-in 8 into your dependent word if you haven't be a pronoun you might want to attend too refers to mine have a lot of attention so they introduced this idea of multi-head attention and so what you're doing with multi-head attention is you have I'm your hidden States system anymap them via projection Wales pictures of multiplication by different w matrices that linear projections into different lower dimensional spaces and then you use each of those to calculate dot product attention and so you can attend two different things at the same time and this multi-head attention was one of the very successful ideas of Transformers that made them a more powerful architecture complete transformer Stein to build complex architectures like be sort of started saying the other way adding from a word vectors with kind attention to multiple different things simultaneously go and have a residual connection that short circuits around done with them going to sort of some the two of these and then they're going to do and normalisation at that point is the bad batch normalisation they don't do best normalisation they do another variant which is Layer normalisation which is a different way of doing normalisation that I'll skip that for now and then transformer block you then go after the multi-head attention you put things to a feed for layer which all size of residual connection is some the output of those and you then again do another layer normalisation so this is them based block that they going to use everywhere and to make their complete architectures didn't go to sort of start sticking these transformable play Deep network and in some sense what has been found is the transformer very well but you know there's no free lunch can't you're now no longer game with carried information exiting carried along a sequence you've got a word at some position which can be costing attention another word information carried along a chain you've got a first of all got a walk the first step of the chain and then you need to have another way of vertically which can walk the next step of the chain and then you need to have another layer vertically that walks the next step of the train so you're getting rid of the recurrence along the sequel choosing some day Mr walkalong notable advantageous and GPU architectures because it allows you to use parallelisation to calculate everything at at the same time slide on explain this as well Karen codines but if you do nothing else words thinnest word that doesn't you have no idea whether at the beginning of the sentence of the end of the sentence so they have a message and method of doing positional encoding which give you some ideas to proposition your word has in the sentence does the encoder system Sophia the word the word embedding you add in the positional and codeine you go into one of these transformer blocks is then repeated in time so you have a stack of these Transformers four times doing a multi-head attention to other parts of the sentence values feeding for the value putting it through a fully connected layer and then you just thought of a p the different places in the same terms formation put it through a fully connected layer and go up and proceeding up deeply how's the little mysterious but it turns out to work the way to think about that I think is the pen each stage you can look with your multi-head attention and various other places in the sentence information push up to the next layer and if you do that sort of half a dozen times you can be starting to progressively push information along the sequence and either direction to calculate values that are of interest the interesting thing is that these models turn out to work really well at source pretend of interesting things in linguistic structure what sort of suggest of diagrams but this is looking at layer 5 of a transformer steak and seeing what words are being attended to by different attention here colours correspond to different attention head sentences in this spirit that the majority of American government surpass new law since 2009 making the registration of voting process more difficult so what we see is most of the attention head from making to making more difficult and it seems to be useful one of the tension head seems to be looking at the word self then the others ones the sort of looking a bit at laws and a 2009 so it's sort of picking out the arguments I'm in modifiers of making in a syntax kind of like way testing Lee for pronouns attention turn to be able to look back to reference so the law never be perfect but it's application should be just on the tension head what is modifying application but another attention head 4/8 is looking strong way at what it's refers back to us the law that seems kind of coal incident for the rest of their model is then some more complex adeva how to use the Transformers of decoder to give you a full new machine Translation system that I think maybe I will skip there and go on and say a bit about but in my remaining minutes contextual word representations to help you through your tongue Victor what is bidirectional encoder representations from Transformers surely it's using the encoder from a transformer network by this deep multi-head attention stack to calculate representation of a sentence that's a great all-purpose representation of a sentence that you can use for tar anti recognition or squad question answering so there's action interesting new idea that these people had and that was their idea was won't standard language models a unidirectional used Focus a gives your probability distribution away with more all but it's bad because you'd like to be able to do prediction from both sides to understand word meaning and context choice which is you can kind bidirectional models when you incorporate information and both ways sort of has problems as well because then you get crossed so if you run a BIOS gym and then emerge the representations by can catenation and then feed them into the next layer when you're running the next layer the Ford lstm will already gone information about the future from the first layer and so it's sort of and words that already seen the future and themselves so you have this sort of complex non generative model are they wanted to do things a bit differently so they can have bidirectional context without Words thing of the see themselves they came up with his well we gonna train things with the transformer and Coda but what we going to do is mosque out some of the words in the sentence Walmart Kia store and gallon and then so I link with language modelling like objective will no longer be a true language model that sort of generating probably of a sentence and we just ended we done by working from left to ride but instead be a mad libs style fill-in-the-blank objective so you'll see this context literally the man went to the master by a mask of me what's your training objective is to say try and predict what this word is which you can do the cross entropy loss to the extent that you don't get store and then it will be try and guess what this word is and you want to let get gallon to your training and model and the fill in the blanks which they blank words is a 51 word and 7 and they discuss how this as a trader if you like to few words gets very expensive to train and if you blank mini words while you've blanked out most of the context of a word and that means it's not very useful for training and they found her outside of 1 and 17 to work pretty well for them want to argue is the open AI GPT which is also transformer model its O'Sullivan classic language model working from left to right eye and say you only get left context for the Bert language tomorrow sorry vol my language tomorrow that's showing up at the top are they running a lift right leg with more Orlando running a right-to-left language tomorrow Sam Sam since and they have contacts from both sides these two language models are trained completely independently and then you just taken Catalina representations together so there's no sense in which I actually kind of having a model that jointly using context from both sides at the time both that the pretrained contextual word representations a bit is using transformer model this Trick of blanking at words and predicting it using the entire can't getting used to side of Conte effective what they seem to show what are the complications show laser complication is a bit useful than a solid not really essential to their main idea was that the head was clearly to be able to have this be useful things string guitar I'm natural language inference tasks in their relationships between two sentences so they're idea was one good objective is this fill-in-the-blank word objective which is sort of like a language modelling objective but they thought to be useful to have a second objective where your predicting relationships between sentences we have a loss function which is two sentences where the sentences might be to success of sentences in a 10 followed by a random sentence from somewhere else train the system to predict seeing a correct sentence vs the random Centre trainee loss based on this next sentence prediction task me something like the man with the what a gallon of milk You're Made to predict true is next man went to the store Penguins of flightless you meant to say false this is an accent dangerously also and training with this representation turn it up this so have for the input that have a pair stainless as my dog is cute he likes playing a representative word pieces like we talked about last week the token embedding for each word peace is a positional embedding Fleetwood sound with the token and bedding and then finally there's a signal in bedding for each word piece we just simply weather comes from the first what was the second sentence before or after the separator there's three things together to get the token representations and then you gonna use those in a transformer model where you'll have losses to the extent that you can't predict the masked word your binary prediction function as the weather for correct sentence or not which is the training architecture what's a transformer as before it's trained on Wikipedia plus the book new model Facebook model was a 12 layer transformer into this corresponded to fight the previous transformer paper and use dried those two layer transformer blocks repeated 6 times gave you 12 players with 768 in mention hidden States and 12 heads for the multi-head attention and then I went bigger and train but large which is sore layers bigger hidden stays even more attention here training these on gods of tpus the volume training spacers for last word next sentence or not then what they wanted to say was this pretrained model play waves on these losses and mash slime with model mix into the prediction and we could then take this model most of it that's wrong we could take this model free trained and it would be incredibly useful for various different task we could use it for named entity recognition question answering natural language inference etc and the way we Gotta Do it is kind of doing the same thing as the old fit model did we not just going to say he is he is a contextual word representation like Elmo did instead with what we going to say is using this keep on using the former network that we trained as a playing with tomorrow fine tuner for a particular task Landis transformer calculating representations for a particular task can a change is we're going to remove the very top level prediction at the bits of predict the mast line with model in next sentence prediction and we get a substitute on a final prediction Mayor's appropriate for the TA Oscar squad question answering our final prediction layer will be predicting star of Spain and indus fan kind of like when we saw Dr q a couple of weeks ago is the ner task our final prediction layer will be predicting the same density recognition class of each token just like a standard in your system the system and tested on a whole bunch of datasets things they test on was this glue dataset which has a whole bunch of task a lot of the class there are inference tasks that kept saying that phrase all this lecture but I haven't really defined it so for natural language inference you given two sentences mountains especially sanctified and jainism and then you can be a hypothesis em jainism hates nature and what you're meant to say it's where the hypothesis and follows from the premise contradicts the premise or has no relation to the premises of three-way classification and so here it contradicts the premise other tasks such as this linguistic acceptability task gluta the the sowing the pre open AI state-of-the-art I'm Elmo work CPT work well the small and large book models work play what's your finding is open AI gpt2 you're pretty good if so actually good advances on most of these time Arthur McBride the previous state-of-the-art showing the power of these contextual language model show form of birth prediction just seen much better again this line of this line you're getting depending on the task about 2% better performance the bird people actually did the experiments carefully so these models are pretty comparable in terms of size but the bi-directional context seems to really help and then what they found was well by going to just a bigger model again you could get another Beacon lift in performer getting from any of the past 2% lifting performance going to the bigger model so this really produced Super strong result found and continues to give super strong results so if I return back to my Connell any artists play giving you 92.2 and you sort of continue to get gain for birthdays get you to 92.4 and Brook Lodge take she's a 92.8 truths in truth and description there is now system and beats Bert large on in which a character Level and transformer language model from Play but you know this continued over to a lot of other things so I'm squad 1.1 and mediately just outperformed everything else that people been working onto squad for ages in particular was especially dramatic was the singer single Bert model and delete everything else that have been done previously on Swan version 1.1 even though they could also show that ensemble of models could give further good and Performance gains as I've mentioned before the sense if you look at the squad 2.0 and leaderboard all the top Rank systems using Bert one place or Another and so that sort of lead into this sort of new world order that ok at seems like the state of nop now is to if you want to have the best performance you want to be using these Transformers stacks to get the best performer making NLP more like edition because really vision for 5-years has had these deep retrain your networks texts and like resonates with from most efficient ask what you do as you take a pretrained resnet and then you find Sharon away at the top to do some classification task you interested in and this is sorted now what's happening NLP as well that you can do the same thing by pretrained Bert and fine-tuning at to do some particular performance task today and one Transformers on Thursday 