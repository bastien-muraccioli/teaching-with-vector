started for today week 5 of cs224n and so this is the plan for today in some sense a lot of this class is going to be an easy class about things like I'm final projects and tips for what you need to do and finding a topic and writing up your work and things like that I'm so far I'm so two-thirds of the class there isn't a lot of hand deep technical content but I hope they're actually just some useful stuff and stop that be good to know about think about this is and took until this year we had a mid-term in this class so you know if we were doing this class should instead be doing the mid-term based on all the material that we covered so far so this should be really pleasant by comparison but that isn't going to be quite the entire class so for this piece here in the middle I'm going to spend a while back on some of the topics of last week so I wanted to have one more look at some of these gated recurrent models and that my introduced last week hope is that now that you had a bit more time to look and read about things and hopefully then have started working on homework for that maybe it starts to make a bit more sense more confusing than before you've got some idea what's your confusions are in questions and so hopefully think about those one more time because I think they are Clyde a complex notion and it's not so obvious what they're doing and why they doing anything useful or whether they're just this what mystery and then also the touch on a couple of machine translation topics does and come up in the final project that we didn't about last week so let's get started so this is how it's working grading that we showed of the beginning and so the main thing I want to do today is talk about this Final project and before that I do that let's just say if one minute on participation so I guess we started in 21st of the participation policy and last Thursday when we took a tendons and that makes it sound riccone in but I wanted to say and the positive viewpoint of the participation points I mean obviously this is a big class there are lots of people and I hope is just that people will variously these sort of engaged and involved in the class and the participation points our way of doing that I mean basically the way this is set up I mean if you do much of anything you should just get 3% for the participation points it shouldn't be hard I mean I will bit sure that there will be some people who at the end will have gotten 7 points in the participation category and then unfortunately we kept you will only give you 3% for the participation category but you know providing you usually come to class or usually like the invited speakers the reaction paragraphs of sdpd students write a helpful answer on tr20 already going to be there on 3% and so one the one other thing and that's a way to get pointers out today so today we're putting up and mid quarter feedback survey and would love to have you filled are there in I mean we would like to get your thoughts on the course so far and here for you guys there are two ways that you can win first if you give us some feedback that can help the rest of your core to be better that we've also got a simple bride built into this and witches you get half a participation point simply for filling in the mid called a survey that be really good to get your feedback on that turn the main thing I want to get to today is to talk about the final Project so jump right ahead I'm into that final project there are two choices you can either do out fault final project which I'll say a little bit about it's doing squad question answering or you can propose the final accustom final project which we then have to approve what's a VAT if you have some outside mentor you can say who they are your project proposal and otherwise and will attempt to assign your mentor somewhere out of the closest star all the assignments for a Simon 2135 you have to do them by yourself the final project in either form of Earth you can do as a team so you can do this one two or three people does that work looks like this if you're a bigger team we do expect you to do more actually two ways you can be a bigger team that I'll point out one way as having more people being to a 3 people that the other thing that comes up is sometimes people want to do a final project for more than one class at the same time in particular to discord I know that at least a couple of people were hoping to do Emma's reinforcement learning class and we are now that as well that we sort of do multiplication because if your two people using at the two classes that means it should be 4 times as great as what one person is doing for one class how how it works with Logitech put on AC Southall because you know the truth is if something is just bad and you are model was broken or you your experiment failed you don't know why I'm here if there's just obvious ways and what you done as bad as it's sort of bad weather your one personal for person and had a few written that beautiful beautiful irregardless of whether you're one person or failed people that you know nevertheless the expectation is that if you're one person will be pleased that if you put together one model and gun at to work well but if you're 3 people will say well that wasn't such a bigger third and running this one model against this task relief there 3 people they could have investigated some other model classes and seen where they perform better or worse on this task and will feel that since of lightweight so we are expecting that sort of both more ambitious Project sorry explanation of them if you're being a bigger team or use it for multiple classes final project you are allowed to use any language or deep learning and framework that you choose to we don't insist on what years now on practice in past years tips on using what they've learned how many assignments to expect that will be true this time as well quickly the default final project so that you've got an some sense of Conte that will be released this Thursday and said for the task for it is a text or question answering task which is done over the the Stanford question answering datasets squad which was a dataset put together by Percy Liang and the department and student so with use this as a default final project and before that we mixing up a couple of things this year I mean firstly that start a code with surveying this year is it the feeling look what we've done to the rest of the class that secondly release the new version of squad squad 2.0 and we're going to use that for the class this year and this central difference in squad 2.0 isn't squad one though every question has an answer in the passage of time 2.0 a lot of question don't have answer so there's this extra significant thing that you need to do which is working out and whether a question has an answer just one example your sense of a swap squad is like so there's a paragraph of text I just put a subset of it here bill waiting the doctor by Mexican movie actress Lupe my area grew up in the neighbourhood town neighbouring side neighbouring camera Madeira and his song Chronicle the hardships faced by the migrant farmworkers he saw as a child question in what town did Bill I misspelt that sorry should have been a can without and I have got confused with our former department chair Alexei can I guess when I was typing bill a can grow up and the Answer you're meant to give his Madeira incidentally the random facts you know about something that was recently in the kind of Technique technique and we going to talk that later in the class Paul anthem Google produce this very strong you natural language understanding representation model called bird and which is one of several kind of Turner Classic I can text or remodel words are coming to prominence in 2017-18 and in general vs produce very good performance for very many tasks indeed if you look at the squad 2.0 leaderboard online I'm at this url what's your fine is that all of the leading systems used in some way or another these days but nevertheless this is actually questions but got wrong and that said no answer this question rather than getting the correct answer even that looks kind of straightforward reading as human being it doesn't look at humour reading comprehension question sets the default final project so on Thursday I'm going to talk more about the final project I'm going to talk about how people will text your question answering systems and the details on the default final project should all be posted by then the best just to give you a bit of context to what the other choice is and today and sort of more going to be aiming at people system final project but let me just say say a bit first about the choice between the two of them why might you want to choose the default final Project I have limited experience with research clear idea of a research projects you want to do this quarter you're just really busy with other classes that are you unloading cs140 and you're just really well less than other classes you're doing this quarter you'll be happy to have just a clear goal towards to work towards and leaderboard of your fellow students that you can compete against do the default final project I think from many people it's actually but good right choice What It's Worth I'm in typically over half of people have done the default final project it's normally that so 55% of Sunday 4th Bible project and the rest of custom final the default final project you get lots of diamonds you get lots of scaffolding there if clear thing what you do and the cost Starfire and general most prepared and most able to help you and in particular I mean the bottom pulley here I mean you know something to think about in making the choices did some of it comes down to how committed organise and kenai you should be wanting to do your own custom final project if you got it something you really want to do for a custom final project great we love to see interesting custom final Project stop doing something that just look not done as well she would have done that if you just done project you should probably choose the default are you doing this think you'll do the final project I hope there's some of this lecture will still be useful for the time the middle when I talk back about him try and get a recurrent networks will definitely useful but beyond that some of the tips on doing research and discussions of so they're looking at how to make your networks work does paper writing l a good topics that apply to the final project as well Direction home if you have some research project that you're excited about possibly it when you're already working on or possibly that you've just always wish to do something exciting with neural networks and that music well you know the custom final project as an opportunity to do that I'm such a chance for you to do something on your own obviously if you're not interested text or question answering that you think you might like machine translation of it's not a chilotii and to choose any topic of your own it's also a waiter sort of experience much more of the research because you know for the default final project it's a bigger more open-ended thing than any of our signs the default final project is still sort of a preset up saying that you don't have to find your own problem find your own days are work out a good approach to heard a lot of that sort of been done for you so the for custom final project is much more your own job to save define and execute a mini research pros that stuff seems appealing or some of it seems appealing and then I'm at the cost and final Project singers just reminded me of that effect about assignment 125 you know for assignment 125 think that I can be a set of stepping stones for learning how to build deep learning systems but you know one of our goals in there you left hand holds as time goes by so you know the silent one was really easy and assignment three we tried to make it really handhold e so people could start to learn pytorch hoping for a silence 4 and 5 that they're actually harder so that you're getting more experience of working out how to build and do things by yourself because if the only thing you'll ever see is completely scaffolder the sign man's it's sort of like when you do cs106 say that you have a great job on the cs106a assignments by don't really know how to ride a program by yourself what we want to get you beyond in the letter to assignments so I hope you have started on assignment for if not you really should start and get underway soon as Evie was sympathizing so this year for the whichever one you're doing we're actually putting more structure in then we have in previous years to encourage people to get going and some particular there are early on components which are worth in the grading so the first part of that is a project proposal which is we want from each team so 1 per team you can just do a joint one which is worth 5% and so we're releasing the details on Thursday which is when assignment for is due and it will be due the following Thursday so actually having an interruption in the sequence of current design and tried so for the next week and what the thing to do is project proposal and then the week after that we back to assign at 5 and then we go full-time into final Project I think that the project proposal is for actually want you to do a little bit research on the fire reading some paper so find some papers turn to your research going to do reader what it does write down some thoughts on how you could adapt to extend ideas in it in your own final Project and then say something about what's your plan is for what you hoping to do for your final project and especially if your due a custom final project doesn't water right there cos we'll wander make sure that you have some ideas to what day you can use and how you going to evaluate said where it's a couple of those things are actually sort of determine for you if you're doing the default final Project so that we can have a project milestones and which is the progress report where a hoping that you can report that you're well along in your final project that you've run at least some experiment and have some results on some days out then you can talk about so that the project milestones is due on Thursday March 7 and so is actually more than halfway through the period that sort of dedicated to the final project so if you're not past halfway cost-effective the murderer is always take people time to get going but never left you know what you should have in your head is unless you're halfway through time you're handing in your project milestone then you're definitely behind and you'll be doing that typical stand with thing of having a lot of late nights and lack of sleep in the last week of class trying to catch up for that so I'm just start saying a bit of you some final projects of some of the solid thinking and types of things that you could do about Dad I'm so you have to determine some Prodigy if you do a custom final what's the of science you know there are basically two ways for any field you can have a project you either start with some domain problem of interest nothing you're interested in of saying I'd like to do better machine translation and then you work out some ways to address at with technology or you start with some technical approach of interest play oh well those lstm same kind of me but I didn't understand why there's that extra 10h and I think I'll be better for changed in this other way and you start exploring from a technical direction to try and come up with a better idea and then you wanting to prove that at work projector people do for this class this isn't quite an exhaustive list but this is sorted in general what people do so the first category and really I think this is the book of projects over half is people find some task for application of open your network model effectively as possible category where people that are concentrate on implementing some compare text Jack and getting it to work on Sunday there and so let me just say a couple of sentences on there play ok for you too implementing some existing model people that's as far as they get auctioneers and is that ok and the answer the weather that's ok sort of largely depends on how complex when your model is ok I'm going to something like we seen already like a and window based classification model and you just reemployment Dad and landed on Sunday and get some results and stop that's definitely a bed project but there are lots of very complicated and sophisticated new all architectures out there and if you're trying to do something complicated well then that can be a fine project I'm so I actually saw a stuck in a few examples of project so I mean he is one that was actually I'm from a couple of years ago so this was in the 2017 class shortly before the 2017 class deepmind to one of the organisations producing the most complicated new all models had just released a paper about the differential neural computer model which was a model of how to have something like a differential differential Turing machine like architecture inside and you'll network this would be a great challenge to try and we implement the difference between your computer and Pritchard deepmind released any source code for because they're not the kind of place then generally releases their Source Code and extremely ambitious project because it was it's a very complex architecture which is hard to get to train and so you know at the end at the end she hasn't been able to sort of train as bigger model or get as good results as they report in the paper that you know frankly we thought it was pretty miraculous that she managed to get it working at all I'm in the period of time we had in the class and she did successfully do an open source implementation of this model which basically work the same as in their paper they're not quite as well that's in the future achievement so you certainly can do something of that sort so so sorry for a technical Direction have some ideas for very model and explore how to make a different kind of more class than look at how it works on some problem that works well project you can do is an analysis Project interesting something in natural language or something on the behaviour of neural network did you want to analyse the more closely sync all maybe these neural machine translation systems work great providing the word order is the same as the source and target language but can they really do a good job of re-ordering phrases for different language types how much does a performance very based on the amount of re-ordering between the source and target language and you can do some experiments to try and investigate that as an analysis problem that book sort of model and we sometimes get to project like that what's the rarest kind of project which is when some people try to do something theoretical which is to prove some properties of a system so this is easiest to do and simple systems for something like word vector want to prove about the kind of spaces that are induced by word vectors and what properties you need to have in models from word nowadays to work or something like that another couple of examples that shows some of the other classes is an example of find a problem and build some models so these three people and looked at Shakespearean sonnet generation and then they considered several different models for Shakespearean sonnet generation and got the best results from the sun see all the details but they have a son of a mixture of Word level and Character level and gated model that feeds and a word level lstm and produce the sonnets in the advert wasn't totally bad time and space is formed shell cover now all fresh beauty my love there will ever time to Great forget each like ever disease in a bestat worship his glory died ok it's maybe not perfect is that one already example of someone who designed a different kind of network and this was a project came out of this class that was then continued with them that they got a conference paper out of the I clear 2017 paper so this was looking at doing a better job at building a new language model and essentially they had two ideas both of which same useful for building better new language models and so one is that in the stuff that we presented so far weather was the only word vectors or whatever presented last week in the neural language tomorrow there effectively two vectors for each word there's one for the word and coding on the import and then when you have the soft Mac's on the other side effectively the rows of that matrix that going to the soft Mac's are also word vectors for determining how likely you are to produce different word these two people had the idea that maybe if we asking the model typos to wordbit vectors together that would help and produce the better model and she done several years ago and that was a novel idea which actually been done to this was done the 2016 class had the second idea which was well maybe doing the kind of cross-entropy one correct word that you're meant to produce work out the Lost based on their baby that's not very good because you don't get points if you produce a different word that semantically similar and so that they had this idea that they could use word vector similarity and then you'll be giving a score for any word that was produced next based on how similar at 1 get the similarity to the word that you're meant to reduce me are useful idea that they are able to produce models way is an example of somebody from last year and who did analysis Project pier one I'm evaluate on some tasks he did several time similarity analogy and the squad and question answering system but the question was ok I love your network models of big and so Android suitable for phones get away with compressing the models of lard so that rather than having white 3 and 16-bit flights and now use quite a bit in your networks could we it's a lot more quantize numeric values so that we can only be so using two bits for the parameter so they literally for bits for a parameter and if you do that naively it doesn't work but if you're explosive ways of doing at and see how to make things work in actually get to work really well and in fact it actually seems like sometimes you can improve your performance doing this because the quantization X is a form of regulariser find what's about The Prodigy lying if you look at work cs224n pages and you should if you wanted a final project you have to find some place to start your 1 places to start looking at papers there's online Anthology of most of the NLP conference papers you can look at in mL conferences have lots of relevant papers as well you can look at papers that cover lots of top I know I don't also forget the advice down the bottom and we cheers look for an interesting problem in the world so I CS emeritus professor at Viking down likes to quote the advice of his adviser Hood Simon of if you see a research area where many people are working go somewhere else you know in the context of this class don't go so far away that you're not using neural networks or an opp because that won't work for project for this class but you know never the less I mean in sunset to bed strategy of saying let's look at all the papers that were published last year let's WhatsApp working in one of their problems or lots of people working on Question answering I'll do it to you know interesting different problems in the world and if you know some here cool website that somehow does something interesting related to language you know maybe you can make a final project out of that other ways to find final together most of the cs231n content was Andrew Andre capacity and who now works at Tesla and among his other things he did for the world he put together this site archive sanity preserver and which is a way to find online archive papers which is a major preprint server and if you say a few papers you're interested and I'll show you at the papers that you're interested in your show papers are currently trending so that can be a good way to learn you think I'll be just good to be in some competition for you and wanting to build a system that's better than other people's and you can look at leaderboards for various can you recite which is pretty good though not completely corrective people's with koh.com and it live leaderboards for whole lot of machine learning terms of language one so it gives leaderboards for question answering machine translation named entity recognition language modeling how to speak saying all sorts of tasks you can find there and find out what current state-of-the-art and Davis it's are so it's a different but often for larger projects the things you need to be making sure of is it something that you can get a decent amount of day that about toucan training model it's a feasible task it's not so enormous you can't possibly do it in 4 weeks want to have some evaluation metric and normally for deep learning you have to have even if you hope to do some human evaluation as well you have to have some automatic evaluation metric doesn't waste this just some code that you can run that gives you a score for how well you're doing then unless you have that you just sort of can't do the Deep learning Trick of saying ok navigation to optimizer scores according to this matric and pretty much and want to do that to be able to do your network optimisation with quiet there's an important part of nop in your class project and it doesn't have to be the only thing you can be doing reinforcement learning as well or you could do it images to caption so doing joint vision end in o p but there has to be an opium den last bit before I get back onto that content from last week so something that you need to do is have data for your 1 people collect their own data for a project and you know it's not impossible to collect your own data especially if there's something you can do with an supervised data you might be able to get it I just thought of crawling interesting website set a table small amount of data yourself if you have any site that has some kind of you know ratings annotations stars on it you can as a form of meditation right so if you want to lie Amazon product review websites reviews on product review websites to People Like will they get star ratings at the bottom from people and then you can try and fit to that as your supervision people have data from an existing project for a company you can use their but never list of most people giving the class as a short and things like that the practical thing to do is using existing to re today the set this being built by previous researches that normally gives your fast and let you get to work building models and there's obvious prior work there are based lines and previous systems that you can compare your performance on etc I'm so where can you find Daisy and I just mention a couple of places here in there lots more so traditionally the biggest source of linguistic data used by academics was this place called the linguistic data Consortium and they have lots of datasets for three bags and named into these and coreference parallel machine translation days at ETC etc and so the linguistic data Consortium licences their days at Stanford pays that licence so you can use any of it but if you want to use that you go to that and linguistics stanford.edu page and it's a sign up peace sign have a sign-up where you basically will you stay that only for good stand for purposes and not as the basis of my startup and then you can have access to that day that and it can be made available by us or otherwise time is gone by the ton of curated in opey de that's available on various websites and effective anything the problem is it just thought cabinets of hard to find different things but there are some sites that have a lot of data for various purposes so anything related to machine translation or just parallel and data across different languages the statistical empty stem.org side as a great amount of days are and that organisation run every year the workshop on the same translation wmt which Abbey already mentioned in her class and they've got data said that we use for those toss and then they're leaderboards for those past and you can find data for that tenancy housing was cold and this the Universal dependencies as Powell it's not powerless over 3 banks in the same annotation scheme for about 60 different languages and you can work on parcels to different languages and things like that I'm not going to bother you with going through all of them that you know they're just cans and tons of other day that says that Facebook has released data says Google's release date send further release several other day the States including the Stanford sentiment treebank and the Stanford 4-person new question answering data popular am conversational question answering and other groups of different universities of release date of said they're just Hands of them you can find data on hey Google where has machine learning competitions sites with list of day the same you can look at research papers and see what data sets they used and of course you can ask the cost after on piazza to try and find suitable datasets for a project at the project so I got a bit more of the same later that joint projects does anyone have any questions up until now on projects well so now we gonna say flipper switching our brains and go back and have one more look at gated recurrent unit and what happens and what they mean this is sort of play material that I be presented presented a little bit differently but you know I hope it might just thought of give one more way of sort of thinking a bit about what's happening about these gated recurrent units and why they might be doing something useful and what are the alternatives to then so if you remember the problem we started with is wanted to understand beckwood and time and say the idea of that is well if we will all this a little bit at time t how much are adjust menhir how much effect is that going to have steps later well we still have looked at the derivatives we sort of saw we got the terms for each successive time step and so cast and the problem is that for the derivatives that we got we kind of got this matrix form for each time step and so different going through a lot of time step matrix multiplies and if the result of those matrix multiplies pretty much either things disappear down 20 or exploded up with depending on what was in The Matrix stop and say that something means week when the grading vs Euro we kind of car whether there is just we can't measure out and say that sort of made people think that maybe this night current your network transition function just isn't a good one to use leads into these ideas of gated recurrent unit we have for recurrent neural network where we're sort of feeding for the each step in time well what happens is when we back propagate we have to back propagate through every intermediate no and that's where we sort of have our gradients disappear an idea of how you could fix there is thus a well suppose we just put in the wrecked connections that will longer distance then we will also get Direct backpropagation signal and so then we wouldn't have the same problem of vanishing gradient play with suitable two ways in which you can achieve that affect you can achieve that affect which Abbey looked at in the end part of the last week's chat with this idea for tents attention your actually are creating a shortcut connections I'm from every time step and using at the calculating the tension distribution but the way the tension was done that we looked at your sudden rushing together all previous time steps into some kind of an average gated recurrent unit 603 want to achieve the same kind of ability to have shortcut war control and adaptive fashion where we still do remember the position of things how can we create an adaptive shortcut connection we start to do with the put into a gated recurrent network we sorted so let's have a candidate update which is exactly the same as the one that used in the simple recurrent neural network do is add a gay calculator value from 0 to 1 we're going to do here is mixed together using update for just just like a simple recurrent neural network then mixed together with simply directly forward the hidden state from the previous time step doing that we're saved in adaptively to play using a computation from one time step back as a recurrent neural network and we're partly the timer partly inheriting they couldn't stay from the previous time step so it's sort of like a shortcut connection but we waiting as to how much we shortcutting and how much we doing our complication troll that adapt using a calculation to set the gate and we do that with the sigmoid computed over the import and the hidden previously and stayed and using as in an equation kind of like a simple recurrent neural network you know if you wanted to go a bit further than that you could see time my XJ get rid of the stuff that was in the past that maybe the stuff in the past sometimes becomes irrelevant like maybe sometimes we start and you sent into a new thought and we just want to get rid of the stuff that's in the past leading to this idea of having a second gate and reset guys and said I reset gave calculated value 4021 just like the other day doing this element wise set guide and the previous hidden stayed and that's them sort of saying well maybe we want to keep some part what previous way and some parts that we now want to throw away and so we put that into the model as a second guide and so an interesting way to think about that is the sort of think about this I'm recurrent neural network is like a little tiny computer as the kind of little tiny computers and you might do any sort of simple architecture class that way I'm for the basic recurrent neural network the way the tiny computer work got a bank of registers age your hidden stay and at each time step you have to read time step you have to read the entirety of your bank of registers computation and then you write the entirety of your banking register about computer architecture that sounds like a pretty bad way to implement a simple computer what a gated recurrent unit is doing is saying well maybe we have a slightly more sophisticated little baby computer instead of there we could select a subset of the registers that we want to read and say the reset gave can control that because it can say what just ignore a bunch of the other registers computer new Value based on registers and then the update Gate which is also a depth of can say well I want you to write stairs but the rest of the registers will just keep their previous value why did having a computer and so that's what we doing here and so this model here is from what was Abby presented S as the gated recover much more realistic and it's 11 something's overlaps with the ideas of attention units Electric White new model and the model that was done white earlier and had huge impact is little lstm long short term in with you that's a bit more complex you know sort of the same right so the hidden state of a gated recurrent unit is kind of equivalent to the cell of the lstm so both of them are using the same idea of something together and mixture of just a Ripley interpret directly inheriting what you had from the previous time step something that you calculated for the current time step and the way you can calculate it for the current time step is exactly the same in both cases this is again the truck calculator play using a source of simple rnn those parts are exactly the same lstm is a little bit more complicated it now this extra hidden state that then worked out with a bit more complexity TM picture in my lstm picture looks if you saw pull apart all of its mate play there are three gates so that you can no everything so you can forget to ignore the import you can forget to ignore parts of your previous hidden stayed and you can forget her ignore parts of the cell when calculating output and each of these is I say forget or ignore parts of what that's meaning is your calculating a vector which is then going to be element wise x play the previous hidden stayed or the cell one said that's why you have this effect of now and addressable bank of registers where you can use some of them but not others of them the bottom part of the lstm is just like a simpler simple recurrent neural network then calculate translate update both of the giu and the lstm the real secret is just keeping on multiplying stuff you add two things together is why you don't get the same vanishing gradient evil effects because you're calculating new candidate update and you're adding it to stop that was previously in the cell and that gives you a simple gradient when you back propagate back that you have to rectilinear connection between cell at time T and the celt time t - 1 really that simple addition there so the secret of most of the power of lstm idea of adding two things together is also the secret and many of the other advances in deep learning recently so in Vision and last couple of years the sort of standing model that everybody uses resnet residual networks and they use exactly the same secret of allowing these adept where you add together a current layers value serotina value from the layer below and other things that you simple ideas of things like highway networks and so on to that proven to be an extremely powerful idea slightly different from the GI you because when we look back at its equations that the you kind of dublinia mixture when you have one Gate value UT and 1 - UT where the lstm adds values control by two different gates at forget gade and an input gade theoretically the ending of two separate Gate rather than a mixture is theoretically more powerful and depending on the application sometimes that doesn't seem to make much difference but there's definitely a theoretical advantage to the lstm there maybe a little bit more helpful disenos again van caterer current units using is that some kind of idea is the why did people come up with these things and why do they make sense let you know Neville era of 2015 package to use weather is pytorch tensorflow in xnet whatever you know it just comes with lstm s&g I use and you don't have to program your own infect your disadvantaged if you program your own because if you're using the built-in one it's using and efficient CUDA kernel from NVIDIA where is your custom built in Warrington all around 3 times slower and so you know the same she don't have to know how to do it and you can just take the attitude that an lstm is just like a fancy recurrent network which will be easier to train and that's true but you know architectural ideas of actually been Central to most advances in deep learning in the last couple of years so they sexy good to have and I have some sense of what were these important ideas that made everything so much better just said the same kind of component building blocks you might also want to use in Custom models that you designed for yourself machine translation so translation that we sort of them cover next week that lots of people being seeing and getting confused by in the assignments I thought is thanks an explain where do ants come from and why are they on reason 5 lyrics play Kinder for efficiency reasons so if you think about producing output in a new machine Translation system and really this is the same as producing output in any natural natural language generation systems such really the same for a new language model all that if you have a very large output vocabulary it's just operation so you have a big M soft next trematodes where you have a rope every word then you have an animation that is not working for me there we go so then we have some hidden state the recalculated in our recurrent neural network and so what we going to do is sort of x and BetVictor by every role of the Matrix put it through a soft next and then get probabilities at out putting if we were there and you know this seems pretty simple but the problem is that to the extent that you have a humongous vocabulary here you just have to do a humongous number of rows of this multiplication and Alexa turns out that doing this is the expensive part of having an annual machine translation on your language model system write the lstm might look complicated and hard to understand but you know it relatively small vectors that you multiply or docked at once and it's not that much work where is if you have a huge number word this is a huge amount of work so just for instance of the pirate pioneering sequence-to-sequence new machine Translation system that Google first aid they ran and a tpu machine because they have lots of GPUs but the way they said it up the maximize performance was of those 8 GPUs running a deep multilayer neural sequence model and the other 5 GPUs the only thing that they were doing was calculating soft NEXO speakers that actually the bulk of the computation that you need to be so the simplest way to make this not completely excessive is to say limit the vocabulary yeah I know words in English and if you look adverbs a lot of them and it's going to be huge number words maybe I can just make do with a modest vocabulary will be near enough surely 50000 common words I can cover a lot of stuff play that was so the starting off point in your machine translation that people use the modest vocabulary like around 50000 words do that well happens if you have this is an unknown word that's not in my vocabulary answer the two kinds of hugs they can be Alex in the source language this sort of optional because Alexia problem having a large source language vocabulary if you sort of train the model on a certain amount of data there somewhere you aren't going to seem so you are going to have words that you just didn't seem your training day that and you won't have any pretrained or train word vector for them and you can deal with that by the just reading them is Uncle giving them a new word vector when you encounter them that's the tricky part is on the translation that you're wanting to produce these rare words but they're not in your output vocabulary so your system is producing the anchor and the ankh which is not a very good translation really and so that was solid but the first machine neural machine translation systems did and so you're obviously that's not a very satisfactory state of affairs and so there's been a whole bunch of work out Mr how to deal with this so you can is there a while U2 charger output vocabulary without the computation by excessive so one method of doing that is to have what a hierarchical softener just having a huge and matrix of words you sort of have a tree structure and your vocabulary so you can do calculations with hierarchical and multiple small soft mixers and you can do that more quickly go to always always things and detail now and just started very quickly mentioning moment if anyone's interested they can look people use the noise contrastive estimation idea that we saw it would the Vic and in this context as well so this is a way to get much faster training which is important and it's not really a way to solve translation time video this means you can train your system and 6 hours and serve 6 days that's a big win and so that's a good technique to use much smarter things so really Cadbury problem is basically sold now instead of kind of things that you can do if you can produce subsets of your vocabulary and train on particular subsets of vocabulary the time and then when you're testing you adaptively choose kind of a likely list of words that might appear in the translation of particular sentences or passages and then you can effectively work with an appropriate subset of the vocabulary and that sort of an efficient technique by which you can deal with an unlimited vocabulary but only by using a moderate sized soft next for any particular paragraph that you're translating people that talks about Batman not ideas you can use the tension when you do translation the idea talk about the end of last time so if you have 8 inch and that sort of means that you can't you're pointing somewhere in the source and you know what you're translating at any point in time so if that word is a real words not in your vocabulary there are things that you could do to deal with that I mean first way if it's a real word how much more likely to be concerned so you might just look it up in a dictionary or word I'm sticking its translation sometimes it's appropriate to do other things I mean quite a lot of things that are noun words to be other things like DB numbers of FedEx tracking ID for kids hub shark the things like that a lot of things like that the right thing to do is just to copy them across things that people looked at as copying models and in machine translation the more ideas that you can we can get into the solstice next year and next week we're going to start dealing with some of the other ways that you could solve this but I hope they have given you a sense of sort of what these anchors are about why you see them and that there are sort of some ways that you might deal with them but you're not expected to be doing that and for assignment for and I just want to give 18 Emits more on a valuation so every said a little bit about a valuation with blue and that then comes up in the assignments I just thought I'd give you a little bit more context on that since you've been quite a few questions about context here is your how do you evaluate machine translation quality and sort of to this day if you want to do a first-rate bangup evaluation of machine translation quality the way you do it's you get human beings to assist quality you type translations and you send them to human being so they could bilingual skills and get them to school things are two ways that the commonly use one instead of rating on scales for things like adequacy and fluency of translations in another way that often works better is asking for comparative judgement so here are two translations of this sentence which is better so that's your fault still out gold standard of translation another way you can evaluate translations use your translations in the downstream task could say I'm going to build a cross-linked or question answering system and in that side that system I'm going to you translation I'm going to translate the questions and then try and match them against the then my score will be how good my question answering system is and so the machine Translation system is better if my question answering score and goes off I mean that's kind of a nice way to do things because she was kind of them taking in runaround lady needing human beings and yet you do have a clear numerical measure that's coming out the back end but it's sort of has some catches because you're often they'll be a fairly indirect connection between you're in town what is the machine translation and I might turn out their son aspects of the machine translation like whether you get a green of the Endings right on nouns and verbs or something directly just irrelevant your performance in the task and say you're not assisting all aspects of quality the third way to do it is to come up with some way to score the Direct are still here and the Direct task isn't machine translation a valuable tool really the last 25 years when people doing machine learning models because as soon as having it automatic way the score things you can then run automated meant to say let me try out these 50 different options let me start very nice hyperparameters and work out which way to do things what is only grown in the deep learning era when all the time what we wanted to do is there's every disco m2m systems and then back propagate throughout the entire system to improve them and we doing that based on having some of jective measure which is automatic metric and so to the development of automatic metrics trying assess machine translation quality and the most famous and still most used one is this one called Blue send we have a reference translation done my human being so sometime a human being has to translate which piece of source material one make a machine translation and you scored based on the extent to which there are one or more word sequences that appear in the reference translation and also appear in the machine translation working out in G precision stores for different values of n so the standard way of doing it if you do it for 1 g by g trigrams in 4 g of word sequences of size one before and you try ones of those in the machine translation whether they also appear in the reference translation new tricks at work here one trick is you have to do a kind of a bipartite matching just can't be that third on in the reference translation somewhere I've got a good example here example Example and it doesn't seem like you want to say ok because there's a dot in the reference that means that this that is right and this that is right and this that is right and every other that is also write that sort of seems unclear so you're only allowed to use each thing on the reference once in matching in Grand but you are allowed to use a multiple times with different order in grams you can use it both any unigram by Grant program and 4 grand the other idea is that although you're measuring the position of ingram's that are in the machine translation you wouldn't want people to be able to cheat by putting almost nothing into the machine translation so you might want to game at by no matter what the and ears if the target languages English you could just stay my translation is I'm pretty sure that will be in the reference translation somewhere and I'll get points for unigram and that's not great but I'll get something for that I'm done and so you wouldn't want there and say you're then being penalised by something covered reverdy penalty if your translation is shorter than the reference translation blue metric is forming a geometric average of Ingraham precision of the sum in normally it's sort of the force how it's done where the weighted geometric average where you putting weights on the different in grams Simon we're only using the underground and by g so good so that means we bring a weight of 0 on G and 4 g and so that's basically what we doing I just mentioned a couple of other things you might think that this is kind of random and so people have used this idea of rather than just having one reference translation we could have multiple reference translations because that way we can allow for their being variation and good ways of translating things because mine was always lots of good ways that you can translate 1st weird but people have also decided that even if you have one translation provided independent and I'm kind of statistical basis you're still more likely the match at if your translations are good translation so probably ok where was originally and introduced Lucy Made marvellous and people do grass like this showing how closely blue scores third way immense of translation quality however like a lot of things in life what's the great measures providing people at directly trying to optimizer and so what's happened since then is that everybody has been trying to optimise blue scores and the result of there is the blue scores have gone up mess of Lee but the correlation between blue scores and human judgements of translation quality have gone down massively and so in this current stays that the blue scores and machines senior the scores of human translation so you know according to blue scores with producing Almost Human quality machine translation but if you actually look at the real quality of the translations they're still will behind human beings and because you could say the metric has been games things help beginning Worcester four anthem out that last time that 12 minutes I just now Anna and return to and final projects and say a little bit more and about final how many different ways can do final go through the steps simple straightforward project this kind of the steps that you want to go through so you choose some tasks summarising shorter version of a text some data set that you can use so this is an example of the kind of tasks that there are academic datasets for that other people have used and so you could just use one of those and that's you're already done or you could think there are no I must to cater for that I'm gonna come up with my own data said and get some online sauce and do it and you know summarise of the kind of things you can find online and produce your own dataset play bigger in just after this about separating off and datasets for training and test data source of a that that support and then you want to work out a way to evaluate your system including an automatic evaluation I'm normally for summarisation people use a slightly different m called Rouge but it's sudden related to blue it's name and it's the same story that it's sort of works but human evaluation is much better and that you need so you need to work out some metrics you can use for the project next thing you should do is establish a baseline so if the work on problem then might already be one there's not bad to try and calculate one for yourself anyway and in particular what you should first to have is a very simple model and see how well it works so human language material often doing things like bag-of-words models whether they're just a simple classifier over words or annual bag-of-words everything word vectors it's just useful the try that on the task and see how it works see what kind of things are already gets ride what kind of things are gets wrong you know one possibility is you'll find that a very simple model already doesn't great on your task if that's the case and you have two easier task probably need to find a task as more challenging to work on yeah so after that you'll been sort of think about what could be a good kind of new network model that might do well implemented tested and see what kind of errors that may sorry forgot that far you're sorted in the right space for a class project but you know it's sort of hoped that you can do more than that after you've seen the errors from the first version you would think about how to make it better and come up with a better project and so I would encourage everyone you know you really do want to look at the day that why you don't just want to be sorted having things and files and run and say ok 0.71 let me make some random change 0.7 that's not a good one repeat over you actually want to be sort of looking at your day the stage in any way you can it's good to visualise the data set 1 stand what's important in that they might be able to take advantage of you want to be able to look at what kind of errors and being there for that might give you ideas of how we could put more stuff into the model that would do better and you might want to do some graphing of the effect of hyperparameters so you can kind of understand that better and so the hope is that you can try out some other kinds of models and make things better and sort of one of them cold here is it's good if you sort of God and well fed up experimental setup so you can easily turnaround experiments because then you just more likely to have to try several things in the time of a label couple of other things I wanted to mention one different amounts of data so it's really really important for all the stuff that we do that we have different later so we have trained a there we have devtest day there we have to stay there at least and sometimes it's useful to have Eva data available from any of the public day the settee they're already listened to different subsets like this but there's some of that there are some that might only have a training said and the Test said and what you don't want to do is think all is only a training said in 8sz therefore I'll just run every time on the tests it but there's some really invalid way to go about your research so if there what you need to do some more tuning and he needs to miss separate tuning day that you sort of 2:30 for yourself by living off some of the training days are and not using it for the basic training and using at for tuning and Dave dayla to go on about them more the basic issue is this issue of fitting an overfitting to particular dataset trainer model on some training day that we trainers and the goes down and overtime we gradually overfit to the training day that because we sort of pick up and down your network about the particular training day that items and we just sort of start to learn days lower fit to the training day that was seen as evil in modern your network think we don't think it is evil that we overfit to the training day that because all you all next there any good overfit to the training day that and we would be very sad if they didn't I'll come back to that in the moment but nevertheless their overfeeding like crazy so what we want to build is something that generalise as well so we have to have some separate day that sell validation data and say look at what performance looks like on the validation day then commonly we find the training up until some point improved out performance on separate validation day that and then overfit to the training day of runaway that are validation set performance get worse imenso then training on the training day that isn't useful because we're starting to build a model that generalisers worse when run on other day that whole point here is we can only do this experiment if our validation day that is separate from our training day that if it's the same day that what it's overlapping day that we can't draw this graph and so therefore we can't do valid experiment you might think maybe I can do this and just use the test set of data but that's also invalid invalid is as you do experiments you also start slowly overfitting to your development day that so the standard practice is you do a run and you get a score on the development day that you do a second run you do worse on the development data and so you throw that S model away do a third experiment you do that on the development day that and so you keep that model and you repeat over 50 times and will some of those subsequent models you keep a genuinely better because you feel work out something good to do this some of those subsequent models only sort of just happened you just got lucky and they hadn't score better on the development day that and so if you cannot keep repeating that process 60-100 times you also gradually overfitting on your development day that and you get unrealistically good Dev scores and so that means two things you if you want a rigorous into US huge amount of hyperparameter exploration I can be good as development say that you have one that you have over fit as much and if you want to have valid scores on the on what is my actual performance an independent day that it's vital of a separate test data that you're not using a tool in this process and I said the ideal stays is that for your real test data and that you never use that at all until you finished training your data at training your model and then you run your final model once on a test stay there and you ride out your pay for a nosey oras honest and say the world usually isn't quite that perfect because after you've done that you then go to sleep and wake up thinking I've got a fantastic idea how to make my model better and you run off and implement there and it works great on the death data and then for you landed on the testator again and the Numbers go up and sort of everybody does that and you know modicum it's ok you know if that means you're casually run on the test data is not so bad need to be aware of the slippery slope because if you then start falling into I've got a new mole let me try that one on the day that then you're just sort of overfitting to the test stay there and getting an unrealistically high score and that's precisely why a lot of the competitions like cable competitions have a secret test day they said that you can't run on so that they can do a genuine independent stay that I'm couple more minutes so yeah getting your new network to train my two messages are here first of all you should start with a positive attitude neural network want to learn if they're not learning you're doing something to stop them from learning and so you should just stop that and they will learn because they want to learn I just like little children but you know the follow-up to that is the grim reality and her just tons of things you can do that will cause you on your network not learn very well or at all and this is the frustrating part of this whole field because you know it's not like a compile error they can just be hard to find and fix them and you know there's just really standard that you spend more time dealing with trying to find and fix why doesn't work well and get it to work well then you then the time you spent writing the code for your model so remember to budget for that when you're doing your final project it just won't work if you finish the code a day or two before the deadline work out what those things are hard but you know experience experimental Carol's of Fernhill but it just lots of things that are important so you know you're learning rights important if you're learning like the way too high things won't learn if you're learning later way too low they will learn very slowly and badly initialisation makes a difference having good initialisation often determines how well new networks learn and I have a separate slide hear that I probably haven't got time to go through or was on son of for sequence models some of the tips of what people normally think a good ways to get those models and working but I just say this one last thing I think the strategy that you really want to take is the work incrementally and build up slowly it just doesn't work to think I've got the mother of all models and build this enormously complex thing and then run it on the day that and it crashes and Burns you have no idea what to do at that point that the only good way is the sort of build-up slowly so start with a very simple model get it to work and your bills and whistles extra layers and so on get them to work or abandon them and so trying to see from one working model to another as much as possible one of another way that you can start small and build up is with data the easiest way to see bugs and problems in your model is with the minute as possible and out of data so start with the data said of even best of those 8 items at once that are official day that you designed yourself because then you can often more easily see problems what's going wrong turn on there because it's only 8 items training when he takes seconds and that's really really useful for me at the afraid quickly and you know if you can't have your moral get 100% accuracy on training and testing on those 8 well you know even model is woefully underpowered or the model is broken and you've got clear things to-do right there can you go to dig and model and you know the standard practice with modern new networks is you want to train your models you want models that can overfit massively on the training set and general your model should still be getting close to 100% accuracy on the training set off you've tried it for a long time because powerful neural network models and Justin really good at overfitting to and memorizing days are if that's not the case well you know maybe you want to be a model maybe you want to dimensions for an extra layer on your network or something like that you shouldn't be scared of overfitting on the training day that you can do that you then do one and model that also generalize as well and so normally the way that your addressing there is then by regularizing their moral and there different ways to regularise your model but we talked about in the assignment doing Dropout I mean using generous Dropout is one very common and effective strategy for regularising your models and so then you're what you want to be doing as regularising your model and no longer looks like this but instead that your validation performance kind of levels out but doesn't start ramping back up again and that's been a sort of a sign of a well regular Rise model I will stop there and then we'll come back to the question answering project on Thursday 