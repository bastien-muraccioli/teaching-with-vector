the day is what I'm going to talk about is the topic of convolutional neural networks play Sexy quite a lot of content in this lecture different things is good to know that since essentially this is going to be learn about convolutional neural networks in one large B for NLP so big on announcement turn the light year convolutional neural network quite a bit of bed I'm want to go through and sort of some detail to particular papers that made use of convolutional neural networks for text classification sentence classification task is there a pretty simple done in 2014 and then the second one is a way more complex CNN there was done much more recently in 2017 first two couple of announcement play the last reminder on the mid quarter feedback survey so terms of you have done this already thank you thank you very much but if you still been putting it off till the very last minute and tonight at midnight is your last chance turn the mid quarter survey to get your hand to give us feedback and to get your half a point ok and then the other thing that you should be thinking about and I know what's of you thinking about since I spent 3 hours talking to people yesterday is about final Project so make sure you got some plans from there in place for 4 p.m. and 4:30 p.m. Thursday I mean in particular sweepers get to do this year's to a fountain if I have read up and have a summary and thoughts as to help inform your work and then just make sure you have in your calendars and the final project poster session for cs224n which is going to be in the evening of Wednesday mass and We're holding up the alumni Centre one more sorted announcement or just general I'm sorry now official in a second half of the class congratulations and you know a few things that the teacher that sort of basics and actually convolutional neural networks as one of them but I mean never less in the second half of the class things start to change in way hoping to much more and prepare you for being real deep learning NLP researchers or practitioners set me concretely well the lectures details of how to build a very basic thing and a more giving you some ideas to sort of some of the work that's been done in different areas and so do extent of this something of interest or relevant to a project or things like that the hope is that well you can take some initiative to find out more about some of the things they've been talked about also would really welcome any questions about things that people would want to know more about and the other thing that you should know about deep learning is the once we get past the fundamentals a lot of the stuff we teach it really buying so things that people are sure of that you know most of what I'm teaching in the second half of the club that's good practice in 2019 but you know the fact of the matter is what people think is good practice in deep learning has been changing really rapidly so if you go back even 2-years or definitely if you go back 4 years can I put just a lot of different things that people used to believe and now people have some different ideas as to what works clear the comb 2021 or 2023 will be some different ideas again as to what people think is best so he's sort of just have to accept that this recent rapidly emerging field and it's good to understand the fundamentals and how things fit together but after that quite a bit of the knowledges this is what people think is good at the moment and it keeps evolving over time and if you want to stay in the field of doing things with deep learning you kind of soul have to keep up without changes it's called lifelong learning least it's a very trendy concert as well as the lectures this is also true for the assignment you know we've been trying to make this assignment so that they stole off very introductory and gradually started to use less scaffolding and we're going to hope to continue that and with the sort of left hand holding in 5 and I guess what we're hoping to do is prepare you both for the final project and for real life making an analogy this morning pairing this to the solar intro sequence when there's cs106 A&B that have tons of scaffolding and then is cs107 you're meant to learn how to diagnose and solve problems for yourself and the debugger there's kind of the same and for neural networks that you over the earliest signs you know we can meet you at every bit of hand holding up here all of these tests to make sure every little bit of it is ok and he is exactly have strapped to things your world your only going to be out of building using your network out why they're not working and what you have to change to make them work and you know the truth is if I talk to a bit about last week you know that's often will more than half of the job that it seems easy enough to stick down his mine unit in the pieces that makes sense to me and then you can spend the remaining 80% of the time scratching your head wondering why it doesn't actually work well and how you could change it to make it to work well the debugging yonex can often be hard but you know the goal is Lexi learn something about doing a and it's kind of animal learning goals of the cost when it comes down to it final advertisement if you feel like it like to read a book I'm just out this week there's a new book on natural language processing with pytorch by Dilip Rao and Brian McMahon Dilip actually lives in San Francisco my copy of this of course but if you don't want to buy it and you feel like having a bit of a look through it I'm a standard library is actually Hessle licence to the O'Reilly safari Books collection and so you can start off at library don't stand for Dudley to you and read it for free free only has 16 simultaneous licences to Safari book also like your classmates to be able to read it for free it really helps if you remember to log out of safari Books Online and when you're done looking at it this is sort of it I mean in some sense I hope you will feel if you look at this boy no most of that stuff already it's not a super advanced bug but it's a good well written tutorial of how to do things with pytorch ent in ok do you like you know most of the stuff in this open I will be a little sad ok so stunning into today so we spend a lot of time on recurrent neural networks and their great for many things some things that then not so good with kind of might like to know about a phrase like my birth or a bigger phrase like of my birth have no independent representation of those spans in a recurrent neural network we kind of get sorted prefixes of a whole sentence bi-directional recurrent neural network you could say what wait a minute you could use it in both directions and to some extent that's true we could get stuff from this direction and stuff from this direction but we still kind of have sort of whole sequences that go to one end of the sentence for another we don't just have pieces of sentences and often with like to sort of work out meanings of pieces of sentences and so we sort of have two problems here we only have sort of initial and final subsequences and also if you look at these representations bike if you say take this last status the representation of the meaning of this setup isn't very dominated by the meaning of the most recent words and what they are trying to predict us to what comes after them and that's part of the reason why I mentioned last time in the question on string like to the idea that while you can do better by having a sentinel and training something that has attention over the whole I'm lstm structure today we gonna look at a different alternative which is convolutional neural deviated as either CNS or continent and the idea of these is we could just subsequence lengthen calculate a representation for a so if we have some peace of texts like tentative deal reached to keep government open and we could sort of just say well let's just take all three words sequences 10th of January two weeks to keep eccetera and we going to calculate some kind of representation for each of those sequences isn't a strongly linguistic idea right we're not worrying about it whether it's a phrase that's grammatical linguistic leave our portable we just every subsequence of a certainly calculating representations of those we going to look at how the groups on let's get into more detail it's the what CNN ZAR and how they there's this general idea of a convolution which you may or may not have seen and son electrical engineering the particular version of convolutions the discrete convolutions which you can mean that you can use the friendly summation symbol rather than integral set a discrete convolution I find out that notation is completely unhelpful so when I'm trying to explain it but I got lots of examples and convolutions of really easy for neural nets and terms of what they do for example classic case of where convolutional neural networks used is invision application so and if you do cs231n next quarter essentially you know the first 4 weeks is just all doing convolutional neural network glory and the son of the central idea transportation is that you want to recognise things no matter where they appear in an image so you have a sort of property of translation in Barry convolution is a way of finding something and different places an image regardless of where it appears this is the division example which I stole from Android into fld l website and soap what a convolution is search but you can think of it as just as a vector and the power switch these little numbers and red where do is slide that play Justice animation I'm so each position you gonna multiply each of the red numbers by the Black number that position and then you're just going to some them up and discrete convolution that switches what that notation the top saying that you're multiplying things together and then you're something them rings ring in the pink with the products some products so it's sort of patch.com into the pink Matrix and that's then your convolved feature deconvolution which for the rest of the day when not going to look at any more so this is all your learning about Vision now going to go back and look at 1D convolutions which is what people use when they're using convolutional neural networks for texts turning point of a convolutional neural network for 10 import so here's my sentence word in the sentence IKEA got a dense word Victor my mate had a 4D want to keep it small and my example that usually do not more the starting points we have some input you know input could just be one hot encoding that's not forbidden here but normally will have these kind of dents word doctors it's sort of the same as the 3D as the 2D one apart from with only got one dimension so we have a filter hero self filter ncol filter is going to do 3 Steps and time 3 words work across the dimensions different dimensions in the convolutional neural network I can get referred to a channel working across the input channels and so we have a patch like this put it on top of the first the animation of the previous slide sorry and we gonna work out the door between those no I did that at home by putting this into Excel and the answer that is that the product is - 1.0 and then at that point we slide out this tricks with get referred to as a kernel or a filter which is the patch that we using for our convolutional neural network we slide it down one and do the dot product of those terms again and that comes out at -1/2 and we keep on sliding that down and we get put shown on the right as an output so this point we've just reduced the sentence a single vector and that seems like we might want to do more than that but the other thing that you would notice sentences So Strong for you know we had a seven word sentence with this three word and col down here I ended up with only 5 positions to put it in so it's become a 5 word thing first of all addressed that problem do convolutional neural networks they add head what I can do is I can add 0 padding at both ends and then sort of do the same trick and save run a convolution on that and now I'll be able to put my filter in 7 different places as life slide at down and so I'm getting out of Victor that's the same length of my import there are different way so this is the most common way of doing things that it's kind of seems logical because it maintains size I mean you know more than one way to do it if you really wanted to you mistake on my spy because this turns out for child about to get you in a minute just explain you know if you wanted to you could have two steps of heading on both into here first convolution of be looking at 00 tinted of and then the convolution would actually grow the size of your input yeah I mean so when we done so far we started with these words vectors which in convolutional neural network word link for so I kind of input head for channels back we would just producing from there turn all without pod so output has only a single channel so it's sort of shrunk things in the columns direction from 4 to 1 might seem bad and for many purposes it is bad and so a lot of the time what you want to do well rather than have only one filter instead of that why don't I have several filters 3 different filters and each of these filters just sort of a same size buy the kernel size x number of channels for play different filters and I'm going to run each someone down the tank I'm here so now I'm in the up with 3 columns of output and so I have 8/3 channel output I'm sure lovely think of this is filters when your networks we gonna learn them by back-propagation like everything else these filters good some house specialise in different things this filter could specialise on is this language polite and it'll produce the high-value whenever it's his polite word maybe this eating and will have a high value whenever it's his words about food and you know the spiritual do a 13 and so we can that's the sense in which people talk about the different features because your hope is that your kind of game different late and features coming out of the text a representation and that sort of a useful sort of heavy turn features in Artex it's often though what we want to do is close the text today's feature so you might just have the question of well in this piece of turf Hawaii walkabout food operation that will quite often do is one of summarise the output of convolutional network and the simplest way to do that is convolutions is called Max pooling over time Paul overtime the each of the channels of otherwise known as features we just simply going to look down and see what is its maximum value 0.31 for and so you know if I use my story about the first two filters that sort of saying well it's not very polite text but it's really about food write the result of summarising and what we've detected there so the concept of Max pooling in some sense capture sting being activated anywhere out Sophie have things like politeness and about food that the output a mix pull in or have a high value if somewhere in the sentence there was a clear marker of lightness something really about food and that's often a useful notion because often what you want to know is you know is there some food in this sentence or is there not there's another other things that you could do instead of Max pulling you could instead do average pulling so he just take these numbers and find the average of them nothing has the different semantics which is sort of what's the average amount of politeness of the what's your hand what percent of the sentences about food or something like that purposes this is better because you're not texting all of the important will do an average time people have found the next C-Max pulling is better because you know a lot of signal to natural language no no matter how polite you're trying to be you're not going to be being polite and every word you're going to say now and then it's like that and 10 prepositions it's not currently polite ride and so there is some politeness showing up prominently than the sentence becomes polite and next bull is actually better for capturing them close the one of the kind of thing that you can do with mint pulling and find a list of thing and don't forget use much but you could do that as well so if you're in pytorch this is all pretty easy stuff to do so there's a handy Dandy can't 1D there's also come to see as you might guess provision but there's akon's 1D and where your specifying how many input channels there are there was a word embedding size how many output channels there are we have three what the size of the convolutional kernel is so the ones that we were showing also three and their various other parameters you can have I can say that you want a pending A1 and things like that and then once you've got one of those you can just sort of run your convolutional filter on the important to get a new hidden stay and then if you want a Max Paul you can just I put up there and then you've got a Max pulled out pot so that gives us the basic Wishaw new network IM4 that sort of makes sense up until there next business to sort of show you what are the things that you can do typing the slides other list useful notions b what is that really come up match and then I'll pay but you know actually turned out when I go onto the second paper when I'm complex convolutional neural network actually netpave they try at just about all of these things and I say no one uses so it's also good to know what they are for looking at various paper can we do things so far then we were calculating these convolution trying them out at every position so we had 140 tentative deal then pretended of deal reach then deal reached to and so we're just walking down one step at a time which is referred to as a Striders of 1 Baja the most common thing to do but you couldn't serve will wait a minute to the first convolution 0 tenths of deal play three words in there even if I skip down to and next day Deal Leeds to and then I did to keep government still have in one or other of the convolutions every word of the sentence plus much computation and I've still got everything in there and something latest news in a stride of 2 and so then I get something with half as many rows out so it's 1 ways and sort of compactor fire your representation and produce something shorter from a longer sentence and we'll see that used to the coming up later other ways to compact the 5 watt representation that comes out of your Centre different notion of pull wing which is local Poole play the fishing world when people talk about maxpool and Vision they normally mean local polling as opposed to that next pulling through time that I showed you first ways of back to where we started and we've done how size 3 strike one convolution and which is producing output as before but now I'm going to do is local Poole pride of two which means I'm going to take Rose and I'm going to pull them together into one row and I could do that again by the Maxine or averaging or whatever appeals to me so I take the first two rows IMAX Paul will I get this I take the next to roll Rosie next pool when I get there next to and I've sorted on the bottom so I have two rows at the bottom and so that's then give me a local next Paul weighing of a stride of 2 and hadn't sort of had exactly the same effect in the sense but with a different resolved as using a stride up to in my convolution because I begin reduced said to something of 40 Waitrose so that's what else can you do there more things you can do to make it complex things that people have sometimes done hallway this is a more complex thing and it's sort of saying well rather than just keeping them Mac being kind of activated two or three times in a sentence George record all the times that is activated in the centre play the race remax Paul way and I'm doing to Max here you look down this column and you find the two highest values for that column put the two highest values not in order of highest to lowest which they are in these colours 0.2 0.3 4 this one and it's 1.6 0.64 this one because it reflects the orders of the columns up above concert this way sync data which is a dilated convolution the dilated convolution of convolution doing it over here doesn't really make sense use a dilated convolution is if I take the we can kind of deep convolutional networks that have multiple convolutional layers clear up a dilated Common solution is sugar in a skip some of the Rose so if you use the dilation of 2 starting at the top you're going to take the first third and the fifth row play them by my fitness I had different builders x my filters set the values that appear here why does one need a news one instead of do the next spread Outro else you to have turns that see a bigger spread of the sentence having many parameters to do things this way you could have said I'll look I could just instead have times for the kernel size of 5 and then they say 555 words in a row but then I'd be having sort of bigger is the specify on my feature where is this way I can keep the matrices small but still see a bigger Range of The Saint racing the concept of how much of a you see is kind of an important notion in convolutional neural networks because you know if you meaning of a sentence new just running 3 by 3 convolution what sort of seeing these three word patches of the centre natural language that's already actually quite useful representation because thought of having those kind of ingram's as features is just good for many purposes including if you want to sort of understand more of the semantics of a sentence somehow you want to see more of that one Sword of God calls you can use the Seymour event bigger filters you can use a kernel size 579 or something convolution you could do something like dilated convolution so you can see spread out pictures things that you can do if you have depth of a convolutional neural network greater depth of a convolutional network is Seymour so this first later and the Rose now and have sort of info about 3 words in second layer convolutional neural network with the same general nature on top of bed and you should have take the first three rows and convolve it again then and then the next ones that those then know about 5 words of your original input sent and so that we can have a deeper company bigger and bigger pictures of the sentence all good ok so the next piece actually so to the stuff again context of a particular paper so this was I'm a paper by you and Kim student ID still is a Harvard student I'm in 2014 so this was sort of a fairly early paper and he wanted to show that you could use convolutional neural network job for doing text classification when what you want to classify is a single sentence so that kind of thing you might want to do is look at the kind of snippets the movie reviews that you see on the rotten Tomatoes Side and say is this a positive or is this and negative sentence description builders FC to the convolutional neural networks at collarbone Western introduced in their 2011 paper that we mentioned before when were talking out window base classifiers so in their paper they actually use both window base classifiers and the convolutional classifier so yeah I saw already said this so their tasks are Simpsons classification could be centre man it could be other things like is this sentence objective subjective is what the main news articles meant to be in subjective as what the opinion pieces they need to be things like question classification is this a question asking about a person location number or whatever what he did use the notation of his paper which is sore different the way the method gets written down what I just showed you that is really doing exactly the same thing word vectors of link k and the sentence buy just catalysing always word vectors together when we go we have a range of words it's a SUBCARD about centre inspector on the list of filter is just being represented as a vector and everything I haven't done one long Victor to the entire sentence where is I'd suspect into a metre size 3 convolution is just a real vector of length HK the size of the convolution filter x the dimensionality of the words it's going to do to build his text classifier is you different sizes so you can have a size 2 convolution convolution the shown here and bigger convolutions 1 channel for CNN within doing a dot product between the weight vector of the feature x the subsequence of The Saint also putting a bias which I thought of them and then and nonlinearity wasn't doing either and but it's sort of we've seen a ton of what we wanting to do is that South and feature to do it through all 3 we're going to go all the way through the sentence he did they were slightly Funnel funny is his windows with sudden will not sided in the notation right as a word and 1 words to the right of Earth so he has heading here just on the writing where is most people do their convolution symmetrically in both directions around things and so we going to do that for a bun channel CI and therefore computer convolve representations just as we've talked about what's that's what we talked about there's Max overtime pulling in the pool in way of the caps and most relevant things and it's giving us a single number for each channel start looking different that have different kernel sizes one other idea he used which is possibly in the idea the things that you put even think about in various ways say a question answering system is soap use pre-trained word vectors was he actually kinda word vectors sofa each word had two copies of the word vector set a two channel set sete Heathrow and he fine tune the seat train how to get the best of both worlds of sort of fine tuning and not fine tuning and all that went into the max poorly operation so Poole we get out one number for each channel so I'm he has convolution 345 143 size so we're getting out of Vector of size 100 at that point and at that point you're taking that final vector and just stick it through a software living your classification into classes so all of that can be summarised in it's big enough to sort of I like this movie very much embedding dimension is 5 doing it in this example we having two channels consider kernels of size 2 3 and 4 and then we getting two different ones 6 of our filters let me apply those play those filters without any padding within getting out these outputs of the filters places for 5 and 6 then once we've got these please set the numbers we doing 1 Max pooling Max of each of these features which gives these 6 numbers concatenate them all together into one vector which we feed into a soft necks over 2 classes as the weather sentiment is positive or negative so that's basically the model so something so this is sort of really actually a very simple very computationally efficient and models to had a build a text classifier a couple more I'm so in one of the assignments we talked about Dropout and you use that so hopefully all masses have dropped out of this fine so he was using Dropout and the spring 2014 and the Dropout paper only coming out in 2014 I just had a couple of years earlier this was sorted still fairly early and to be taking advantage of drop out so that part training you drop out Victor please sample you're the normal random variable about some of the features ring thing time you don't do the drop out but because so drop me out of one of staff your scaling your white Knight probability that you use for dropping out so that to get sorted the same scale as before as we discussed in this Simon Dropout as a really effective form of regularization widely used in your network only do that he actually do that kind of another sort of funky formal regularization set for the soft Max white vector he can strange norms play norms of the weight vectors in the softener fix number result of the hyperparameters actually set to the value 3 and if your weights were getting too large there will be old so they didn't blow up a very common thing to do I'm not sure it's very necessary but I guess it gives you some few other details of this one of my hopes is so cheers about how there are lots of things you can play around with and Mac with if you want to try different things Project so here are some pics final hyperparameters so is using revolut nonlinearities 34 and 5 of the convolution channel feet size a half as usual and get several percentage improvements and drop out because quite common actually L2 constraint S = 3 5300 dimensional word vectors Miles Davis performance and here is the big table and I was too lazy and to redo please different text classification dataset what's a different one to these two about Stanford sentiment treebank subjective objective language question classification of is asking for person name and location a company or whatever the classification thing there's another sentiment play the Search lots of models some of the models down ohia a traditional feature base pacifiers in particular and banging me back in 2012 and sort of pointed out that by taking certain step features normalisation that you could actually get quite good results snow feature Pacifier so many people use their as a baseline you better things here were 3 structured new networks that my group was very fond of in the early 2010s 3 top a his CNN model Sullivan Mick play the CNN model Wayne's like in this cold in this colon sometimes it doesn't rain like in these columns but in general what you can see from this is that you know this is an extremely simple and compilation your network model and an actually does and kind of well on this system weibo with this table and again and terms of site writing your proposed project proposal he should do his kind of think about what you're reading because you know what a papers are perfect people with what they claim and sometimes if you think about what they're claiming and whether it's reasonable why it's not all there are ideas of how you could do things differently or show something different play good cribble with results table is well I said as I had a couple of slides back and that the statement that Dropout gives you to 4% accuracy improvement in his new Alness is systems because they're older and we're done that was invented didn't make use of Dropout but you know any of these sort of manual net systems up here and presumably would have given them a couple of percent gain as well so arguably this is fun of a biased unfair comparison and the right thing would have been to be comparing all the systems using Dropout despite that you know this was still up people noticed this paper cos it's show that using this sort of a very simple very fast convolutional architecture could give you strong results for text classification summary sing that you should be thinking about the project and otherwise we effectively building up a bigger 2 kip of different tools you could be using future work whatever it is running off with we had word vectors and then we can build bag of Victor models by just taking the word vectors and everything that surprisingly good baseline to start with we suggest you and many cases for things like projects you should use that see how well it does make sure you're working better I'm in particular you can do even better with that if you still have had some extra will you Liars on top which is an idea that's me exploding deep everything network window models which were very simple you just taking these sort of five word windows and computing a feedforward network on them and they work very well for word classification problems that only need local contour speech tagging my 10yr but then we've gone ahead and looked at some other models I'm very good for text classification and a very good because they parallel lies really well on GPUs which is something I come back to again later so this what sort of representing sentence meaning next year efficient versatile good method which has been used quite a beard and then they start contrast with recurrent neural network worth have some advantages there some more cognitively pawsable but just had a reading through the texting I'm getting it meaning network for good for things like sequence taking and classification building language models predict what's coming next can do really well when combined with the tension but they also have some disadvantages their way slower than convolutional neural network do is get out some kind of overall meaning representation of the sentence you know what does this mean are these two and phrases paraphrases with each other there are now many results that show that people don't get better results with recurrent neural network play results using text on your network step then head towards outcome accomplish Play Shellac a picture example so before getting to that I just want to introduce a few concept that we haven't seen all of turn up when we do this what of time in the Secret models pad talking about gated recurrent unit emu all that we can sort hear that we can calculate something put it through a signal nonlinearity and gets a value between 0 and 1 and 4 vector of values between 0 and 1 and then do a hadamard product with the Victor and sort of gated between its value and 0 so that suggest the idea that you can also apply Gate play when you're building multilayer network the success of lstm have been proven that was that really took off was people start exploring how can we have videos of skip connections and getting in a in a vertical Direction conversions are big this one is a very simple one but a very successful one that's basically just about to skip connection is referred to as a residual stitches used in residual networks otherwise known as in business a valued to the you can stick it through Akon block and the typical content block is you go through a convolutional layer you then go through a rolling on linearity another convolutional layer and then when you come out these two values so there's the same idea that sort of summing values as medical in the same way as no stm and then you put the app for love that to another relo and this thing here is called a residual block and then commonly or stick residual there's one little trickier which is you need to use padding right because at the end of the day since you want to some of these two pathways you want them to be the same size health and drinking in the car never someone so you want this sort of have a pending at each stage so they stay the same size here together different more lstm B block play you're going to Smyths Hoover and students who's the same guy who is behind lstm and you can see the a highway Bloch Wyatt sort of similar you've got your kind of thinking of moving and identity x the skips and nonlinear blah you can have a go through exactly the same staff Condell O'Connor is that unlike this one this time there's a gate so there's he gave in the seagate and so you're multiplying both of through here and the path through here by a gate just kind of like the sort of forget input gates are we so before and then something them together more powerful clear that it is more powerful I mean very simple semantics cos you can think of the semantics of this one is is just you walk this way and you just sort of Carry forward your value and do nothing office blocks job to the is to do is to learn a delta that is meant to learn what kind of deviation you have from doing nothing what's a nice simple semantics well and your networks to learn thing this song a more complicated apparent semantics because you're taking you know the identity multiplying by the sword Prada Honfleur navigate Tiana hadamard product more powerful this a lot more control and take pieces of the different ones and so on if you think about it for a bit longer I mean mathematically is actually not any more powerful that you can represent anything you can do with this one with that one think about daddy as well you're kind of keeping only part of the identity who is key the identity and see it is your job it's the this one isn't keeping overhearing the conflict erratically can so you can sort of anything you can complete with this as a function you can actually computer with a resnet block and and so then as quite often in your network LAN can sort of some kind of proof of computer can be computed or not it's sort of comes down to learning and regularization question one over the other these actually prove better as something to use in a learning architecture normalisation people a building deep convolutional neural network a2015 platters play almost always use batch normalisation layers because this make sure life a lot better and if they're not using batch normalisation layers they normally using one of the other variant ideas that people have suggested such as layer normalisation which is sort of meant to do about the same thing batch normalisation does I mean I think many of you will have seen somewhere in steps or otherwise the idea of doing a z transform which means you take your day that you work out it's main and you work out standard deviation and then you rescale by subtraction and multiplication so that you have a set of data which has and mean of 0 and a standard deviation of 1 is that right so best normalisation is effectively doing exactly that but in a weird way doing is that your taking each mini batch so whatever just random 32 examples you stuck in a mini bar through a layer of your new network like a conflict that we saw before and you take the output of that minibar free transform on at and then it goes forward into the next can't walk or whatever and the next time you have a different many battery just see transformer bit weird just do it on the output of these mini batches proven to be a very effective thing to do get it sorted means that what comes out of a command block so it always has the same kind of scales that doesn't sort of fluctuate a lot of mess things up and it tends to make the models just much more reliably trainable because you just have to be much less fussy about a lot of things because you know a lot of the things we've talked about about initialising your parameters and setting your learning rates sort of about well you have to keep the scale of things about right so they don't get too big or too small where is it you're doing the steps normalisation you're such a forcing scale and two being the same size and so therefore you kind of don't have to do the other stuff as well and it's still tends to work pretty well so that's a good technique to know about one last thing to learn about there's a concept of convolution I guess I really sort of renamed this I name this wrong because I wrote them one-by-one convolutions because that's the term in normally say that's the Vision world may had to deconvolution call this one convolution can have convolutions with the kernel size of 1 I see that it seems like no sense whatsoever because the whole idea of taking this something for med if I the other words surely and calculating nothing actually happens in a size 1 convolution is number of channels previous layer if you'd calculated whenever it was camels or something like that but the one by one convolution is doing is acting as a TV with embedded fully connected network over those channels doing a position specific fully connected network play your days are do that I'm for various reasons you can do it because you want to met down from having a lot of channels to having fewer channels or you can do it just because you think another non-linearity or help and this is a really cheap way to do it because the crucial thing to note put fully connected layers over everything they involve a lot of premature convolutions involved very few parameters because she just doing at them at the level of the single word my complex model this is just a sort of almost Tobias aside it is just shows something different that you could do when it's something that you could play with talk about machine translation we talked about the c2c architecture that was introduced in 2014 and has been very successful for machine translation actually the year before that came out there was a paper doing your machine translation by now kalkbrenner and want some in the UK and this is sort of was actually Ibiza translation paper of the modern era if you think that far enough the exit couple of people that tried to use neural networks for machine translation in the 80s and 90s but this was one of the first one that we started that and they didn't actually use a c2c architecture so what they use was for the encoder they used to convolutional neural networks and so that they had a stack of convolutional neural network voice trump down the import and then finally pulled up to get a sentence representation and then they use the sequencer model as they decoder that sort of something that you could try and some other applications that for in code as it's really easy to use convolutional neural network icon using on your networks as decoder as well so that's all will be harder to get your brain around and it's a news the latest match something I want to mention cos will turn to it in just a minute is so so far we've done convolutional model word so that effectively picking up these word in g units of two word of three word subsequences the 10 developed fully soon was well maybe in and also be useful to use convolutions over character convolutional neural network over the characters of the word I'm generated word in bed this idea has been explored quite a lot and it's part of what you guys are going to do for a sainement 5 is build a character Level comes nerve and for your improve machine Translation system I'm not going to say a huge amount about the foundations of this today and because Thursdays lecture is in talking about subword models and we'll go through all the details of different subword models is a show you're a complex convolutional neural network which is also used for text classification so essentially the same task as you and Kim's model and this model actually is built on characters is not built on words so we are the Foundation of it and having a word like model what's the paper from 2017 same here and people work at Facebook AI research they kind of had an interesting hypothesis for this paper which was essentially to say that you know by 2017 play using deep learning for vision worth building really really deep network finding that they were much much better provision toss so essentially to some extend the breakthrough is that once these ideas are done prove that wasn't just that you could build a 6 layer on a player convolutional neural network provisions building really really deep networks for vision tasks which headpins or even hundreds of layers and that they train on a lot of day that prove to work even better that's what's in your head and you then indeed is happening in the language processing the observation is is there no people kind of pathetic they climb a doing deep learning but they're still working with three layer lstm only we can make some progress building really deep networks 4 natural language processing goals that is precisely what they said about to do it Simon build really deep network with sort of looks like a vision star convolutional neural network that is built over characters trovit here and it's officially deep that a spider making reasonable so little bit of a challenge but we can try and look at this so at the bottom we have the Tech a sequence of characters so when people do object recognition on pictures normally all the pictures and made the same size right you make every Picture 300 pixels by 300 pixels or something like that so they do exactly the same put in IP and they have a size what's the document which is 1024 character that they truncated spark if it's shorter than that they pedder until it's a size 1024 and then they're going to stick to it into their staff first part is that the eat character they going to learn a character in bedding now and the character and buildings of dimensional the piece of paper 16 by 1024 through they're where you've got kernel size of 3 and 64 output channels something that's 64 4 in size convolutional block Alex playing the details that convolutional block on the next side but you should be thinking of that resnet picture I showed earlier where you can either be going through some convolutions or taking this optional shortcut DJ block where you can be going through convolutions an optional short car then doing local polling in the same way people typically do in Vision what people do in fishing systems is yourself shrinking the size of the images things that have the dimensions needs Direction but at the same time you do that and your new network you expand the number of channels and say it make it deeper in terms of the number of channels at the same time as you make a smaller in the XY size of the actually the same apart from these a one-dimensional convolution head 64 channel 1024 character call work so we're going to have 512 switch off actors in between our 128 channels can I can repeat that over and over again write so there are two more conversational Bloxwich I'll explain more they sort of residual block and they do exactly the same thing so now they're 250c temperature like for character block 156 channel quite high enough but they repeat that again and I pull again so now they've got 128 positions which about 8 characters 512 channels representing their they pull again they have convolutional blocks again and then lo and behold cos I said even the weird ideas going to turn up right up there they doing kmaxx pulling and I keeping the 8 strongest values and it needs channel point they've got something of size 512 by 8 the like 8th of the 8th character sequences have been deemed important to the classification and there are per channel there 512 of them there then putting that through play connected wires vision systems at the top have a couple of fully connected layers at the end and the very last one of those is effectively sort of feeding into yourself Meg 48 times the number of classes which might just be positive negative to class I might be topical classes to sit like a vision stack but they're going to use that for language the bit that I haven't quite explained was these convolutional blocks but it's sort of looks like the picture that we had before or an apartment slightly more complicated so you're doing blockhouse play convolution Sum number of channels pinning of where you are in the sequence put it through a batch Norman's we've just talked about putting it through a value non-linearity free things again or remember there was the sort of skip connection that went right around the outside of this Block and says it's a sort of procedural style block what's the kind of complex architecture even put together and chat in your final project if you are entitled so4 experiments o they were interested in and wanted to make a point of it while some of these the traditional text classification datasets have been used another papers like you and Kim's paid that are effectively quite small so something like that rotten Tomatoes data said is actually only 10000 examples 5000 positive 5000 negative and they sort of have the idea that just like imagenet was needed for deep learning models to really show their worth and Vision that probably the show the value of a huge model like that and you need to have Andrea play the set so they get some much bigger and text classification dataset so he is an Amazon review positive negative day the said and which they have sold 3.6 million reviews 650000 play the Search and here the day experiment so the numbers at the top Davis previous result printed in the literature fortnite new things that they want to sort so the ones that have a star next to them turn off the source use ring method I'm using special techniques as well that I cut off Dimension of these numbers that error rate So Low is good are you getting the bed these are all of their results can you get out of these results well the first thing that you can know this is basically with these resolve networks that work in bed at Ryde so the one I showed you the one that I had the picture of isn't the full thing but they have ones line 17 and 29 in terms of the number of convolutional layers the deepest one is always the one that's working best so that's a proof of deep network didn't keep on working I'm something interesting but no hear is I guess they thought of this is cool why don't we try and even deeper one that has 47 layers and see how well that work sort of interesting for that so for the 47 weihua infection worse than this one in One Sense show the result of South well so they didn't experiment of let's try train a 47 layer network without using residual connection it was a lot worse down about 2% and they trained one with residual connection there is projected 1 of a percent were sort of work just about as well but never less that kind of different as situation in vision because residual networks that people are using and Vision this is sort of like the very minimum people use so if you're using residual networks and Vision typically he might use resnet34 if you're really short on memory and want to have a small model that you just know you get better results if you use the resnet-50 and in fact of used and resnet 101 and work even gonna again and so somehow you know whether it's got to do with the different nature of language or the amount of data or something you haven't yet gone to the same dip that you can indeed assault I'm sorry other thing they comparing here is that they comparing three different ways could be using convolution you can be using local Max pooling and you could be using K max pooling general the slightly different numbers if you can see each one wins and one at least one of these days the theatre at least two of these days does Max pooling win 4 for the day the sea look at the numbers always does pretty well because next Pauline goes pretty well here whereas the convolutional stride works badly and over here next pulling works pretty well and the came x pulling works kind of badly so they're recommendation at the end of the day is you should always use pulling of a simple kind that seems to be fine and nothing else she worth the trouble of thinking about doing was there any other conclusions I wanted to tell us most of I guess there overall message is you can build supergun and text classification systems using company takeaway that message just a couple play there's one other thing I've wanted to mention and I think I just thought I'd mention it very and you can look the more detail if you want to so we some have this situation that networks a very standard building block fitting LP big problem that I just don't paralyse well and the way we get fast computation and deep learning is we find things that parallelize well so that we can stick them on GPUs GPUs only a fast if they can be simultaneously doing the same computation many times feel for combination on your network because precisely you're doing the same from the computation every position but that's not what's happening in the recurrent neural network because she have to work out the value proposition one before you can start to calculate the day of position to which is used for the value of position 3 this was a piece of work done by sometime 24mm car instructor Richard social and some of his people at Salesforce research on saying how can we get the Best of Both Worlds how can we get something that's kind of like recurrent neural network but doesn't have the bad computational properties idea that they had was well rather than doing the standard I'm starving where you're calculating you know an update of 10 the date value and you're gay the preceding time slice what instead we could do is we could stick a relation between time -1 and time into the max pooling layer of a convolutional neural network calculating a candidate in a forget guys and an output guide but the in the how are yous inside ring wire viacom national operation there's no free lunch you can't get through recurring the penalty this is giving you son of a pseudo recovery are modelling in association between adjacent elements of each time slice but it's so just worked out locally rather than being carried for and in one layer found is this you made your networks deeper using this idea well then you sort of start to a game expander Windows employee the amount of information being carried forward play Since was that you could build these kind of models and get them to work necessarily better actually on the slide is often better you get them to work kind of as well as an lstm does but you could get them to work much faster because you're avoiding the standard McCarron operation and keeping it as something that you can paralyze I'm in the mix pulling operations what's a kind of an interesting alternative way of searching trying to get some of the benefits I think long term this isn't the idea that's going to end and so next week we're going to talk about transformer networks which actually seems to be the ideal skin the most steam at the stop there for today 