to get started with that let's that we want to accomplish today read that I saw yesterday from research group at umass this is a nice bridge between last week and today so last week we talked a lot about tattooing in very expensive testing regime that the field has moved toward in the era of deep learning and I kind of preach approach to all of this where you balance your own budget in terms of money and time and resources and everything experimental paradigm and I thought we would have to compromise nothing that you want to do in the paper is just be up front about where you had the car open and honest about your experiment gradient one factor that I didn't include in that discussion but I think I will for future years environmental concerns that we might have around very ambitious hyperparameter tuning regime this paper is about a haven't seen the paper itself but the seems to be one of the core tables from the paper is a quite shocking rights column here play some bassline things it gives you like air travel average an American Life ever ready is a kind of shocking statistic Claire car lifetime herein this is kinda striking because training a state-of-the-art in IP model for tagging CO2 with experimentation that number jumps up to almost 1 year for the average American in terms of CO2 this is a nice bridge is going to talk about the transformer today among other things timer for modest but I opening amount of energy consumed for the transformer large but once you start to do hyperparameter optimisation you're talking about more than 10-years American Life CO2 emissions to do this hyperparameter optimisation environment environmental disaster right are you interested in deep learning for NLP but also concerned about the CO2 footprint of training you should be yeah I really think this is this I think they should be part of our thinking about the kinds of experiments that we run in where we invest Regina check out this paper once it's available transformer I guess if I'm successful my lecture today will be dua for the first part of class today until probably about 5:30 is talk about contextual representations and then Cindy is going to take over and do a mini lecture approaches to dealing with very long text learning dense representations of hello I love you sugar representation I do think this is one of the exciting things that happened in recent times and NLP Terminator motivate that I'll do a little bit as like high level insight about why I think this is a wonderful development in addition to all the is there a wide variety of tasks decor idea by talk about three approaches to Learning Centre represent in the final section which we might just leave free on self stir what is you to that notebook that I posted at the start of the course easy and practical ways that you could bring Elmo invert into your prod having to do a complete code overhaul so that you're in bed tensorflow associated notebooks in addition to contextual rapstar and The Notebook Smyths 2 2019 which is a nice overview paper that kind of does more of providing high-level insights about from word vectors to context to overrepresented the guiding linguistic and empirical ideas behind this home call out the cs224n lectures on these topic and they are somewhat different from my lecturer so they they offer much more of the history connections with different tasks where is given my limited time I've decided that I'm going to try to just convert to you we'll get a sense not only for how they work but also kind of why they perfectly time because as you'll see working to build on a lot of stuff that we've already done it's like we're assembling raw materials in a really creative and interesting way to achieve some big games how much transformer in a just mentioned here the court papers project sites which have code and also pre-trained representations available for text I thought I would call out especially this wonderful kind of notebook paper called the annotated transformer by Alexander Ross who's on the back of the server actual transformer paper happy with a few comments from him the paper is pytorch clear pytorch code the technical Concepts so that by the end he has fully implemented accident in a bunch of weight and I just think that I mean what a wonderful kind of with all due respect to the original authors of the paper and it's quite a nice paper what's that original author team is strictly more valuable because save you from this all the time in reading NLP papers that from the language how the pieces fit together with the dimensionality of things are and I can't quite do the Matrix multiplications in my head the map on I try them with pad and paper are there in like every single ambiguity is fully resolved there softer him and I encourage that as a way to study what's happening in the Franklin College yourself because not only is it wonderful contribution but I say there's no better way to deeply learn a paper them to do something like the high-level motivation is I see it kind of with my wingless hat on show you some examples here this is just around the verb break what's the world record talking to the house broke into the movie broadcast Word format sounds like break how many word senses there are cancel I don't know what they were dealing with different lexical items or 2 or 6 or what I kind of have the feeling in looking at paradigms like this notion that we can the world into different wordsensa appreciate is breaking down nature is not quite cut up in that way at Field launch of metaphorical connections and linguistic connections that are being shaped by the morphosyntactic lion environment in which this verb is being used it's Particles and what we know about the world presenters of strictly speaking ambiguous in some sense and we proceed one reading but not others because of what we know about the world that feels like the real picture playing with the Furby here are a few more flat beer flat no flat surface sore throat a party a fight of ball off it tenses of the verb flat or into a or throw into even relate lexical items or 2 or 1 I have no answer to this question but I can tell in the particular morphosyntactic context and bringing kind of world knowledge about what I expect the read yes I think points towards contextual representations as the right way to go because if you think that to the first unit of this course send it words in isolation as vectors do for one smash together all of those sensors into a single vector I have to do the really hard work of kind of breaking up this break into a bunch of different vector neither of those is going to work out even if it did work out we have this kind of average your static representation what we know from these example other be influenced by the con intuition that the face broke means what it means or Don broke means what it means because of other word suddenly different so I caught a crane called a fish that's a bird set up the steel beam you probably into it at a machine and for eyesore crane you might just be uncertain about what kind of created is Bergamo 4383 be there reading swear a machine catches a fish or a bird but they're just highly unlikely given what we know about Road Norwich turn lose sight of the fact that you were steered one reading another because of the synthetic environment the semantic and already transfer cream and then I think we could extend this two examples that I showed you when we talk about discourse and dialogue so are their typos I didn't see anything about the sense of any there at the end play the preceding disc downtown I didn't see any entire sentence in especially any I think content is being shaped by the preceding the idea that static word vectors are just not the way to go is reflecting on this like if you pluck theoretical English toffee convince them that words represented as vectors of real numbers tough guy a little while you could convince them that this is a smart way to go and in fact it's quite close to what they think about when they think about word very high dimensional objects with lots of release if you think about word now what things is better very far from what language aren't you convinced the linguist of is just 4 words in isolation and that's where play linguist think that word senses are conveyor cartoon about usage they would say well that's going to be shaped by the whole discourse context and everything the radio that words could just be single vet do I think Elmo Bert get them on board with that Jericho motivation and I can't left out of the slideshow cos you can find the stables in the paper breakthrough on lots of tests you might have experienced this yourself Bake-Off winners tries to burn Elmo and things like that so us know about that picture so I thought I would feel expected that I thought we might think about here just by way of high-level motivation notion of model structure linguistic structure is that easier in kodi up here for example the Rock Rose the vectors for those words that can be like go out so isolated vector number average time to get a representation of the whole Centre very high bias in the sense that it's making a lot of presents a word meaning for interacts to form a meaningful anything that deviates for song on the road you can earn a very free-form function for combining these three meetings into a hole meaning for the centre is linear structure the linear structure order by just the song function is on the left we can learn lots of other functions in one thing that were doing in terms of bias bias for a kind of linear flow through the if you think about sentence processing but it is a substantive thing to say about a rip turn the left here you do that in a different way so here the bias is that there rock roasters possession order procedure from bottom to what the linguists call the bias right if we're wrong about the structure we might be in real trouble in terms of what the model is good carpet restructuring this can be a huge gain in terms of telling them can a very high bias and then if we take this Aaron and here and run it by direct turn a bunch of these attention Mac what about today pretty far along the spectrum to being unbiased about what kind of interact presumption of left right directions not only that but we can hold his attention mechanisms that are kind of allow us to leave around in the sentence if that's what the data tell us very free form and I would say that all the models that we talked about today pushing in the direction of assuming that we don't know anything about the state is going to be like going to be kind of lots of dependencies that we need to learn learn them in a very free-form way give the model as much capacity to learn from data and not bias in anyone of the models we look at today in bird is the furthest along a few more things by way of setting the stage because I did want to impress upon you already have all the right ingredients for all these models are incredibly complicated stuff happening in there these are things that are familiar the team will be attention you'll see this in the transformer and improved remind you that we talked about attention here is an nli example every dog danced some poodle dance word representations down here and then is hidden state sign in for the premise and anion and for that I passed two watches HD best form of global attention that we consider calculator dot product score play the Spectre by all the vectors in the Prem normalised VR softmax function scores then down here are used to wait everything in the premise extended approach we take the moon here is a vector that kind of summarise is in the premise as we did by attention binder into the classifier so that the ultimate classification decision here the Sitting representation is cappagh and this Final stayed over do some weights there's a couple ways you can do you get the classifier on top of that global attention with dapra remind which we apply mi state looking back in all the Prem Estate the transformer you going to see this all over exactly the same that we haven't really talked about a lot unit is this notional subword modelling first unit because we built the vector space you saw that if you kind of broke them down into the Character level and then built up representations that way you could make up for some of the deficiencies in the original Space by kind of taking it information that shared across the some parts of it will look at especially elm somewhat more sophisticated way so I thought I would just introduce this impressionist let you go off and studied in more detail and turn down here to bottom I just have the word rules the Character level help me learning beddings for each one of the characters just like you would 4 words get interesting so well I've tried to signal here are a bunch of convolutional filter Lidl that we've talked about in this class they're not so common in an LP but their use-by the motm to get where is a bunch of filters like this is a filter of length of 1 this is a blank too 3 play Signal is that I'm doing a kind of moving window across this example presentations via a dense layer that connects the characters that I'm connect represent combined via what's often called maxpool play stop on here without a window of 3 actors that are formed at each one of those falling representation is just taking all the maxval what's the difference Christians in general this could be the main or the song for some other operation like that calling us common and that's what it a representation for the entire sentence you just piece together patients that you got from exploring or through all of those that looks like this particles to this first filter angry boss level how the MOT and starts to represent were they actually had a few more layers as your see but I thought I would call this out as a separate contribution cos this is one approach listen to somewhere different haven't discussed but I think it'll be really into it for you can you do for the transformer and 4 berth is have what's called positional in coding bacteria so I have word vectors that rock and rolls and they have their usual in bedding embedding space positions in the centre buy this dark red hair 123 they have their own vector representation time into just say a representation of the word that is context-sensitive that knows what position it was in in the Linea good learners representation idea that kind of very free transformer paper learning them you might encode some positional information do this kind of sinusoidal thing position is correlated with Sian periodicity of these waves is that you could generate them for basically any you sort of test positional vector4 learnt in betting mystery for my dear that you could apply any classify positional encoding as a way of infusing them with a little bit of word order in is there a test experiment 8 according to be the same get bus commonality between the first words of those two sentences but also something different yet again if it was like an l i p sensitive to position within the exam play Take the dinosaur adaptor what exactly do you do with the letters that fall better sensitive you can see it time for add to your word encoding in high-dimensional vector the position newspapers at some dimension H diesel free yeah I kind of if I was trying this just as a simple model and adding positional encoding our tribal sun don't have to worry about level ideas let's dive into these models I think that this Can't Be Your Substitute for reading the papers this more as a kind of guided tour great help you as you were substitute for that banging your head against newspapers and reflect because I think this building the most on stuff that we've are simple idea executed on some of the ideas down here I have my standard example the Rock rules with a starts in them has a standard in bed I'm going to transfer that into two ions one travelling forward and one travelling reverse that's kind of orange and Lauren end of the thought you've seen before they use I think lstm cells the paper think it's deeper than this selection you just two layers but obviously you can imagine that this can be as deep as you want once I've got the Ireland's running for the classification decisions that are going to produce the next token premises down here share premises and these two are separate inbox switch models that are trying to protect the underlying ufologist this left when he do the start symbol uses that as it's prediction X1 that are getting searched is Rock here it's just going to run in other important thing that seems meaningful they pointed in the paper is that there share prime minister for the takes you from the embedding into the Orange take you into the soft representations conceptual contextual word representations like if I want to represent rules hear that do that by combining it's in bedding concatenating the 2 hidden states that go along with it for each of these are in its please of the two states that I drew from hearing from here these two States from here where does Elmo turning play this to another task is learning a task specific waiting on maximum life score tell you had a weight the various some together after the wedding and then I have the single representation appearing yellow which represents rules but no that's it on its own this around different if the preceding words are for that they do some other tricky stuff in terms of representing words in this is going to drive directly on that convolutional stuff that I reviewed before so imagine I want to word Up what is first of the Character level bunch of these convolutional filters 7 of varying lengths find me a Max pooling in the weather I showed you before and all those representations from all the different filters get concatenate quite a wide representation what they call highway layer developed machine learning intuition about how essential you can learn some dating information wires in a extreme case you could actually Graham primitive that would tell represent I will later ideas more flexible and allows you to learn meaning information that will still allowed to pass on yellow hair just more dense representation lstm very similar from one of the same offers for developers analysis play Somewhere intuition just now applied very layers are on ideas experimental play they do a dimensionality reduction final WordReference maybe all the tell from back here you're kind of constrain the word representation has to be double of the 2 hidden have this Final project Shania cos these are of much higher dimensionality then you'd want to accommodate for the rest give you your word representation appear and that's what gets passed information about the models that they were medium original and original train on much more data than premier accounts network this is about lstm information in the number of highway layers in the world timer of how big these models get and wanted I have for you if you want no even more NLP works as you have these options files set up the hyperparameters to the model and you can just look at them at the website and see the filters were like terrific model any questions about home or before I move on 108 however trying to put it away turn the scene from the other one some around this play but you can't have these things running both directions without doing some asking otherwise words imagine that these ions are being trained separately LBC in this way as a kind of reminder that the Prem doing this multitask top new data representation if you just kind of know that you can just get straight back out what you put in lockout words what is the densest s automatically on it YouTube These Are and Tesco type oven is to use these representations in another task rate context of the paper the idea that they're really pitching is there WordReference another model so in the simplest case and it is kind of what you seen my notebook further benefiting from August other stuff past performance and propagating that back onto your nan's or we turn your nan's and we please assume that the task specific learning the fine-tuning that you did was just on this soft maximum reason in principle why you couldn't have that property down further to the I have in mind or is that for Transformers in propagate down to all ok so let's build up the score model structure here I'll make sure I highlight when I think of the real set the bottom it'll be familiar when is for example the Rock rules showed you before they don't learn these embeddings in the paper they rather do them in there Bestway generalised that you get very similar result embed sound that gives you the word represent and the call this out with his equations here because they're presented in a kind of Normandy away in the paper of these two on the sea all of the player is the one where the real action man is this is where the paper the tidal attention Is All You Need form the next represent warranty product attention that I showed Fallout that is exactly what I showed you for the inner light week that normalization by the length of the river just now that's like the scores relies on the happened to do the song in the paper of the weighted attention cuz instead of looking back from the hypothesis into the premier Buckingham you want to be an a view of entirely familiar night of the represent the dimensionality of everything that we're going to see if we build self-attention is just attention all around rebuild are these two kind of sublayer structure piece form but I called a liar here the attention where did representation that I got here out on the in this is familiar to if you think back on the attention lecture a lot of what we doing attention is kind of take the representation we care about that's what's happening here so the orange representation is actually just this kind of attentional thing on WhatsApp opera together they do song and then they drop out you see a liar here in the sea turn the drawer this year this kind of Subway or struxureware elevate something below that was an interview Sunday Sun the fact that all these dimension out these are representation layer normalization this is a kind of optimisation trick that helps the network to learn I've given the calculations here this is a conversion of Z score make sure that what is layers are scaled get two layers of a feed-forward network with a real you activation funk I kind of similar things this nor this value here is here I'm waiting a BIOS how many activation another dance cff again one of the sub player cff Dropout on cff ucf liar lighter yellow yellow here we stood around attention tell me showed you before about it UK would be like whatever we picked you like 50 it's powers from this network structure that every single one of these representations from the Grey on a pest cancel I don't see any spare 650 down here play some of these design choices because it's at least you don't have to do tuning of the dimensionality of each one the paper you know that I've just started here what raisins is m in which this network is going to get big is westcord multi-headed anything new it just going to do what we did before a lot more with an example are the green here into its own attentional space meaning for is that this is that attention calculation before but now we have parameters you ok particular head of the motor the representation please dance call that HR actors over here who is well same calculation except now it's W2 w is a sneeze represent vol 3 Newport all the way up to what they do in the papers a day heads who is like if I picked 800 as my dimensionality natural dimensionality for my transformer these would have dimension one networking grow what they're trying to do is kind of provide ensuring that you have lots of parameters to have pacifier learning how to map nothing here is that there is not just one 6 6 repetitions of everything that I have in grey did you hear that we do Motorhead at attention at the bottom of each one of these layers the Hulk done in parallel a huge advantage of the transformer over like Ireland's is that a lot of this real Speed games and that's one free form and unbiased about how you there no words interact with each other you get a lot of optimisation wireless sterilised word to work as I drew it in the way it was trying to review which parameters were shared vs worth which was we'll have copied all these inputs and just had a bunch of different studio gradient information flowing back show me kind of why you can think of Jesus the position to understand the diagram in the paper which I absolutely did not understand so side is Chris repeated for every state in the end one state for compactness but like the rock rules would be free wrap code Estate 10 code estates and mythology and Culture I didn't quite confront in my Core discussion but just imagine everything I was doing exam do you like a dialogue agent or summarisation or Translate old estate play the Harry Potter send back everything in the pretending to all things line in both sides Adams of Chris repeated for every decoder state find me just wanted to point out this is a complexity and I'm like I talked about in the decoder self-attention is limited to preceding words that word that has to have turn on light you don't predictable for the transformer who is the if you have the transformer in bi-directional encoder representations from transfer build up a model I have rules show class token that Bert has positional embeddings just as before also have level in bed he did in the six sample in dark where is it if I got a two example thing like premise and hypothesis you embeddings one for the fur is infused with that high-level position does represent talking you just do a bunch of Transformers I think the signal there is that you do a bunch of self attention on his representation that flows into the transfer turn on the self attention staff it's that Dropout and sub layer structure with the feed-forward network at a certain point in that mixture of ideas but surely the important thing here the start of each one of these transformer blocks we do all of this attention all not an rnn enforce rnn it's more like this dense thicket of k'nex was it National repeat the transformer blocks U2 in the number of them to use this model to make predictions flowing in both directions you can't just do a standard language modelling instead as a test stick on masked language make predictions about certain words that have the mask Valley actual word and then learn to predict the word that was there turn about 15% of the time into the net missing a bi-directional mob problems word seeing there language modelling just trained in a really innovative pieces for doing transfer learning and fine tuning play adopt in the paper is that this class token here is used as the summary representation of the whole exam means is that in the simplest case you could just add a classifier on top of it for an alive for example where we took the last Ada the first in the last eight and use them for a classifier who you just use this Final transformer representation the class ok so just do they advocate doing a lot of fine-tuning of the premature so you put this classifier on top play do you update the premises for that classifier but allow that gradient play all the book done fine tuning code that a release next Friday think about mass language language modelling task they train the network The Wonder primitive that they release prediction task which is binary sentence yes they can generate their own data go masking out certain positive instances are actual sequences of examples from there man went to mask store set gallon mask milkshake is next because those two sentences did actually occur learning negative example some random s training proceeds in the way that you would expect using this class label the whole net high level next Centre it's been jointly trained against the language modelling task local coherence nobody notions really right funny and delightful Twist Bert is that the vocabulary is time a Burt model yeah I did the uncased one from the Snowman vocabulary is only got 30000 gonna be so good tiny Love Academy vocabulary puzzler how's it going to do well with only 30000 tokens well if you look at those talk here you've got like folder that's a word #google pause the moving in this other funny word Fragma use their tokenizer which you can download their code and load in the tokenizer here this isn't too surprising that doesn't normal thing maybe it's unusual that horrendous this contraction but more left it looks like words but then if you say falafel gifts Sesame Street it does not know that word that is not in the vocabulary until it breaks it up into a huge number of smaller token laundry symbols and boundary symbols tell them model that it's word inter can I use an underscore for white cremation Islam ladder strength from sub this is great and the Sorcerer shows you why you shouldn't do your own tokenization with bird because you want to let it do it's own divisions talk about an efficient representation gas 2-years ago but this will be in some basic information about the models they released her Birkby a large share this is the large one this is truly enormous and if you try to work with this on your laptop everything my fault I guess that's the last are limited the sequences of 512 tokens that's a long as you can how many positional embeddings Leila which reminds me of the has the flexibility to run for longer is about birth Stadium Qatar part of the transformer ride it doesn't have the decoding part that kind of handle and possibly by the way you doing this Force approach Dakota was a hassle it was getting in the way of being truly by Direction change to this letter mass that's one of The Shining like a second sentence yeah yeah if you just mascot it will turn to 3 minutes before 18 just point out for that nope architecture your developing for your from contacts to address I would say surely it's going to because these representations were training a lot of day I know it might be hard for you to bring them in busy working with cause maybe you're using scikit or using pytorch on your own way of doing the fine tuning that they advocate stop you because you could just use The Notebook tool twist on the standard thing for our nan's right so far as usual embedding space steak can't express representation just look up your tokens get the indices look them up in the embedding and that finally gives you the vector sequence that the iron is actually process contextual representations intermediate steps and go directly from what is a vet where is A&A here had to be identical he had to be your network a real head to do this this is a complete used of the SST sentiment frame framework with Elmo you get impressive results with back to the base through the song easy to bring me things then you've got to do a little bit more careful preprocessing a bit vectors cos they are very large straightforward to bring it into the Paradise running even better recording power any of that play some morning if there's anything else what would be the if you have ideas that come into my account early more discourse only the blind because I think you can bring ring and stuff like that stop it linguistic can't text brilliant matters from this very ideas read the papers and let me now it's turning over to sin related service short talk is going to be about 401K in the course with mostly talked about and of learning distributed representation words or sentences but like mostly Limited of a few words right if you think about the assignment that we've done for like word similarity and a lie for sentiment classification shout at sentence like just I think I'll send it straight if you have a belief that passed out to a paragraph breathing like a Languedoc some of your projects you might be interested in are you tasks for longer talk about a couple of ways representation before oh yeah so far you know that we've been doing representation there any have the word or sentence level and this is good because we can have like a doctor that represent or imply that we can't or we can using some distance metric and those things are like they're useful I'm for a wide variety of Argo for today is how can be apply and are you methods like the one that we've been using two longer tax so for example news articles scientific papers reading books or transcripts of that example tasks that we might want to do it like that you have like the transcript of a pro hearing and you want to predict whether that transcripts in a positive manner you might want to like cluster documents together so 4PW hundreds of different news articles and you want to cost you together the one that are talking about the same event or can't something that you want or even reading comprehension so I'm if you're familiar with squad that's a reading comprehension dataset on that kind of last one can't find the answer and within some but there are even reading comprehension and datasets that look at the new QI dataset takes an entire CNN article and kind of does a similar thing to squad we're on a question and try to find the answer somewhere within the horrible so you can see how that would be a significantly more difficult task as you kind of have to finally example be summarisation where you know if you want a girl from a very one text to a photo presentation and they still capturing Oldham talk about a couple of methods and that early hopefully for you guys and why start off I want to refresh and unlike representations of words we seen a ton of different you know from the very basics just using like one-hot represent one word in your vocabulary or do you some of these like obvious and I will even from today using something like burger Elmo but kind of like the question you want to go from work doctor's to something longer it's like you're if I have better than representing and like eating the individual words in my input how can I have composed anyone have any ideas just off the top of your head actors if you think it was just like to take all your word vectors and really good approach which is like you know often done like not even at the baseline but just like as the primary approach to get like a document doctor giving a bunch of work activity and especially if you have like context-aware representations like maybe that's good enough yes I don't like the first good be fine they said and just to do what sort of like a bag of War doctor sort of thing where you somewhere average or kind of doing next over the word doctors and in your paragraph wake from drawback the only thing like documentary can be very different context in the world like like the structure of the sentence or like I'm don't worry A1 kind of like you know step above doing that is 2 instead of just like to appear everything to use like the structure of the sentence itself you know too matrix operation but motorway so like you know maybe you can put your sentence for combine the word based on that forestry that comes it's also like a pretty good approach but it does like rely on your person to be accurate and doesn't work as well once you have more than one think that you might I want to try is like using an urn and as your document holder rate so I something Singh is like training in Ireland as an autoencoder for the paragraph on talking about you have event you know like having the last property although back from whatever dance from pastor like me if you're doing the classification something that you could do with use the output at the last time step on as kind of like an embedding for your whole document in like we've seen this one houses on the site you're about was kind of like you know the situation were like doing this informations in the first sense and then you're a bunch of just can I fill her permission so if you use the very last thing baby do the vanishing gradient problem where all of your signal is gone away especially for the case that was thinking about now I were you have a very very long time your document you know is like many sentence is long and you're taking kind of output of the last time stop as your presentation and then yes exactly so through the vanishing gradient problem you know love me that contacts from the beginning is important I've been in a lot of from the beginning the abstract or like the first entered an order called single for whatever task you might want to do we've developed specifically kind of through this representing 1 tax document length and this might work familiar because it's from the authors of for today like this in contact since the doctor McCallum I'm just you like I just do a quick brief review of kind of what is going on when we do train go to Beckton essentially we have an advisor sliding window over the current word and secured like the current word would be I'm like little links including like this whole section of the sun sing the surrounding word of a current word to try to predict our current world and that's kind of like at the objective that rising 4 population of this word embedding matrix will get updated and then by the end of training what happened containing that it's like a distributed representation of that word and hopefully you know in bed some sort of contact information and like something else semantically meaningful seductive very very similar so the only thing we really adding here is this extra back door that gets combined with the words in our context window which is the document in addition to a word embedding matrix we also had a document m like a row for an each document ID in our training course when we are trying to predict the kind of like centre word in Ark also incorporating the document doctor would you can kind of think of as like a memory for like the concept of the document overall can a paradigm or training this word embedding matrix simultaneously with the document m yeah you can get like your document representation by Simply extracting a row from the door influenced by where in the visit somewhere so you can have just like think about it as and then we feel like the entire document as a whole so like for each and context window within the whole document dogman I do bachata in the middle position as the thing in addition to this will it ever be whatever BBC never tried to predict according to the paper no I guess I'll do you like but no it's not actually not as much by the language he could just jumping even though the settings it's not it I just feel like yeah that's possible one of the trade soul it's like you might want to initialise a word embedding matrix with like some factors that are already through trained and then your kind of like using this is more like a retrofit introduced like I think that just me possible but it might be like more of an edge case this way like one could I could even get like that are word embeddings because you had help overall context of the document as well as like the words that are true spell the word embeddings are like all of the doctor you could think of the representation for one word in one document as part of being of both the document actor open any work done on my documents volume level b the beginning middle and end of the document have to find the first second and third period but there's only work on text Tyler revised method for discovering which parts of the document were important and I think you might be drawn somewhere inside them and freeform well has enough generally that you could do it at the paragraph level play pudding yeah so turn to summarise simultaneously learns word for every word and also a document doctor for every dog you and that some of the benefits so that it's kind of like this unsupervised training paradigm so you can get good document doctors without having to train for some specific downstream and it's kind of like truly bass sound like the word play didn't talk about with what kind of happens at inference time so like you know if you use like the doctor that kind of approach to get document vector is like you know what happens when you have a document I wasn't Prodigy original training corpus so like isn't present in that document New Tricks what do people actually proposes for inference time is you this me you will the word actors in no longer at dated but old men the document Matrix and with this new document and an actually like retrain for a few people but I'm driving back to her for that one potential issue with this and that really like non-deterministic because of pre-existing state like within the model so if you have do this input document you might actually get two different actors this I just like training for Warriors to wrap it Up on here are some good resources that you can turn to and if this is something that you're tackling I'm in your own something involving one tx34 presentations 4th package out with we sing before with words that provides a very good doctor that you can just cannot use out of the box so that's really helpful and then for general document in betting on the floor library has like some good classes that do like various different types of bedding so you can have like a whole pipeline combine them in a variety of ways simple polling or like also and Ireland if you are working kitchen stuff play music yes I was talking with Chris earlier about like how effective it would be to you or liked even used and as I can encoder for like a dog has like like a cough with a pre trained model the maximum sequence length is like 512 I believe so that could be long enough for something that you want but it might so can you just depends on your task yeah you could do that yeah and then I had to make a design decision that's like how you want a combined like advert sentence including level and there's been a lot of work down in there is like a whole plethora of thing that is over it's not in the spirit of the birth paper 2 sentence tokenizer do you like that should come just keep putting Saxon learning continuously as opposed to doing it 30% practical opportunity to think about applying it passionate read to a bunch of obstacles computer play something else 