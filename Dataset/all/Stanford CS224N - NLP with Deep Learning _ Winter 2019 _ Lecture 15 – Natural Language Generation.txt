is it going to be learning about natural language generation it's probably going to be a little different to my previous lectures because this is going to be much more of a kind of survey of lots of cutting-edge research topics that are happening in an orgy right now a few announcements so I guess the maintenance man is just thank you all so much for your hard work I know I'm the last week would have been pretty tough Simon five was really quite they think and it was a chance I just need days so we just really appreciate all the hard work you put in we also understand the project proposal with sometimes a bit understands the Expectations for some people these are both new components of the class this year that we're not present last year we have to go through some running as well as the teaching staff so just we really want to thank you so much for putting everything into the class continue to give us your feedback both right now and in the end of quarter people going to be doing today learn about what's happening neural approaches for natural language generation play super super broad title natural language generation energy encompasses a huge variety of research areas each of those could have had their lectures and we could have a whole a whole quarter worth of classes on energy is a cover a selection of things today is Messi going to be guided by the things which I've seen that I think a cool or interesting or exciting so it's by no means going to be comprehensive but I hope you're going to enjoy some the Suffolk and learn about we're going to start off by having a Ricoh do you know about triangles generation same page text about decoding algorithms so we learnt a bit before about a greedy coating and but today we're going to learn some extra information about that and some other types of decoding algorithms I'm going to go through really quick tour of lots of different analogy tasks and a selection of neural approaches to them open talk about probably the biggest problem in another research which is an alternative evaluation and why it is such a tricky situation concluding thoughts on energy research one of the current trends and the where we going Sara cap play natural generation to define it just refers to any setting in which regenerating some kind of text energy is an important component of lots of different tasks such as machine translation with ready-made tractor summarisation with an a bit more about that later log both chit-chat and Hospice creative writing tasks such as writing stories and writing poems what's a sub component of a free phone question on doing a squad project right now that is not an analogy task because you're just extracting the answer from the the source documents but there are other question answering tasks that you have an atom I was generation component that's name is another example of a task that has an MOT so I can print he is a pretty component of a lorry different and then I'll be socks story 12 recap is what is language some people are little bit confused about this I think it might be because the name language modelling sounds like it might mean just simply in coding language representing language using in bedding for something so it's a reminder language modelling as a more precise meaning of bowling is the Task of predicting the next word given the word so far the song which produces this conditional probability distribution that does a task is called a language model that language model system is on and then we often abbreviated as rnn I hope I remember my cat is due remember what a conditional language model is conditional language modelling is when you're predicting what words going to come next but you also conditioning on some other in as well as one of your words so far what's me examples of condition allowed 1LN include translation where your conditioning on the source sentence turn your conditioning on your input text 020 summarise dialogue your conditioning on your dial how do you train tomorrow I guess it could also be a transformer based language model or a cnn-based that you know about those this more could be now to remind you about is that when you are training the system then you feeding the target sequence for your trying to generate so works as target sentence from Corby you have some cheek and you feed that into the decoder predicts what words are going to come next important thing I'm training with feeding the goals that is the reference to target sentence into the decoder regardless of what the decoder is project that's a this is a very bad decoder that isn't predicting the correct words it's not a problem at all that doesn't matter input call target sequence into the decoder I'm going to call you later this training method is called teacher forcing which might be afraid you've come across houseware the first the fact that the teacher that is kind of like the gold input is forcing the language model to use that every step instead of using its own predictions on each day train a model which might be conditional it's another recap on decoding algorithms are you got your train line with more which might be conditional question is how to use it to generate tax does he need a decoding algorithm rhythm is an algorithm used to generate the text from your trains language in the NMC lecture few weeks ago we don't about two different decoding algorithms we learnt about greedy coding and name search pretty capos kodi you just take what's the most probable word according to the language model you can sell where's the next words you feed eternity and put on the next step going until you produce some kind of endocrine or quite me with this because you didn't inside my eyes yes this is how we decoding work to generate the centre turn down before a kind of lack of backtracking inability to go back if you made the wrong choice the output from decoding is generally pretty poor like it can be ungrammatical they can be unnatural kinda nonsense beam search decoding search is a search algorithm which aims to find a hypermobility sequence so it within translation that sequences the sequence of translation words by tracking multiple possible sequences at once is it on each step of the decoder you going to be keeping track of the most probable partial sequences which legal hypothesis hyperparameter of the beam size by considering lots of different hypotheses I'm going to try to search effectively for hypermobility hypermobility the end of beam surge sometimes stopping criterion which we talked about before but I won't cover in detail again Sharon you choose the sequence for the highest probability adjustments give me some more time here's the diagram that we saw in the NMC lecture of being searched is computed and in this scenario rehab team size of 2 looks like off to be done it's expiration problem this shows the full tree that we explodes and then we come to some kind of stopping criterion and identify the top 5 offices and that's highlights beam search decoding I was watching TV the other day I think the hosts the AI hosts in Westworld may be used in search sing I wasn't expecting to see on TV there's a scene Westworld is about sci-fi series that has these very convincing humanoid AI systems scene where one of the license is confronted with the reality of the fact that she is supposed is not human because she sees the generation system of words as she says the TV is that you search because that diagram looks a lot like this diagram here maybe with the biggest free cool cos you know that the mainstream when you see him search on TV and then if you zoom in really hard you can see some other exciting words in the screenshot like knowledgebase forward chaining and backward chaining identify the same things forward and backward fuzzy Logic and neural mainstream good enough Westworld maybe it's good enough redeem citrate with talks about how you have this type of premises k or the beam size and one thing we didn't talk about the last actress and now leaving a recount portion what's the effect of changing up being 5K rees-mogg ok then you're going to have similar problems to greedy was winding you are actually no problems at all maybe I'm not traumon centre call just kind of plane incorrect output get Logic I've been size then you're doing your search algorithm but considering more hypotheses are you having a larger search facing your considering more different possibilities this produces some of the problems of search life is to have this ungrammatical you know downsides to rate in k pause logic has more computationally expensive and I can get pretty bad if you're trying to you for example generate your outputs for a large Uno test set of an empty the motorist is that increasing cake introduced some other problems in an Mt increasing the beam size to match actually decreases The Blue School what's a kind of character intuitive right because he was thinking of them such as this album but try to find the optimal solution so surely if you increase k then you're only going to find a better solution right I think maybe the key here is a difference between optimality and times of the search problem that is finding a high probability seat there's no guarantee that they actually difference between blue score and actual Translate ATK which are the ones that show that increasing been size too much decreases the Pooh score by saying the reason why this happens is because when you increase the beam size too much then you end up producing translations that are too short that kind of explains it to a degree the translation for too short therefore they have low blue because they probably missing words that they should contain why does large beam sizes give you short Translate where are these two papers I didn't see an explicit explanation of why PC sometimes have been search which is when you really increase your search space and make the search much more powerful so that can contain a lot of different alternative finding these hypermobility which are actually the thing that you probability probability but they're not actually thing that you want open tasks like for example chit chat dialogue we are trying to just say something interesting back to your conversation partner beam search with a large beam size we find that can give you an output that is really generic give an example here she went to me start examples from project I was doing are you human hitch up her said something national diet so I save a grocery is 1 hr but said back depending on this is very characteristic of what you see happening when you raise and lower the beam size low beam science it might be more kind of On Topic like are we can see the in healthy healthy I'm a nurse side you know your food and so on that kind of relates to what do you say is kind of bad English writing this and repetition and it doesn't always make sense right but then when you raise the beam sizes then it kind of converges to a safe so-called correct response but it's kind of generic and less relevant right it's kind of applicable in all scenarios what do you do for a living see the particular distant I was using here is one called persona Chapman I'll tell you more about later chat dialogue dataset where each PlayStation partner has persona which is a set of tree talking about being nice I think it's because I was in The Pursuit we kind of have an unfortunate trade Goldilocks zone that's very obvious I mean there's there's a yeah kind of Unfortunate trade-off between having kind of bad bad output bad English and just having something very boring the problems we get with me play we talked about greedy decoding and being the joke can we have an adaptive beam size dependent on the position that you're yeah I mean I think I might have heard of a research paper that does sound that adaptive me like razors sea of the it sounds awkward simple things fitting into a fixed base interview might be possible I suppose you have to learn the Criterion on which you in B&M search and redefining so here's a new family of decoding our presents which are pretty simple sampling-based eco I'm calling you or something so I didn't know what else to call it the simple something I said that says that on each time you just want to randomly sample from the probability distribution very simple it's just like greedy decoding but instead of taking the top words instead just sample from artists something was to differentiate it from top end song the musical top case on playing but I already called k the beam size made it must be confusing something and something for now here is also pretty simple I need to randomly sample from a probability distribution but you're going to restrict to just the top 10 most probable words sing but it's like the simple intro something acid but you want to truncate your problems history the top most horrible words the idea kind of like how being Serge Gainsborough hyperparameters kind of go between greedy decoding and hey you got a high performance which can take you between research and cures instrument event is one then you translate the top-10 just taking old marks which is great cats eyes that you don't want to talk that's just something it should be ok I hope if you think about it if you increase and then you're going to get more diverse and risky output right because you're giving it more more to choose from and you're going lower into the probability discrete that's like a thing decrease and then you're going to get more college and Eric save output by restricting more to the most high probability options is these on more efficient and beam search I think it's something important no multiple hypotheses to track right because I've been search on every set tubidy.co.uk different in a beam size many hypotheses at least you're only generation one samples there's only one thing to track so it's a threesome advantage of I want to tell you that's kind of related to decoding is a softmax temperature septi of your decoder your line was model computes some kind of probability distribution pty by applying the softmax function to a vector of scores that you got from somewhere like from your transformer off in your hand stop my phone saying that the probability we that's off give me the scores hear of a temperature on the soft Max is you have some kind of temperature a primitive tool ring to apply that softmax so what we doing is with dividing all of the schools or logic to make all them by the temperature hyperparameter do you think about this little bit you'll see the raising the temperature that is increasing that I promise you make your probability distribution more uniform is kind of comes down to the question about when you when you multiply all of your school's by a constant how does that affect the soft Max right side to sing apartments beside the exponential by yourself on paper and as a memory shortcut a good way to think about it is that if you raise the temperature then the distribution kind of melts and go soft sand machine uniform and if you lower the temperature like a cold then the probability distribution becomes more spy the things which are rated as hypermobility become like even more what are the high probability compared to the other things easy way to remember they had to work it on paper and then I realised just the temperature visualisation thing usually how is the max temperature is not a decoding is it in the decoding algorithm section that was just because it's kind of a thing simple things that you can do at test time how did it happens the song decoding our coordinates you can apply testime in conjunction with a decoding out beam search using some kind of sampling then you can also apply ace of my temperature to change you know that kind of whiskey vs safe here's a summary of what we just people in Ireland it gives kind of low quality output in comparison search especially for high beam size it's searches through lots of different processes for a high probability a Turner deliver better quality than grease perfume sizes too high then you can have these kind of counterintuitive problems we talked about before where you retrieve some kind of hyper ability but unsuitable helping there's something is to generic or something was too short about Malaysia the methods are ways to get more diversity via by randomness in its so this is good if you want to have some kind of foods and put open-ended your creative generation setting like a generating poetry stories then something is probably a better idea than being such because you won't have a kind of sort of randomness different things crazy top 10 song playing allows you to control diversity by changing it play soft my temperature is another way to control about these there's quite a few different knobs you can turn is it going out for them it's just a technique you can apply alongside make sense to apply it with greedy ticket get more spiky or flat only one section to you so section 2 is energy tasks and neural approaches to that this is not going to be overview of all then I'll do that be quite impossible there's going to be some slight I'm going to start off with a very deep dive into a particular and multitasking have a bit more familiar with andalus summarisation start off with a task definition for summarization one small definition would be giving some input tax you want to write a summary why which is truth the next and contains the main information effects play some candy single document or multi-document which meant means that you just have a summary why have a single document summarisation you're saying that you want to write a single summary y of multiple documents x 1 pdx1 and up to extendable have some kind of overlapping content so for example they might calls from different newspapers about the same make sense to write a single summary that one of those thanks to sunrise things that about it further subdivision of conditions in summarisation play describe it voice and data sets here are some different really common datasets especially in neural summarisation respond to different lengths and different styles of text no one is the gigabyte where is he want a map from the first one or two sentences of a news article to write the headline and you can think of this is sentence compression especially if it's kind of one sentence to headline cos you're going through a long as sentence to a shortage headlines style Centre next one what about is this is a Chinese summarisation tested but I see people using and it's so from a microblogging website where people write summary of the posts so the actual summarisation task is he got some paragraphs of text and then you want to summarise that into I think it's single sentence the 12 actually are the New York Times and CNN Daily Mail they said these ones are both of the form you've got a whole news article which is actually pretty long like hundreds of words and then you want to you some of that into you like maybe a single sentence on multi sentence summary x 10 written by I think irons for people who NN18 just spotted today when I was writing this list is as a new 13 you like Classic months they said from wikiHow so if not I can tell this seems to be you've got a full how-to article from wikiHow and then you won't have all this down to the summary sentences which I kind of cleverly extracted from throughout the wikiHow article that kind of Vikings my local newspaper at the seams this is kind of interesting because it's a different type of text as you might notice most of the other ones and used not so that kind of process different challenges play kind of division of summarisation is sentence indicates play releases but actually different task in summarisation you want to write something which is shorter and contains the main information but it still may be written in just as complex language fishing you want to rewrite the source text using simpler simple language right so like simpler word choices and simple sentence structure that might mean it's or server not necessarily simple Wikipedia is a standard a surplus where is you've got you know you got a simple version and a lineup so you want a map sentencing 12V equipment sentence an hour date of this is New Zealand which is a website that rewrites news Brent learning level I think so you have multiple options for how much it's in flight so definition of the many definitions of summarisation is different tasks review of like one of the main techniques for doing some position charities for summarisation you can call them extractive summarisation and abstract to someone is that in extractive summarisation you just selecting parts of the original text to format the whole sentences but maybe you'll be more granular than that maybe phrases or words summarisation you going to be generating some new text using energy Generation X visual metaphor for this is carried by the difference between highlighting the parts pineapple things nobody techniques are that extractive summarisation is at least to make a decent system to start because selecting things is probably easier than writing text from scratch customisation is a restrictive right and you can't really paraphrase anything you can't read compression if you can only just select Centre and of course a tracker summarization as a paradigm is more flexible and is more how humans made Samurai a very quick view of what pre-euro summarisation look like and he we've got this the diagram from the speech and language processing book pre-roll summarisation systems were mostly xtractive play pre Nero and auntie which we learnt about me and he likes it typically had a pipeline which is what this picture is showing stop I might have three parts content selection which is essentially choosing some of the sentences from the source document to include tell me about some kind of information ordering which means choosing what order should I put the sentences in basically a more non-trivial question if you were doing multiple document summarisation because your sentences might come from different documents are you going to do a sentence realisation that is actually turning your selexid sentences into your actual son doing kind of freeform text generation there might be some kind of editing for example like simplifying editing or removing parts that redunda continuity issues so for example you can't refer to a person as she if you never introduce them in the first place then maybe need to change that she'd the name patient systems had some priest Bizkit albums contents free sample you would have some sentence scoring functions this is a more simple way you might do it he might school all of the sentences individually and you could score them based on features searches are there you know topic keywords listen important sentence complete those keywords using statistics tf-idf example is pretty basic but powerful features such as where does the sentence appear in the document if it's near the top of the document that is more likely more complex content selection algorithms such as for example at these graph-based algorithms which kind of view the document as a set of sentences and their sentences of the nodes of the graph and you imagine that all sentences centres have an edge between the edge is kind of how similar the sentences are you think about the graph in absence then now you can try to identify which sentences are in Port sentences for central in the car play some kind of general purpose block craft algorithms to figure out which notice central and this is a way to find central sentence summarisation is a task blue about rouge now which is the main automatic metric summarisation plans for Ricoh oriented understudy for Justin If evaluation and not sure if that was the first to make out with your if they made like that too much blue is the equation for one of the rooms matrix I'll tell you more about what that means Lisa and you can read more in the original paper which is the Boston are the other idea is the Richard actually pretty summer say blue it's based on n g mobile app differences of blue doesn't have a gravity penalty about a minute league one is a religious based on recall while blue is based on precision singing in the title I think you can say or you believe precision is more important for machine translation only wanted someone of your translations to avoid taking really conservative strategy when you only generate when you save things in a really short translation that's why you add the privacy penalty to make sure that it tries to write something wrong enough contrast recall is more important for summarisation include all the information the important information in your cell information is in the reference summary seem to be important information so recall means you capital of if you see me you have maximum length constraint full summarisation system than those two kind of freight when you want to include all the information but you can't justification why you have record precision please often and F1 that is combination of precision and recall version of is reported anyway in the summarisation it's entirely sure why this is maybe it's because of the lack of explicit maximum control search that I couldn't find answer is more information on Rouge if you remember blue is reported as a single number number and it is a combination of the procedure a different n g which is usually 124 scores are usually reported separately for each and G is commonly reported Reese cause orange One route to Andrews confused with rogue one a Star Wars story came out and see so many people miss typing this and I think it's real so rude one is based on you Graham to basement by removal app it's kind of an allergy to blue really except recall basement position base when is Ruth which is longest common subsequence overlap so the idea here is that you're interested not only in particular engrams matching up with in Uno how many how long will sequence apparent read more about these metrics in the paper that wasn't on the previous page important thing is there's now a convenient Python implementation of maybe it is not apparent wow that's exciting but it's actually pretty exciting because for a long time there was just as pulse quite hard to run and quite hard to setup and understand so someone now there is been here it is show Python version check what it really does match up to the postcode for people we using a using Rouge your doing summarisation for your project make sure that you series about later I need an assignment for you thought about the shortcomings of blue as a man has some shortcomings as well as a metric summary spell Lisa tell me 12-year old approaches for somebody station so 15 I don't have another dramatic reenactment on freeze spell published the first c2c summarisation paper this as your NMC has recently been super successful what with you abstractive summarisation as a translation task and therefore apply standard translation 60 methods to it play the standard detention model and then they did a pretty good job at Google words that's the one where you're converting from the first sentence servicing news assholes the headlines what's the kind of the same order of magnitude of length as an empty right cos I'm into your sentences sentences this is kind of sentence sentence sentence and you can get pretty decent headline generation or sentence compression using this kind of matter turn off that since 15 there been lots more developments in neural and starter summarisation elements in a collection is make it easier to cough it's pretty obvious because in summarisation you know you don't want to copy have a quite a few words and phrases the thing is if you make it to read it properly then you copy much then there's other research showing how to Singh is some kind multi-level attention as I just showed the attention is being pretty to attractive your authorisation so far so there's been some weird looking in a can we kind of make this attention work and I'm more kind of high level low level course finding version so that we can kind of maybe do our selection of the high-level and low-level what does kind of error is having some more kind of global content we're talking about the pipelines pre-authorisation they had these different content selection algorithms you can say that naive attention attention seek to seek is not necessarily the best way to do content selection for summarisation maybe you want to know kind of global strategy when you choose what support when you're doing a small-scale summarisation but if you imagine that your summarising a whole news article and your choosing which information kind of deciding when you speak louder fancy Michael what else we got learning to directly maximise Rouge other discrete goals he might care about such as maybe the length of the summary and discreet here because Rouge is a non to French of all functions of your generated output there's no easy way to training in the usual way this is the kind of theme of Resurrection Priya ideas such as those problems that I mentioned earlier and working them into these new six especially because you were looking particularly interested in summarisation a lot of the ideas so we're going to explore here actually kind of applicable to other areas of energy or just gonna end up eating plan play the first thing of this is making it easier to copy which seems like probably the first thing you want to fix if you just got basic sequence attention a copy mechanism which can exist outside of summarisation this is the basic attention they good at writing fluid output as we know but they're pretty bad at copying over details like real words correctly mechanism is just the idea of saying let's have an explicit mechanism to just copy over words John Paul you can you please text distribution 22 kind of selects what you're going to copy allowing both copying over words and generating words in the usual way with the language model the now you've got a kind of hybrid extractive at start of approach to someone the papers which are which proposed some kind of copy mechanism variants and I think the reason why there's multiple is because there's kind of a few different choices you can make about how to implement this there's a few different versions of how to implement cognitive but you can look at I'm going to show you diagram from the paper that I did a few years ago with Chris just one example of how you can copy and paste is he said that I need to go to stop you going to calculate this probability pigeon and that's the probability of generating the next word rather than copying it are computed based on your current contacts to your currency code against eight then years you've got your attention distribution as normal and you've got your kind of output you know generation distribution as normal and you're going to use this pigeon we're just descaler you can use that too kind of combine mixed together these two probability distributions is it saying that the final distribution for what was going to come next it's kind of saying there is a probability of generating x your probability description of what you were general probability of copying and then also what you're attending to it that time using attention as you're copying necklace kind of doing double Duty as being useful for the generator to know maybe Tuesday rephrase things also be useful as a copy mechanism and I think that's one of the several things that these different papers do different me I think I've seen a paper that maybe has like two separate one for the copying and one for the for example do you want PJ masks me this kind of soft thing that's between 0 and 1 or do you want to be a hard thing asked me either 01 who makes agent about like you want the Pigeon to have supervision during training do you want to come and take your day set saying these things a copy of these things and this is now become pretty pretty mechanism seems like he's like the idea but there's a big problem with them which is why I mentioned earlier and I probably is that they copy too much so when you when you run a fan systems on summarisation you'll find that they end up copying a lot of long phrases and sometimes even whole sentences and unfortunately you're dreaming having an extractive summarisation system isn't going to work out because you're no copy augmented Sikhism has just collapsed into a mostly extractor system which is turn with ease bad at overall content selection especially the input document you are it's quite long like a news article that hundreds of words that you want to write a simple sentence like the kind of smartest choice to on every step of writing your several sentence summary be choosing again what pretend to what kind of Mako Global beginning and then summarise Thomas's no overall strategy for the lectern I hear the paper ok so how are you do better content selection for Euro sunrises if you remember in this printer or summarisation with that you had completely separate status in the pipeline already had a content selection stage when you have the surface realisation that is the text generation sage these are just completely mixed together you're doing your step-by-step surface realisation that is text generation and then he 232 also doing contents doesn't make sense so I found a paper which is published in the last year it's a nice kind of simple solution to this problem what's maximisation paper the main ideas free simple it says I have a content selection say Force office in Euro sequence tagging model problem you run through your source documents and you could have tagged every word as include or don't include so just deciding what seems important things like a Jamaican into the submarine what does what's my potential stage says that now you're see seek with attention system which is going to generate the summary are you going to reply and moskinator play A Hard constrain the says you can't attend two words that would turn off be pretty simple but effect is it a better overall content selection strategy because I doing this first content selection sage by secret sagging your kind just doing the selection thing without also at the same time during the regeneration thing which I think comes out of your better ways to make better decisions about what to include this also means as a great side effect you have less copying of long sequences in the generations cos if you're not allowed to attend the things which should be including then find to copy really on sequence if you want to copy a whole I'll sentence but the sentence has plenty of you can't really copy along sequence you have to break it basilisk including those forced to be more attractive to put the parts navigate to the mosque does during training have they got back to the mosque think it might be trained separately and check the paper but they might tell me about is paper which use reinforcement learning to directly maximise room paper from 2 years ago and the main ideas I can use rl2 directly optimising escape route out the better by contrast the standards Max and like your training that is the address of we've been talking about the whole class so far directly because it's a non differentiable function use this altar need to who scored during training and then use a reinforcement learning to back changing from this paper if they just use drl object when do you get higher risk SAS fully optimised this route allometric they were aiming up tomorrow when you do that you get lower human judgements meaning of the oral only model has actually pretty pretty bad readability relevance human judgement maximum likelihood this is a quote from the blog post that says we observed in our models with the highest Racecourse also generated barely readable summer this is I suppose a problem right if you try to directly optimise for the Med stop finding that you're kind of gaming a metric and not optimising for no just if you need a blue was not a perfect analogy to actual translation quality so it's not a perfect analogy to something they found that if you combine the two objectives so they're kind of you know protect the language model sequence objective and also call somebody that gets a high risk or objective and you can buy them together then you can get a better judgement cause machine the end is the closest thing we have to measure actual sunrise I'm going to move onto dialogue which is a different energy kind of family of task I like in composites a really large variety of settings I'm not going to have them all because I kind of overview of all the different kind of tasks that people might mean when they say dialogue there's task-oriented dialogue play we're trying to kind of get something done in the conversation what kind of assistant are the same day have you know maybe the trying to help a human user do something like maybe give me customer service or recommendations on questions helping you you know just ask I'm buying booking something to virtual systems on your phone can do or can kind of task-oriented dialogue tasks are co-operative task everything we got two engines were trying to stop the task together via dialogue you up to that would be adversarial so anything we have to agents to a try and compete in a task and that competition is conducted through the opposite to task-oriented dialogue is a social something where there is no explicit ask other than to socialise so chat dialogue is just doing it for the social fund or the company I wasn't even work on kind of like therapy or mental well-being dialogue and obviously going task or social it's kind of a mix biodiesel ones where the goal is to maybe off a kind of emotional support to the human user so very kind of Brief overview of how the d plan it's kind of changed I like research kind of the difficulty of open-ended freeform natural language generation systems for often not doing freeform predefined templates meaning you have a template when you just fill in some slots with the content retrieve an appropriate response from a Corpus of responses that you have in order to find an appropriate response for the show me simple systems they had some very complex things going on like deciding you know what their dog sauces and what temperature should use them and so on and the all the natural language understanding components understanding the context so far 1 effective but since again 2015 which is when and became stand just like lots of papers applying methods to die led to renewed interest in open-ended freeform dynamics do you want have a look at what are those early $16 papers look like his two kind of the first ones to people quickly applied CMT methods the dialogue but it quickly became very poor naive application of standard nmt methods has some serious pervasive deficiencies when applied to a task like chat dialogue and it was for something station examples of serious deficiencies generic naceur boring responses Romans irrelevant the dialogue agent kind of something back that's just kind of fun related to what the another one is repetition this is pretty basic but it happens a lot repetition within the utterance and maybe repetition a cross references the difficulty is kind of lack of contacts by not remembering the conversation if you do not condition on the whole conversation history there's no way on but it is a challenge especially have a very long history to figure out condition on it is the lack of consistent Force maybe those two papers that reference on the previous slide if you know if you train a model to maybe take the other users last actions and then say something back and say something back I've done your dialogue agent will have this completely inconsistent lives in your lives problems and give you a bit more t-shirt on them this irrelevant response response that kind of unrelated these atoms unrelated because it simply generic this is kind of like an overlap with a response problem unrelated because the model shoes and kind of change the change the subject to something unrelated position of many there there a lot of different papers which kind of attack list of relevant response problem but just 141 for example is that you should change the trailer time to optimise mapping from in protest response TV or maximizing the conditional probability of tea given us what's my stand maximum mutual information that's what this is here Mason are you kind of rewrite the objective like this and you won't see somebody so you can go and get this paper here so you're trying to find your response tea that kind of maximizes this thing which is kind of like saying it needs to be probable given the end like as a ratio of its probability in its very then it gets penalized and it's kind of like about the ratio and probability given the input and just the sound alone this is meant to discourage just saying generic what's the arrival response there's definitely a strong link between the irrelevant response problem and the kind of generic or boring response problem to look at the darkness or boring response problem play some pretty easy fixes that you can ameliorate the response problem when you're really getting to the heart of the issue is a different easy task you can certainly do after example you can just directly up rail words during games hooray words kind of get a boost to their probabilities and then now with more likely to produce them dreams ezkeys free sampler sampling decoding out of them rather than being certain we talked about earlier we could use softmax temperature as well there's a kind of test and fix what days is a kind of late intervention intervention would be maybe training and model differently so I'm calling his kind of conditioning fixes because he fixes kind of related to condition your model on something that's going to help with me last for sample is maybe you should condition the decoder on some kind of additional number lesson work showing that you know if you're doing then maybe you should go and samples and related words that are related to what you said and then just kind of attend to them when you generate and then you're more likely to say something is kind of content full interesting compared to the boring things you're saying Tracy could train a retrieve and refined model rather than a generator from scratch retrieving refine I mean you supposed to have some kind of Corpus of just general things you could say and then maybe you sample one from that test all the training set and then edit it to fit the current stop this is a pretty strong methods produce much more kind of diverse and human like an interesting artist is you can get all of that kind of fine grain detail from the samples and then you edit it as necessary to fit your car no downside to these kinds of methods so maybe it can be hard to edit in to actually appropriately for the situation but it's certainly away to effectively get like some more they Gusty and effects of the repetition problem there was another kind of major probably notice for replying seat speak to simple Solutions in more complex patient assimilation Loch repeating and G during concert a really quiet effect is a drinking search when you're considering you know what am I type of the Seas which is just the top k in the probability distribution you say well anything that would constitute repeating and G just get through repeating on G I mean if you did too would you now be creating a repeating let's say 2 g if we dividing it was banning all repeating by g auto G or whatever then you sent you just have to check for every possible word you might be looking at him search whether that was create a repeating and G well I mean it's by no means a kind of principal solution right if you're taking should have a better way to learn but as a Kind I think that works Six Nations coverage mechanism this is mostly inspired by the machine translation setting a covered mechanism is a kind of attractive that prevents the attention mechanism from attending to the same word small maybe repetition is caused by repeated what's the same thing when you times then maybe you're going to repeat you know the same output many play Simpson quite pretty well but it's definitely complex thing to implement as that can be resetting the dusty make the simple solution easier and are the complex Solutions maybe you could to find a trainee injective to discourage optician try to sing differential one of the difficulties that is that because you're training right why you always looking at the gold input so far then you never really do the thing where you generate your own output and repeating yourself so it's kind of hard to find the penalty in that situation play kind of non-differential function so I kind of like how what's the tool paper with optimising Cruz memory kind optimise phone stupid to storytelling storytelling there's one diminishing neural storytelling we're going on right now use the some kind of prompt to write a story example of writing a story give an image or given a writing prompt icing next of a story giving the story so far generating a story from an image what's interesting here is that we have this image is a picture of an explosion and then here you have tell me about the image for Written in the style of Taylor Swift you have to be the only light bulb in the night sky I thought oh god it's so dark out of me as I missed you I promise there wasn't any straightforward supervise captioning dataset you know Explosions and Taylor Swift lyrics they kind of Londis is that the user kind of common sentence including space particular kind of sentence in codeine called skips or practice trains this coco image captioning system to go from the image to the encoding of the sentence and then separately they also trains a language model conditional language want to go from the sentiment coding to the Taylor Swift lyrics and then because he had this shared including space you cannot put the two together and then go from the the picture to the embedding to the Taylor Swift Style play music I think so a story generation systems recently and this is an example of a system which need a set when you write a story given a pump is very impressive stop a convolutional language model 66 system that generates the story given the in show me details for encourage if you want to check out what's the state-of-the-art in store regeneration you should check this out there's a lot of different things going on with her fancy attention and combinations and someone and they managed to generate some really interesting impressive stories example we've got some sing story generation that kind of guy vs non-generic it's solicitor Jurassic World the Proms you can see here kind of the limits of what is state-of-the-art solar generation system although it's kind of InStyle it's mostly kind of American descriptive it's Not really moving with helpful if there's no kind of events here so the problem is getting worse can you generate for longer when you share it along a long text then it will mostly to stay on the same idea without moving forward with new ideas Fallout alarm and to hear if you want to check out about post generation other things I'm going to skip head because I'm once again energy evaluation section because that's pretty import we talk about automatic evaluation Matrix is word overlap objects such as blue and bruised and meteor we know the old machine translation they can't even worse for summarisation mostly because someone station is even more open and did the machine translation origination of You've Got a Match D and G is even less even worse than just like dialogue then it's just kind of a disaster is not even a metric gives you a good signal at all anything else open and dead like story generation show and you can check out the bottom that was overlap matrix I just not a good fit for diet boxer showing you some plots of the correlation between human school on a dialogue I'm here if you're not seeing much for correlation at all 60 on this diet the correlation between the blue metric and the human judgement of whether it's a good Dalek response is the Croatian is I mean looks very weak and that's not the papers that show much the same thing what other was metametrics commute coffee capsules how powerful your language model think about generation decoding algorithm is bad in some way then perplexity is not going to tell you anything about something you apply to your training language you've got a strong Irish model you necessarily how good your generation is you might have that watermaster evaluation or well what about word embedding matrix that tricks you want to compute the similarity of the word embeddings or maybe the average of the word and bring the Cross Centre the Western South videos rather than just being very strict and saying only the exact same word counts you say well if the words are similar in whatever his face then flexible but unfortunately the same paper I said before shows that this doesn't correlate well either with human judgements of quality at least for that start the dialogue task they looking time is showing the correlation between human judgements and some kind of average of word and bedding based that's not a great correlation we have no automatic matrix to adequately capture overall quality and natural language generation what can we do instead often the strategy is you end up to finding a small kind of Focus automatic metrics to capture the particular aspects of the generated text that you might be interested Jean-Paul you administer fluency and you complete that by just kind of running a well-trained language tomorrow over your text and generous and probably see and that's kind of our property for the how well that's good fluent if you're particularly interested in may be generating tax in a particular style then you can take a language models trained on the corpus representing outside and now the probability tells you not only is it a good text but it in the right side that's another thing as well they're like you know diversity and you can come up with usually just having some statistics about you how to use in ground words input new kind of computer similarity score with the info things like is that yes we have a really difficult situation with a non evaluation and there's no kind of trick often the captures overall quality lots of these things then to track some important things that you should know which offer how automatic evaluation metrics for energy are really regarded as the gold standard you know that human evaluation is slow and expensive are those the only problems with human evil do you have access at the time on my new knee cancel all your problems so you have unlimited human about does that actually solve your problem no from personal experience evaluation in itself is very difficult to get right it's not easy at all because humans do a lot of weird things humans are unlike a metric or an automatic there exist is a call and they don't get bored of your task and I don't want that anymore interpret the question you asked as a kind of case study this not going to tell you about Project ideas where I was building some chatbots and it turned out of the human evaluation was kind of the hardest part of the project show the chat box for the Persona chart data investigating controller Builder all aspects of the generated text such as you know whether you're people can I have the same problems we noticed before models that control year the specificity of what we saying and how related what was saying is to want use a set either you know something like yes I'm studying at the moment and we can kind of control turns control knob then make say something about generic like and then like 20 dots or something just completely bonkers it's just over there what you know and there's like a sweet sweet spot between when you say that sounds like a lot of fun how long have you been play we have a nobody can turn to determine how semantically related what you say is what they say sing us a way to control the output of the energy system but I want to tell you about how the human evaluation was so did generate using human evil how do you ask for the human quality judgements what kind of simple overall quality questions like in a how old is comes out in go engaging comparative which of these is the best response they were just made your promise with all of them unnecessarily very suggestive the difference pondence have different expectations and this affects their do you think this user is a human Orange and entirely on this response knowledge of Vauxhall opinion parts and what they think they can do catastrophic misunderstanding of the question so for example if we ask with this user was a chatbot engaging then someone was gonna say it wasn't getting as always right back quickly is what we meant we might like that engage in conversation partner but they took a very much for assumption engaging mean overall quality depends on many underlying factors and it's pretty hard to kind of find a single overall question that captures does overall quality doing it's me and the breaking it down into v more kind of factors of quality when we sort is that you have maybe he's kind of overall measures of quality of the chatbot searches how engaging was at how enjoyable talk to you and kind of maybe how convincing human and then blows in kind of broke down as he's more low-level components of quality such as you know what you mean just saying when you showing it was listening you asking questions and someone and then below that we had these kind of control attributable to the knobs to be returning and then the goal was to figure out how the thing is factories be open so sponsor findings here and I think I maybe the ones which I will where is 200 the middle so if you know romantic engaging us which means in German easy to maximise handout Argos smash to get kind of nature human performance in terms of engaging us the automatic humanist that is kind of durant asthmatic that was not at all easy to maximize all of us hallway way below humans in terms of humanist convincing being Hume play we were as enjoyable talk to humans but we were clearly this is not the same thing as conversation quality slingsby found in this sum evaluated a chatbot for your actually got humans to evaluate each other human disturbance more conversation 3 poorly on intergas flue and see listening they didn't ask you telling of questions and Mrs kind of the reason why we managed to like approach human performance in kind of enjoyable nice to talk to you because we just for example turned up the question asking mob ask more questions and people respond really well that because people like talking about themselves so interesting Great British shows that there's no obvious just one question to ask one question to ask is clearly humanist completely different reeds on our we are doing right where is asking his multiple questions kind of give you more 11/2 going to skip this just because I'm ok so here's the final wrap up thoughts on energy research the current trends and where we going in the future convert 3 exciting current trends identified energy and of course your mileage may vary you might think that other things wanted Tinkerbell firstly incorporating discrete latent variables into energy so are you to go check out this list if no because there were some examples of with some tasks such as for example storytelling or task-oriented dialogue we are trying to actually get something done the more kind of concrete hard no son of the things you're talking about my entities and people and events since you know GO station and does this missing what kind of modelling is discrete data variables inside these continuous energy methods is alternative strict left or right generator so there's finishing work recently in trying to generate text in ways other than left right some kind of parallel generation play something initiative refining a pint of top-down generation for a piece of text that maybe decide the contents of each of the sentences before writing the words third one is like alternatives to maximum like your training with teacher for forcing it's just the the standard method of training a language for all that we've been telling you about in the car so far do you know there's a mountain bike on looking at more kind for this level rather than Word level time to slide I don't have time to put the references in but I will put the references in later and it'll be on the courts website so you can go check out later is there going to be a review and audio research where we where we go what is I think that about 5 years ago and opium deep learning research was a kind of a wild west right like everything was new and BBC One Show what the new research landscape was because your methods kind of changed it was uncertain how much it's a lot less while I'd say I think it's kind of standard practices have emerged and show that sort of things changing but you know there's more people in the community there's more centre practices we are things like towards so we have to take radiance anymore I'm not mess wild now but he does seem to be one of the wildest parts remain what are the reasons for that is because of the lack of evaluation metrics that makes it so difficult to tell what we do identify like what is the main method to the working when we don't have any metrics that can clearly tell us what's going on language that says that the neural energy community is rapidly expanding so in the early years people mostly transferring successful and antimatter to various and time seeing you increase lovely more inventive energy techniques emerging which are specific to be non-empty generation sets how do you get back is there a motor saying there's increasingly more kind of neural energy workshops in competition sing on open-ended and he by both tasks that we know I'm not well suited by the automatic metrics that work new Generation workshop storytelling workshop challenges as well where people and so therefore example conversational dialogue agents to be these different kind of community organising workshops and compositions are really doing a great job to organise the community increased stability and standards of the valuation so this is great but I say the biggest ROBLOX progress is definitely still evaluation last thing on share with you is 8 things I've learnt from working in an orgy when is the more open and other task the harder everything because defining what your doing becomes harder telling when you're doing a good job becomes harder make things more welcome how to constrain your tongue easier to to computer when is aiming for a specific improvement can often be more manageable than Amy's improve overall generation if you decide that you want to increase diversity to achieve and measure and just saying we want to do over generation quality because of the valuation problem is if you using a language model to do energy a model that is getting better probably better generation quality if you've got a strong language more anyway to improve generation policy as we talked about before there's also other components that can affect generation apart from just language model and that's part of the problem is that that's not in the training busy to look at your output don't have any single metre quicker tell you what's going on it's pretty important to look out lots of Formula and opinions it can be time consuming but it's probably worth huge mountain Project you need to automatic metric unit is perfect so I know they already know this because he wrote it all over the project instructions then I probably meant that to like maybe you need several automatic how you might track multiple things to get an overall picture of what's going open and multitask is the more likely you need probably several metrics you want me the questions as focused as possible so they found out the hard way if you define the question is very kind of overall very thing then you just open yourself up to the responded kind of misunderstanding you and if they do not actually not their fault it's your fault and you need to change your questions and alkalis Singh is repeatability is a huge problem in today's an opportunity planning in general and the problem is only bigger in an orgy I guess that's another way that it still I'd say that really great if anybody could publicly release all of the generated output when they write energy great practice because if you release your generated out play sun comes up with the great automatic match Fabio Diner Dartford and then computer metricon or you're least with some kind of imperfect magic number then future researchers have nothing comparatives lost me my last thoughts about working energy is that it can be very frustrating sometimes because things can be difficult and it's hard to know when you're making progress but the upside is they can also be very funny so this my last night Harrison Bazaar conversations that I've had with my chatbot Paradise 