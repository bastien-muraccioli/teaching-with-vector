inviting me over and thanks everybody for coming so it's exciting yeah it's odysseum video buddy torium because I was going to talk about stuff that we actually done over the last year's just complete background ID 1/2 years ago first with an 80% position NVIDIA starting robotics position with the University of sobek back to teaching as well it's ok my bad when I had a meeting with with Jenson want to see you often but you're talking about your body what are the areas where are you kind of laboratory has an impact on industrial setting happening right now is really robots in warehouses moving around navigating through hospital hotels driving car the big elephant in the room always but we wanted to look at into Mrs side is really robot that I'm not just about navigating around but robots dead and physically interact with end in self-driving cars of course the main Focuses not to physically interact with your environment maybe with the tires but otherwise you don't want to rush into anything so if you want to have a look at these kind of robots and it's really about interacting with the environment touching things doing this in close proximity to people around you and application to make of his kind of robot industrial manufacturing and you've heard some recent exciting news about where this kind of deep learning technologies for example I have an impact on manipulation settings it industry people with physical disabilities a colleague HR tech Charlie Kemp investigating first number of robots can help people with physical disability and the case even with robot might shaving the person of putting up a blanket when the process lying in the bed so you can imagine that these cases probably has two kind of being contact with a person and all of this has to be saved by the same time the robot has to have kind of the perception capability to reason about what is going on around it any general helping people elderly people in hospitals so this is kind can a frog and we'll have to possess a lot of research to be soul we started the 11 Seattle user goal week only 12 piece of scientists the summer fight sample web 20 several of them here in the room from stand for the bus working with with all the smart people what I want to talk about today project that we started in the lab louder go back to do certain task in a kitchen and the cleaner not doing cooking yet we chose kitchen kind of Essen integrated test environment right in which we can different aspects of this manipulation problem or puzzle right this is the kitchen we have done IKEA kitchen we also have a simulation model of the did you for the kitchens really that again we can integrate different manipulation components into 8 we can do task planning motion planning and things like that make it in a stage play more more complex right so initially actually that you'll see today is most very simple task that just pick up an object and move it somewhere longer on you can make it more and more complicated by introducing people into the kitchen so the robot has stopped interacting with them to understand what the person wants to do and maybe help the person cooking and even may be helping ingredients or cooking ultimate idiots that many of this house we just and inform any of this other application of India pretty matching for example also in industrial manufacturing specific thing we wanted to look at initial Israel we have this kitchen here nice trailer time that we have a 3D model of the kitchen between us we know where the drawers are we know where the doors are even have any more of the objects you might have seen the famous Ennis Connor from the ycb object dataset I'm kind of getting tired of it but I haven't even any of but also got the Cheese Box this kind of object so the idea was under these super strong assumption call the system that can do let's a reasonably simple pick a place like just opening a drawer on getting an object on the perception side use actually in this case just because we did play True depth camera on the robot itself we have to death camera the kitchen in the 80s use um2 to track what's going on latest Ocado that is used by video by the Isaac SDK team that is focusing to put more on navigation system Building and navigation Swann support of course for running interview processing and put a crank arm on top and for some of the more recent work also we could we put a depth camera onto the rest of the the robot so that you can get close up if you want to pick up what's involved in such manipulation task then of the high-level you have is what people called task in motion planning so a support of the washroom is focusing on this way you say you know where objects are found in the world you have models of everything and then you one of my tablet say set the table then you say the target configuration is that the played in the fork and knife and then you have a high level Littlemoor logic planning base descriptions over you say well for the table for example if the table is in the drawer in order for the robot to be able to pick it's not the table for play this in the draw for the robot to get playdough out of the dryer the drawer has to be opened in things like that so you have three conditions and actions and then you can change these and can come up with a plan that achieves the desired configuration asking motor planning and what you do 20 with this high level plan is also the actual motion planning for the robots so that kind of cheese thing of course if you want executed in the real world you need a destination you need to know actually what is the status of the world where the objects are and want to talk about able to grasp the place object you'll be taking better off to pick up the half with him also objects and we can win this please specify all these different define ingredient Eastern on the manipulated self you need a controller that actually executed all this plan when I go through some of these these areas and give an example for the kind of work done in give one example on sete iPad is with a lot of work on 6th optic postestimation and I know there's some really cool and exciting work of course going on here as well so we can ourselves alongside to this problem India 2 years of course if you have an object this is our famous Jesus unlock the ox quite sample to pick it up 1 approaches that you want to estimate where it is relative to the Royal the task is you get an image what is an image and u105 sample in that specific 3D position and video orientation of the DSM orientation in Translation lot of work has been going on in the computer vision community especially with focusing on this problem for quite a while there's many standard datasets and how to evaluate your technique a lot of the focus has actually been on image estimation which means you get a single image of the scene and then you immediately go to an estimate for the 60 post of the object a lot of liquid coming out of order standeford here and the typical approach most recently has also been for example on just redressing more left the image to a 60 Pole large scale training data sets focus of this is all so that you really wanna regress the possessed immature unique ne20 out if you have symmetrical object then the Christmas whether that even make sense with their whether they said uniquely define I'll talk about it on the next line of it another word on trekking but again there to also just kind of a unimodal estimate Kalman filter Style reception where the use of particle filter but it turns out that if you just use assembling this 460 space what's down more left to just being able to do tracking because what space of this object is pretty complicated SME we wanted to focus on is really estimating now the full 60 Post distribution and uncertainty wing bed even for symmetric why is orientation on certainty it's not as simple as you might think touching this is an object like I think only observation you get the question is what is the orientation of the of the object or the equivalent Christmas what is the camera view point pause that uncertainty in the orientation that you cannot resolve base of that single image because you could be anywhere anyway around the object Ride or could even be in this case if it's a metric you could kind of me with the object from the bottom say that that is just because people are using dash cameras how to use a texture and visual information turn off the object and also use colour than you might be able to actually resolve that uncertainty and you have kind of a unimodal distribution of where you are relative for the object doesn't make sense so on the one hand side of course can help you resolve some uncertainties but then even in this case even if you have that optic model at some point of the object from the top even in this situation you still don't know where your are relative to it even if you use if you have a text this part is really they're gone the one in the cemeteries resolving pretty complex actually distributions you cannot redefine actually the shape of these distributions in advance because it often depends on your specific viewpoint and depends on certain locations you have in the sea specific uncertainties just because part of an object is included and because of that you can't resolve to do is who want to do £60 estimation where we keep track of these full distribution SIA approach it is a better sexually idea of cinema and in colleagues they had that was actually best paper at CCTV in 2018 and they kind of their focus was specifically on orientation uncertainty over objects I just want to give you a brief summary on how did work he mentioned you have this object learning is kind of a the Cult reviews on to this object so the idea is that you have an ordering colder this input images and output image going off noisy image of the objective you onto the object from an essay distance to the object turn to encode this in this case it 205 statue can be coded into a clean image of York place with many penis new points and incense what the network learns is too kind noisy image in cold it's such they can specifically also recover the viewpoint onto the off can you do this for many pairs train please just can't generate and something views points onto the object viewpoint call the code it's for the few what urine testing image of the object decoder because you don't need that you're in cold air you just compare start with your cookbook cosine similarity measure 30 telsiu which view from which view you're looking at the Open they did indeed work it just shows wanted like OK that is the orientation of the old nice thing is if you have lit testometric optic like this one look at it from here search very well that is around this object that have the same that looks the same right so very naturally recovers or includes this notion of that object might look the same from different limitations of network is that you care about getting the single orientation out of it and they didn't worry about different translations of the object so it was just a single image in a single what's the foundation for what we did then we said we want to incorporate that into 460 can you develop this technique called poser bpf ROBLOX particle filtering so we thought we're going to bring in some good old into hereby videos party that you and represent densities or uncertainties by samples and then play density areas by having samples in that area and the idea of the specific problem it's just rather than saying find someone's gonna say about this for 60 post base I'm going to stand for certain parts of the state-space in others and gonna solve analytically conditioned on the samples nephews this in different contexts like activity recognition you might have hurt you from Stanford Sebastian his past Lane where what day did couple the slam problem by sampling protectrice off the Ball talk to tech tree they would then build a map you have a sample based representation that encapsulate both uncertainty of the path and uncertainty of them that corresponds to the please use the same idea you said we're going to sample the translation of the object which is x y z solve for the orientation using a discrete distribution this works particle now has a translation again XYZ where's the revision over the rotation on the translation of the doctor this week's first can take an image a particle that has a specific translation that will give us box for the optic in the in patient says that the options further away than the bonnet box will be smaller for example and things like that right and it moves throughout the image 10 ft this region of interest single particle lit period to the same code book that you sort by the Son of My Father beans for the occasion 291000 I do want orientations ok then that gives you the likelihood and that like us what you used to wait your Particles and then you do overtime you can do the resampling and advice then of particle filter techniques by this the only inside what you really to say we do the seemingly crazy thing of having every particle honest 200000 bins that represent the possible orientations and we can do this of course efficiently because we only need to compute a particle in the comparison with the coat book that suppository beauty can do this in parallel we can do all of this thing in real time take me to sample can you see the particles of the kind of uncertainty different bounding boxes of the here's that initially off the Mark let me play this again when the handle of the mark is not visible the uncertainties actually much larger is the orientation right because he don't know where it is exactly but then when the handle comes into view the uncertainty really goes signature really nice handles all these kind of different you what you can do now is even so this is kind of is only based on kind of this letter local region of of interest images you can trust to Global localisation which means you can do detection with this technique as well where what you do is you just randomly 5000 particles restart the bounding boxes of these 5000 particles need to sample the orientation do it and vertical again to this full distribution in very quickly actually Converters then and then we can even reduce the number of particles stop can be done pretty pastel 50 particles what's 20 frames per second and I think this was intuited assassin in turn he has two more than 30 friends now what we have this we can estimate the posters of this individual object what's in this letter in this kitchen setting you also 1st made with the daughter daughter and things like that for that we used to technique that my student tenor Smith introduced to my logo with cold dark then start accumulated real-time tracking can you who is bad if you have a 3D shape model of an articulated object could be can you play a record Hospice Furniture things like that the very fast against you based implementation of if you have a deep space observation of a scene you can then match your dad's information against the model and estimate the articulate the problem is that the estimation is not 60 pulse of a head but we also need to estimated joints of the head as well everything is in this case since it's just PewDie model-based it's very generous so can be applied settings see the trick model alongside with the point load and we found it also for the robot manipulation invitation is tonight search places to high-dimensional and also that is which means if I found your mother doesn't match well if you don't have a quick model than that checking if tracking fails reinitialisation is also not that easy so you know what I've got many of these tasks you can do it of course if you have enough training day but once we have their framework now reply to the whole kitchen where what you see here is how do you stuff is going to be the point cloud that received from the depth camera data the colouring indicates what object the different points are associated with we can do this we can use the technique that I just told you to get the initial post office objects which of the most likely orientation use this to initialise data so we're running down practice individual objects start to track also where the Robot is in the scene and also to check turn off the phone the time soon but you can see the ball but now moving in now the dart estimate tell the manipulator where it should move in order to pick up the object I'm sorry this only works if you have a good model off of that scene what is not fully autonomously and one problem was for example with this initial system is if the depth cameras too far away you're not going to get the object pause actually accurately enough so that you could really always crap now what we have is we have a depth camera also on the the Armin Van you get a more accurate estimate for the options that you actually want the nice thing is we can do all this tracking in a single joint optimisation over all the poses of this object and the robot you can incorporate physical constraints into that estimation problems such as by sample objects shouldn't interpenetrating rupees to the parcel here then the manipulator control and for that we using a technique that Nathan Radcliffe introduces caught his Romanian motion policy if you have a rat protector of how the robot arm should move what do you need to generate at a very high frequency you need to generate control commands later in one of the most fundamental workers hepatita also is with someone hear it said you wanted to reasoning explain the tasks space in the to metric space that is going on here but you need to generate control commands for the Moses for the joints of the road how this approach is working if you can define the define points for example points you can now define or you can compute for example features in the environment such as optical to that point what's the define for example for this one for the gripper you can define for example what is it erection of the go point where you want the the end effector to actually go in the Workspace what it gives us for each of this point on for some of the proximity of these optical that things like that it's going to give you a TSI acceleration where the point would like to move Generations can also take into account the velocity so for example when you're moving a long and obstacle that it will not penalize that allow you to dismiss them off and on you have to find all of the combine them all together and computer explorations of them actually in the controls in the joint space of the play define things in the operation of space and then you pull them back into the joint space and you're just an example doesn't like open drawers Nathan the background and what we doing here for using dark to check the status of the Kenneth right in the wrong as you can see can't enjoy this pretty nice smooth it's still a local technique right it's still kind of just nipping from the current state a lot of his to acceleration so you still have to kind of have on top of that a global to check optimizer or past another example now together this is work that prospects indeed how you now let us know if you do higher level planning and you wanted robot to execute such a plan 1 significant using a Scottish behaviour trees stop serving the current state and then based on the what motorbike sound for the system should be in is logical dynamical system in this case let's assume the robot wants to alarm go back to these kind just can't turn off the system of environment lightsaber preconditions for certain actions are still fulfilled smoothly and quickly right can react to One of you could not argue can do all of this in end-to-end way but I think your bed it off the model basically says really that you can now really this a really long term task in motion planner right that is very generous continue 80s D1 why sample this is Garrity with letter k and Thomas coronavirus working on Task in motion planning which is reflected as higher level reasoning what's the word that you see the state is still happening simulation which means a Tassimo spending scenarios and then they evaluate half-past they can do the planning if it's in areas but often work in the real world yet because it perception isn't good having a system here that has pretty good perception we can actually start running this task and motion planning systems in the real world the task is in the focus of the on dealing with with uncertainty or not knowing certain things done this case the robot knows that this skip top drawer garden which one input in the other one the motorcycle he didn't use the ar and Peace the full perception system that we have so it reasonable ok so it's going to be the upper drawer and then detected in the upper drawer and then you can do all the with again the preconditions and all of them so you can specify these task at a very high level of abstraction and the play nothing is all the rest and then we can executed now actually in the real recurrently putting all the pieces together so then we can also do some longer-term a benchmarking of that system so you could imagine if you run this on whatever hundreds of tasks for many hours how often is it going to fail because I'm sure it's going to fail once in awhile what are these failure cases right so far the pillow cases that we've seen when the cameras are just off what is really is the Pulse estimation of this object is an accurate enough let me auto note to the next actually the idea is can we get away with without this explicit £60 estimation for manipulation tasks so so far again we assumed that BFG Morris of this object if you have a small and you can take where it is then you can also determine which grass do you want to use for example do you want to be able to do all of this without optic models right and I want to talk a bit about some of the more recent work with done in that direction anunnaki inside you for Ada networks if you wanna sort this problem with keep running and here we used flex simulator video sister play the 80s you have on model of an object and you can just a what's a brass the simulator in order to evaluate whether that's a good grasp you can do this all in parallel with many instances of this class and you can do this with many options and then you'll do some perturbations on it to figure out of the grass this actually also stable or robust to this and then what you get this for example maybe for this object to get some training data that tells you for that these are the 60 grass that actually work do this now for body if the specific piece of work was probably knighted TV with Arsenal on get in I want to know the code and everything is available for that into this is that network that looks at turn off an object and then samples cross that have a high chance of succeeding this is let us know we have an input image what's the point loud I'm a bit tricky at then if you do it in the in the simulated world is that we have occlusions and things like that right so we want to train a network to actually just observed appointment so this the backside of the up evensi so know what you do with this how can I go into the details of that we have a deep network that takes that as input and samples quiet same place actually might make a mistake trivial we have another network that kind of invalidated discriminator that takes as input creation of brass anti optic point loud so using pointnet plus + 4 SIA predict whether that's going to be a successful Class 10 so you can imagine this has been something that's cleaning up Supergrass in every pint we can also use aside for retirement where we say evaluate US into the Deep network propagate through the network to increase the probability of success with respect to the £60 of the river for this you can actually move the proper into a configuration that has a higher chance of succeeding can you just go into the grass simply here and again this is one of these autoencoders in this case a conditional variational autoencoder station we get a successful grasp and we encode this as the point load of the object along with the griphouse object so it's all centred around the object coordinate frame from the camera so we don't know turn coordinates frame of the object itself and then we want a coat this Aldi Leighton space add value along with the optic model again loaded into the 60 grasp so the idea is we want to be able is that if that lady's face was 60 then of course you could just copy that value over right but the idea is we can actually compress the 6th grass space into 2D space if we condition it on the object point find this send the decode of course again generate set across destruction the loss is then trust the difference between add the network generated vs the position you can you train this again be successful grass interns from out encode the 60s space into the 2D space you how we gonna use this I've tried that little space later on is case with Norway the encoder in the previous work with this particle filtering with will buy the decoder here we can throw away the encoder can do this weekend then take a point loud Elaine value in the networks going to give us a 60 + 4 actually have a more recent version that is working even better that is just using actually the an approach that's again formulation Outlast even better hi lovely voice just a more robust training for the same bedding space what is b look like then Arsenal GTA if you move to this 2019 space than this across sample condition on the lady value trajectory that space you getting kind of this wrapping around bedroom off the Ball be traded on different object different object types in space can be used on different objects and then for example in this case this would be the area which trying to turn the grass around the head the grass Centre clean up this process again we have another network that takes its input out of the with the point cloud representing the gripper then it's appointed + that's just trained to say whether that's going to be successful or not and be using the same simulator training data 4th essentially what age.is if you look at this would be an example physalis input to the network and then the score would be awkward 13 now propagated through the network picked to the post of the grippers so that you can increase the score is iteratively then the network generated rosset is a higher chance of success put this all together in his some results time of the icv submission we had brassica straight there's actually 88% on the very first attempt right which means of course if you're allowed to redraft then we're getting much closer to home this for objects that are not if it was only trained in simulation what's all these objects were not in the training data install the most recent version actually a new network which will single network that we tried on more than objects and more than 200 shape classes so it's a very general place on a single object can you train it on a specific object it doesn't generalize as well 1 extension to there this now timing works lot of scene can I come up at imagine you have this scene so far we assumed that we have only the single object planets can do it call last year where you can segment the scene even if the optical not known so unknown object instance segmentation someone tells you hey arctic number 3 does it matter which class Costcutter for weekend kind of part around the object from the scene single here now we can point cloud without the mustard and can feed that into the grass that generate good grass for it another net it's not going to look at this process and predict which of those are going to be condition or not will you might say you don't need this condition that because you just condition checks on your point loud but it turns out that that's actually much better at detecting conditions with included areas right because there was also trained on these full object models so I can learn to predict conditions even if you don't see in the point cloud significantly better than just using the Vault observation data I bet you can now of course she's trust that are not in collision with anything never much higher chance of succeeding evening this is the view from the Robber pick up in this case the the segmentation that the goal we do the segmentation of the scene which of these are mostly contributing to the condition grass to be one executed in this case it would pick the one that was contributing to this grasp planning on that object this other way just the same check on the scene again and I can say ok I can actually pick up invitation round so we can start saving any object model in there right no 3D model your reasoning or 60 posted we can really do start doing more complex actually manipulation tasks in the set and you can also mention that you can use a very similar approach placement replacement is get to the grass in problem right where you would say your hair 4.0 that are not serving and you can now start sampling replacements for an object on a surface and you can now then you can actually do the full can of transferring wear your reason about taking up sit up and putting it on another stable without using any 6th pause the song looking at right now doing this also with visual feedback in real time must be something the role of Simulation cost simulator is going to be really really big what it's right I think he's simulators on our days are becoming more more realistic boat on the physics side but also on the photo realistic rendering side and training robot in in simulation is going to be very powerful just to save time make it or somewhere safe and play Killing more efficient and in simulation you can provide much more the training data then if you would do a chat in the real world by you off and don't have excess to the internal state for example of the ox how many years of course how we training visual detection systems is trust in simulation where you can into a scene and you can randomise over the lighting conditions play put this off Indians also seen by you can put the opportunity to draw awesome things like that so that you can simulate more realistic view point 18 there's a lot of course like randomising the colours and things like that so that you're detect us become robot like to be able to say that we sort this problem now which mean we can train if the fury and simulation is going to work right away perfectly valid the real world not figured it out yet so that it's really working as well add some additional real world data I haven't seen any technique that really perfectly transfer from the simulator to the real-world even on the perceptions what can we do with the robot then collect a small amount of additional training data for for the detection networks and that influence the results doing it in similar cost control we can now what's a policies in simulated environment and they're again similar he said yes with domain randomisation when this case you might not know how long that rollbase or you might not know the mass and the size of that little cylinder that you want to put it in there when am I saw that and trade policies for this yes pharmacist not working on a Technika is it called Basin you is we do most of the training in the in the simulation go into the real world to a small number of rolling out in order to properly find the simulation get away with training something that works well in the real world but only require a minimum amount of real-world experience lipsy we see Direction going on in general right but what is work by Fabio Ramos was it last year that is really phrases as a bayesian estimation problem where you have a simulator that has so I can promise us you treat a simulator black box but it is certain promise and you cannot many rollouts and with that you learn invertible model that goes from statistics over this rollout musicians of unlikely with over the parameters and then you can update an overtime and refined yourself link distributions using the real boat I think stimulation is going to be really useful for protection for control but also just for testing your whole robot system so here's an example of in the real kitchen and one of them is action stimulated version of that kitchen is so we work content creation team that generated are pretty nice relation of the kitchen most of us seem which one is add my work might have made it even more similar that it's not trivial to see anymore so that kind of him said what is possible with these kind of simulators write also on the photo realistic rendering site the goal would be that you can actually run your robot control system again to sit against the simulator and as we know there might be some times you might have to do some debugging and things like that right so you can do this against the simulator if the simulation is realistic actually a major effort photo-realistic and especially also a physically realistic simulation of such an employee hunting creation for these kind of settings is also one of the open crash last light here mainly and I think we've seen especially also thanks to Deep learning on the perception side on individual components and what I thought about today is kind of this attempt of bringing this different pieces together so that we can solve can have lots of cake Proms right that go all the way from the low-level control to the high-level task planning and also perception he really useful to move motor what assistance because often by doing their you really learn about what's working what's not working for that you really need to integrate these different components and I think everybody's community we need to start thinking more about benchmarking environment in which we can really test or different albums against each other not only on that's a simple past like just picking up an object but much more the context of large-scale Indian video editorial of course videos the promising robotics died on the one hand side it's clearly of course GPU acceleration 440 planning and inference perception this robots will give more more perception and learn set the clear case but also I think the simulation if you don't have a lot of experience physics-based simulation photo-realistic rendering from gaming and computer graphics and I think that's gonna be really important moving all of this form open questions I think one it was really keeps on coming up send peace environment you saw today this one Argos mobile read explicit representations of the World ride explicit shape models explicit reasoning about things like £60 and everything and an alternative is more like the second part that are described which is more implicit right like the 6th letter Chennai Frost of an opera you might say the never gets no idea of what it's doing but obviously if you can generate good grass for different object it must implicit we have these kind of reasoning capabilities right at the question where on the long run should be be on on lightsaber should be in checked into the Deep networks in order to do the learning better and nothing and clearly we've seen some nice recent work on 3D box of pastry presentations that are not explicitly recovering 3D shape but just recover kind of a 3D shape feature space that is well suited for certain tasks like matching object into a scene and things like this is really one of the big questions be happy to discuss some more also the same of course on the control side we need to go and representations obviously that are not just good for a specific task labelling task but they have to be good in the conscious of control task and of course like the Chelsea is doing here it's also really interesting directions interstellar nation how close are we there are still many different components that I think I'll still missing there right how do we do seem to real better content creation is going to be a big problem if you want to train all robot in thousands of environments they have to look pretty good so who's gonna do this furniture manufacturers day off and everyone else if they would be willing to share with us all this once I think that would be really cool is a really big area still on the one hand side on the halfway itself Ride or something sightings building better touch dentist but also then how to take advantage of it because it could clearly be connected to the 15 benchmarking is gonna be thank you very much what's it this is the lab and of course many of the I've been working with against several of them here if I have one made it I can show you one thing that I didn't know it's like something like this just to show you that we're not only doing some kind of stuff but this is turn on the next time it's going to be a dick you build with multiple depth camera outside head tracking system play Real Time 16th of real time here but you can actually get the robot to do is Pakistan next to the robot in his kind of moving the hand that then real-time gets first you track it you cannot just copy the joints over but you have to cut because of shape of the Allegro hand some real matching to that but then you can she get this to call things Friday fully automated ways just not possible get and you can use the same moving forward maybe also for imitation learning and things like that stop standing there with done standing rock using the biotech sensors here pick up objects but also church to learn and models of his touchtennis right so that you can go from the this case ultra measurements to contact points and county corrections and six sete a what is the combination of both you really wanna use and I think it's also what we do write like five top of the visual perception gives me clearly first one it tells me where I have to put my move my hand in order to grasp it and it also gives me a pretty good prior on way I should grab it but then of course at some point clearly touch that takes over right so I can play then close my eyes and I can do all of these sorry the Oyster combination of those clearly the dark side you seem we were able to moving in this Direction but it's clearly not soft yet and again what are the promises lottery just on here I said have tap standing on the palm and everything was still kind of stuck to again I think a lot of the words that we're seeing is kind of isolated with about like learning a predictive model 55 v 8 my finger and things like their door slip detection I haven't really seen a lot of work so that you can really like yeah this is working for arbitrary object in-hand manipulation salsa that was actually exactly phonograph the white coffee we can set the scene and then we can reason about his other s and we can reason about in the way of being able to grasp it and I think actually what you could do if you could do now the full complex planning where you say exactly where do I need to pick which object in order to grab them this one and what might be the optimal ordering I think for many of these scenes simple heuristics pretty well and I'm pretty sure that I'm using very simple heuristics where you just say ok here the object can't drive it now this looks like it's in the way just driving to get it all the way and then you look again another one in the way you write so kind of simple your 600se take them over the other might get a sexy pretty Powerade the deal would be that issue rhystic you can train it simulation so that the deep networks for Salford County Court issues need really very complicated all the rearrangement landing I'm so behind are there any you would like today home setting where you don't want to put everywhere or example if you want to get an object out of a drawer then clearly it's good to have a camera on the robot so they can if you looking into the we also carry out cameras with play It's Tricky where to place them right if you have replace one I have something closer to lift human it's not a robot on a mobile page Nutella because you have this fixed you point in sometimes you want to be able kind of to lean over and look into something so it's very difficult actually to where the best places Kent things you can imagine also putting more like cameras outside because have any any perception right now on the I think the one I would love what's better like touch skin zombie Arms in every way around and people are looking at but it still ready for prime time yet streamsets what's the left 