I'm Abby I'm the head save this class and I'm also a PhD student in the sampling of me telling you about language models and recurrent neural network review what we're going to do say what's it going to introduce a new NLP task that language modelling the motivate us to learn about a new family of your own in your networks or Orange these are two of the most important ideas for you going to learn for the rest of the car bring some fairy core materials shuffle language model Singh is the Task of predicting what word comes next pizza tax the students opentherm out out a word which you think might become Minecraft quite hear them yeah he's alright I thought students might be opening students open their books seems likely students open their laptop exams someone that one just now becoming a photo poor performing language modelling right now I'm thinking about what what comes next you are being a language model formal definition of what a language model is terms of words x 1/2 XT is something that confuse the probability distribution of the next word x 2 + 1 comes up with the probability distribution the conditional probability of 1 XT plus man is given the words so far steaming at XT plus 1 can be any word w from a fixed vocabulary V claiming that there's a predefined list of words that work you can the language modeling as a type of plastic find number of paths assistant does this thinking about language model as well tomorrow is a system which assigns probability to a piece of text what if we have some piece of text x10xe capital T courtesy of this text according to the language model can be broken down definition you can say that the probability is equal to the product of all of these conditional probability the form inside the product is exactly what I was one of the guys everything to somebody equivalence predicting next word give you a cyst give the probability of you use language models every your texting on your phone in a writing a message then most likely have a smartphone it will be predicting what words you might be about meet you at the your phone might suggest happy min airport or cafe off to use language is when you search something on the internet for example Google and you start typing your query than Google tries to complete your query for you and that's what were the words to Mike that's what I'm going tomorrow how would you learn a language if I was answer question in the pre deep learning era which was really only 4 years ago the answer would be you would learn and G language about Ingram micro play wasn't in Grand language model is used to end definition and engram is a chunk of n consecutive 1 g or Una g is just all of the individual words in the sequence it would be the students open playground will be all of the sex of chunks of pairs of words the students students opened opened there and so on 400 g and 4 g anagram for predict what word comes next you're going to collect a bunch of statistics about how frequent different engrams are from some kind of training data and then you can use this statistics to predict what next words might be like ingrammicro firstly to make a simplifying assumption the next word x 2 + 1 only on the preceding and -1 what were distribution the conditional probability of x 2 + 1 given all of the words so far we just going to simplify that and say only depends on the last end definition of conditional probability is probability is just a ratio of two different products you've got the probability of a particular end got the probability of particular N18 to read because of all the super strength is going to give an example with words on them so that's the definition of the probability of the next one remains how to get all of these and G and -1 g where is we're going to get them by counting them in some large approximately of the number two appeared in our train example what's 924g language we have a piece of text that says as the Procter started the clock the students open their predictive wireless simplifying assumption is that the next word depends only on the last three words last 10 - discard all of the contact so far except for the last few words which is students open their says that the problems you've next one being some particular w in the vocabulary is equal to the number of times we saw students open their w / the number time to sort students open their in is that in or training corpus we saw the phrase student opens at 1000 resource student open a books 400 the property flexepin books is there similarly let's suppose that we saw a students open their exams one exams given seems open the question is does the order two words matter and the answer is yes the order of students open their does matter it's different to the I want to raise now is was it a good idea for us to discard the Proctor actual example that we has the example was to start at the clock the students open there books for exams is more likely given the act examples more likely because the property on the clock heavy implies that exam scenarios they're more likely to him I think it should be exams Christine here is there any training course opening something means it's more likely to be booked in exams because overall books are more common exams let me know that the contacts is the property at the clock then it should be listen here is a problem with the Sinn Fein too much context then we are not as good as predicted words and would be a cat one from within g hair done is the equation which going to call the sparsity problem is if the number on top of the numerator what if that counts for some particular word double student open the w never occurred let's play students open their common and it never appears in the data probability of an excellent Petri dishes will be 0 that's because it might be common but smart picture no problem that's past lifted never seen an event happened at the train find 0 probability to that event solution to this problem add a small delta a small number Delta to the counter for every word in the brake Abbott every possible word that come next has at least some small cover all of the other word for supposed to be bad take me to school smoothing because the idea is that you're going from a very sparse problem almost everywhere it goes from that to being a more smooth probability distribution what everything has at least the small problem which is possibly worse than the first one if the number in the letter is it example that would mean what if we never even saw the trigrams students open their in the training then we can't even calculate this probability distribution at all for any word Toolstation if you can't find students open their in the corpus then you should back off turn on the last two looking at times when you and see what words back off because in this failure case for when you have no data if you're 4 g likes model you're backing off to A Tribe another thing to note is that these past the problems get worse if you increase remind me tomorrow for example you might lodger contact said I can pay attention to words that happened long ago and I was going to make the best making and burgers suppose you say I want a 10 grand is that your going to be counting House Museum certain 9 G and 10 what's the time around so many of them the one you were interested in probably never occurred in practice music on 10th march that was too spicy from this phone and Grandma Thomas torch about what do you need to store in order to user and Grant discount number for all of the engrams that you observed in is there as you increase in than this number of engrams do you have to store and count increase size of your model and model gets bigger language models in practice exam a simple trigram language model over 1.7 million word corpus in a few seconds on your own who was ladies to do this was the same one he met and assignment one it's rotoscopers what's yourself you can follow that link at the bottom of the slide this is something of my laptop if you sex context of the by gram today what word is like I said at the top next most likely words our company Emirates you just looking at these probabilities of the sign top 2 most likely words had the exact same from what is this number is forever quite small integers meaning that we only saw today the company in today the bank for time listen example the sparsity problem because overall he is quite low counts we haven't seen that many have a very granular ignoring the problem I would say the overall he's a top suggestions look pretty can you use a language model to generate text Heywood music place anywhere asking language model what's like this probability distribution over the words you can sample for me that is select some words with you know the associated what's the word price your next words and then you just condition on the last two words which in this example it is now the price can you probability description anything continue this process again this one off you will get the piece of text so this is the actual text that I got when I ran this generation process with this program save Gold part shoe lasts and shooting just farted considered and rejected and I am after 6 European End Primary 76 account share that's good surprise it is good it's kind of surprisingly chromatic all you know it mostly menopause can you say that because only make me sick you shouldn't be surprised if they think remember this is a trigram what three words spelling and higher we need to consider more than 3 words the time if we want to model language you know increasing n makes the sparsity problem worse and how does a younger man with no nowhere you can the commas and other punctuation adjust another kind of word is it where or take set a language to my wife another hospital NLP with deep learning so you're probably thinking how do we build a neural Lanka I guess you forgot remember the language model is something that takes inputs which is a sequence of words x output probability distribution of what the next so when we think about what kind of criminals have been missing this car so far we've already met window based TV show how you could apply a window bass neuro model 2 Mario U take some kind of window around the word you care about which in this example play you get the wedding venues for those concatenate the layers and then you get your decision which is the Paris isn't apply a model like this to me his own example of a fixed window neural language can we have some kind of contacts which is as a process that and we trying to guess what words make a similar Sinn Fein stance set a fixed size window we have to discuss the context except for the windows is the door fixed window is a size what we'll do is similarly to the ner model percent these words with 1 factors is there is the one about and then we can linear and nonlinear functions to get some kind of hidden layer to another new layer and a soft MAX Function and now we have an output probably because we trying to predict what word comes now vector white hats will be a lengthy weather February and will contain the pro percent of that isn't for Charles suppose you got all of the words that set off different where is well then this language knowledge to tell us that's unlikely next words are books and should be last week we're just applying Windows mobile to a different so what's are some advantage is that there's no spark anagram lines model has a Spotify if you've never seen a particular 10 g in training then you can't assign any Pro you can take any example for g u into the yard in your maps and it will give you an hour good prediction but at least it was if you don't need to store all of yourself and grammar disadvantage by comparison you just have to store all of the word vectors for all the words problems with fixed window fix window is probably you're probably going to be losing some kind of useful contacts if you try to enlarge the window size then you will have to enlarge the size of your weight Factor so your weight Matrix width of w because your mum's playing it by you which is the calculation of your word you grow as you including Radio window which is more stuffing what's 1 x 2 and all of the words in the winter then x completely different this is your picture have your weight matrix w have your bedding for inviting so we have E1 E2 what's the play can you spell you can see the kind of forceps fast way by the way turn in this how do you separate to the weights I'm in the weight matrix in one section is not shared with learning a lot of similar functions everything there should be a lot of common process learn about how to process Jedward I'm saying that kind of been a when does a lot of come description so in conclusion I say that the biggest problem that we've got with this fixed size neuro model give me some kind of your architecture they can process any come from the fact that we had to make this something a something in that there was so this motivates us to introduce this new family of new architecture is called recurrent neural simplified diagram that shows you the most pictures of an orange stop again and input sequence X1 x2 in this sequence is of any opportunity like that you have a sequence of hidden city 180 days in the previous model we have a sequence of in States and we have as many of them as we have agents 8ht is computed based on the previous sentence input on that step play called hidden call state boots mutating overtime devil version we often call these timesteps right Sonya steps that go left to right weight matrix w is applied on every time step next eni LinkedIn how to have different weights on every exact same transformation you can also have some output from the Orange musically outputs on you no because you don't have to be them or you can put them on just some steps and not it depends on what simple diagram of an iron in here I'm going to be a bit naughty settle so here's how you would apply in Ireland to do again let's suppose we have some kind of text so far 4 words resume that it could be it could be kind of long advice and you supposed to look up the word complete the first hidden state H1 could you set based on a previous in state and the current the current input that's the one with the question is where we get call Adina hidden state 80 the initial States and it can either be something that you learn like it's a parameter of the network and you finish line sing something like maybe Siri the formula used to compute the new hidden state based on the previous one and also the current inputs is original play mini transformation on the previous in state and on the current in turn on the light so then you can pick the next 7 States and you can keep on rollin the network like rolling because you're going to compare find me if you remember which language modelling to predict which word should come next after the studio it's over here we can use the current state Age 4 do a linear layer and put it through softmax function and then we get out creation hopefully we'll get some kind of for what it's going to be the number of worlds number 14 in the settings the number for the States is a number output as input as with the language model to be used the output is the input of the next step and the answer is yes embeddings are you learning me in bed a choice you could have the embeddings be for example cruising in buildings that you download and use those and they're frozen or maybe you could download them but then you could find to them that is allow them to be changed as parameters of the network or you could initialise them too random values in London from scratch play some music of that only you say we use the Matrix to the update we and why does one she suddenly learn both we and W exciting wh0 but yeah they're both mostly they replied also question about bankrupt but we're gonna covered turn off now so advantages and disadvantages of this our language advantages that we can see listen to the fix window one advantages that can process any length of him advantage is that the conversation preceptee can in theory use information from many steps example which was has the Proctor started the clock the students open their doctor and maybe clock about pretty important hints for what might be coming up next at least in theory the hidden States at the end can is the stages of the model size doesn't increase for Long Eaton the size of the model is actually fixed it's just why and we and also the bison also the get bigger if you want to apply it to more longer inputs because you just applied the same way advantage is that you have the same weights applied on every time before about how the fixed sized window euro model it was less efficient because it was applying different weights of the weight matrix to the difference words in the wind change about this is that it applying the exact same transformation to each of the end lines good way to process one input that is applied to every input disadvantage is that recurrent computation is pretty slow you have to compute the hidden state based on the previous agency you can't compute all the instance in parallel you have to repeat actually if you're trying to computadora down over a pretty long sequence of Interest this means that the R9 can be pretty slow is that it turns out in practice it's quite difficult to access information from any steps should be able to remember about the proper and the clock can use that to predict exams the least the ones are presented in this lecture and not as good as that learn more about both of you disadvantages later and course am I going to learn something about how you can try to any questions at this point answer the question is why should you assume that the why the same what is a deliberate decision in the design of an arm is by definition a network where you apply the exact same weight spine every step play the question why do you assume maybe should be why is that a good idea little bit about why it's a good idea and disadvantages I suppose why you want to do that and saying you mean that transmit transfer probability for the open-air the oven they're so approximation approximation transfer the question is saying that given that these words the students open that are all different and happening in different contexts than why should we be applying the same transformation each time so that so that's a good question I think the idea is that you are learning a general functions not just you know how to deal with Stu this one contacts for trying to learn a general function of how you should deal with a word given the word so far you're trying to learn a general representation of language in contact so far which is indeed and very difficult what's the mentioned something about naproxen is the owner of the hidden states of Vector it's not a single numbers are factors of thanks for the new 500 something so they have quite a large capacity to hold lots of information about different where is that you can play Fun information in different contacts in different parts of the states but it is indeed an approximation and there is some kind of limit to how much information play profit a single name the light fuse for training effect is given that you can have any linked inputs what length is the input during training I suppose in practice you choose how long the inputs are training either based on what your data is or may be based on your efficiency concerns and maybe you make it especially shorter by chopping it up the question Stargate 2 lb of length and that's one of the good things in the advantages list is at the model size doesn't increase for longer input because we just unroll Dr nana playing the same weight again again for as long as we like there's no need to have more weights just because you have a longer in Dimension of The Dimension of the lowercase example assume that those are just my back is like the ones that you are using assignment 1 sample word back and you just download them use them or maybe you'll under from scratch in which case you decide at the beginning of training how big you want what are in a language model is and we've learnt how you would run forwards but the question remains how would you train an online always mc19 answer starts where you going to get the big cup of tea sequence of words x12xe capital T read the sequence of words into the R&B trouble you can choose the alpha distribution y had tea for every set shows on the previous slide only shows doing on the last step but the ideas they would actually computers on every step it means you actually predicting the probability of the next word on every cell ok someone who done that then you can define the loss function and they should be fine to you by now this is the cross entropy between our predicted probability distribution whitehats tea and the true distribution which is why sorry just yt which is a one hot vector representing the true next words which is x for this cross entropy between those two factors can be written also as a negative log probability me if you average this question to be lost across every step every tea in the Copa stands up to then this gives you your overall last the entire training set even ok with a picture supposedly corpus is the students open their exams etc it goes on for a long time then what we doing is we be running rnn over this text and they're not every step we predicting the probability distribution white hats and then from Peter blows you can calculate what your losses which is the JT on the first step the last would be the negative log probability the next words which is in this example YouTube there is a negative log probability of the next one you can add them all up and average them and then this gives your final single dose ingredients across the entire corpus all of those words x120 cup of tea is too expensive because you're corpus is probably really big as a student Australia in practice would you actually recorded envirogard your sequences something like a sentence or a document some shorter unit x another thing you will do is if your members to Casa gradient descent allows you to compute gradients for small chunks of data rather than the whole corpus of time secure training in Langar tomorrow what you actually likely to be doing is Computing the last for a sentence for that actually about your sentences and then you compute the gradients of the facts about your sentences update your weights and don't worry they won't be as much as there was last week but there is an interesting question right so the characteristic think about our nan's is if they apply the same weight matrix repeated what's the derivative of a loss function let's say on settee what's the derivative of that loss with respect to the repeated way matrix wh riveting the gradient with respect to the repeated of the gradients with respect to each time it is equation sets on the right the notation with the best goal line in the eye is saying the derivative of the last with respect why when it appears on the ice step so why is that why this is true I'm going to remind you for multivariable Change this is a screenshot from a Khan Academy Oscar on the multivariable chain rule and fibrosis check it out if you want some more cos it's very easy to understand as is given a function f which depends on X and Y which are both of themselves functions of some variable t the derivative of chain rule across X and Y separately and an atom turn multi variable Arsenal with trying to take ab derivative of the last JT with the Spectre weight matrix we is this kind of diagram has a relationship with all of these individual Appearances of why but it's a simple relationship is just a quality and then each of those Appearances only wait affect the loss in different ways reapply the multi variable is the derivative of the loss with respect to we of those chamber of things the right is just one because it's an equality relation that gives us the equation that I wrote on the previous slide sketch for why the derivative of the last with respect to our recurrent matrix is this suppose you believe me on that is how you compute the gradient with respect to the current is well how do we actually calculate this impact what's a is you going to calculate this some by doing backprop backwards going to write to LA movie orange and you going to accumulate this summer as you is you shouldn't those things separate received them by accumulating 1 can be computed in form in terms of the previous one computing each of these gradients for the previous one is called backpropagation 22 this sounds way Moorside 5 and is it sounds like it's time traveler something but it's actually pretty simple it's just you give to applying the back prager them to a recurrent network how you break hard to marry your ex back after you back to the first match so you can break into question is surely how you decide to break up your batches affects how you learn right because if you choose one Saturday to be your batchwrite then you'll make sure update based on that play the next one based so if you decided to put different data in the batch then you would have made if that's true and that is why stochastic gradient descent is only an approximation of true gradient gradient that you computer respect of one batch is just an approximation of the true gradient with respect to the the last over the hole credence approximation and how you choose the badger potato canasta and that's why for example shopping all day so is a good idea and shuffling at different epoch is a good idea but the courier best you die is that it should be a good enough approximation that over many steps you will minimise your ass is the question as you compute forward prop do you start competing backprop before you even like what's the last is that the question because you need to know what the loss is in order to compute the derivative of the last need to go to the ends of your theme somebody that there is only one last which you get at the end of several steps then you need to get to the end you could compute the derivative of two kind of Jason things everyone yes so when you keep forward probably something at the hang on to your love turn off now play that was a maths have you bit but now going on to text generation with someone else about earlier just as we use the n-gram language model to generate text you can also use an rnn language model to generate Exe via the same repeated sampling technique so here's a picture of pallet wood with your initial we happy there is a promise of the model or initialise it 2-0 or something like that we have the first word Mayan suppose I the inputs and the insurance you can get off first stage 1 we can compute the probability distribution why have one of what's coming out music distribution to sample some words so that's supposed to be sampled word favour what is that we use the output of words as the input on the next step favourite into the second set of the car and then we get a new hidden States and again get a new position and from that we can sample any word continue doing the process again again and in this way we can generate generating the text my favourite season is spring and we can keep going for as long as you like let's have some fun with us you can generate trainee on in Lancashire on any kind of can use it to generate text in that Style is the common Hall kind of genre of Internet humour free sample turn on language mobile trains on a bomb text the Dr nandi just Mobile generous dates for step up to the cost of a new challenges of the American people that will share the fact that We Created the problem were attacked and so that they have to say that all the task of the final Days of War that I will not be able to get this done so if we and especially think about what did that text look like that we got from the n-gram language model the one about the the price of gold this is kind of recognisably better than that it seems more fluence overall games contacts in that kind of makes sense for longer stretches at a time sound tunnel oh that's pretty good but you can see that it's still pretty and overall like it was quite difficult to read it because I didn't really make sense write out of it was cafe play some depressing get from you thing on insta general text but still very fast model that was trained on the Harry Potter Harry shouted panic London sncf close by Cedric carrying the last bit of treacle charms from Harry slaughter and also him the common dream poster The Shining Rob from when the spider hadn't started again I'd say that this is fairly fluent it sounds totally like Harry Potter books and impressed by how much it does sound like in the voice of the Harry Potter book character attributes I say that Harry the character does often panic in the books so that isn't right bad things all that we have for example pretty long run on sentence in the second paragraph that things that really make sense like I don't know what to treacle Chalmers it sounds delicious but I don't think it this is pretty nonsensical thanks model that was trained on recipes he deserves the title is chocolate ranch barbecue Parmesan cheese coconut milk eggs and the recipe says placed each pasta over layers of lumps shape mixture into the moderate oven and simmer until firm embodies fresh mustard orange and cheese talk together the dough in a large skillet add the ingredients and stir in the chocolate and pepper so one think I think it's even more clear here in the recipes examples in the pros example is the inability to remember what was happening over all right because the rest of you could say it's pretty chair the title of what you're trying to make which in this case is chocolate ranch BBQ and he's actually unit make that thing by the end of your seat remember what with ingredients beginning into you if you make something and put it in the oven you need to take it out so clearly it's not really remembering what's happening I've roller will try and do it seems to be just generating recipe sentences and putting them in a random order I mean we can see that it's very fluence is grammatically right it kind of sounds like a recipe with a promise that's just not at school bye free sample shape mixture into the motor oven is grammatical but it doesn't make any sense past example model this trains on paint colour names example of a character Level language level because it's predicting what character comes text not working play table to come up with new words please note is that this language when I was trying to be conditioned on some kind of in what is the colour of South I think represented by the three numbers that's probably order been on for the colour my funny my favourite one is stinky Bean which is in the bottom right Joseph I think he's do sounds kind of like quite bizarre light up Blaster speaking to are you going on more about carrots and other language models in the future Alexa and you also going to learn more about how to conditional language model based on some kind of input such as the colour code it's a pretty funny but I do want to say a war in a lot of these kinds of articles online often with headlines like we forced a bot to watch you know 1000 hours of sci-fi movies and it wrote something like that so my advice to you have to take these with a big pinch of salt because often the examples that people put online were hand selected by humans to be the funniest today we definitely hand-selected by humans as the funniest examples that would be our Nan came up with and in some places they might even have been edited by a Hulme when you look at it play Harry Spotify closing quote expectorant set off any qualified keeps putting my work experience to like in Greece speak truth was we know sister in the Harry Potter example there is an open quotes and some clothes quotes and it looks like the model didn't scare upright all of these open close and close do we expect the model to put a higher probability on closing the credit given that inside so I say definitely yes and that's mostly the explanation for why this one really interesting work in trying to look inside it and sites of language models to see whether it's tracking things like are we inside an open quote or a postcode limited evidence to show that maybe there are certain you are on on your hands inside the inside which are tracking things like are we currently inside a quote on play what do you think the authority of the probability about putting a clothes Crouch in I suppose because you don't want to Infinite I wouldn't be surprised if I didn't like I wouldn't be surprised if maybe some other were strained language models just open quite another the question is what are the dimensions of the WHG still going back to the answer you asked me about why we or something else will be if you say that the hidden size has size end then why bnym suppose that the embedding is have size d then we will be deep fry questions about generation orange for example so yeah I'd say it probably is preschool maybe especially if you're interested in making sure that certain bad things that happened you might apply somatic rules like early I mean if there's a thing called being search we're going to learn about Alexa which eventually doesn't just continue it explores many different options for you could generate and you can apply some kind of rules on that way if you have lots of different things to choose from then you can and maybe get rid of some options if you don't like them because they break something play music so generating from language model you can't just use generation as your evaluation metric so you do need some kind of measurable match the standard evaluation metric for language models is called to find is the inverse probability of the corpus according to the language you can see that's what this formula is saying for every words XD locatie in the cork sing the probability of that word given everything that can so far but it's in vs whatever that play when normalising this big the number of words which is caps why we doing is because we didn't do that text you just get smaller and smaller as your corpus got bigger to show this is equal to the exponential of the cross entropy loss cross entropy loss is the training objective that we using to train the language rearranging things little bit you can see that the text he is actually the exponential of the Crescent we're training language model to minimise you song my perfect as well remember that the lower plexity is better because perplexity is the inverse probability of the corpus so if you want your language model to assign high probability to the corpus I want to get low back so have been pretty successful in recent years in improving backseat so this is a result table from a recent Facebook what's on in Lancashire understand all of the details of the table but what I'm telling you is then on the top row we have an n-gram language and then in the subscan grows we have some increasingly complex and large orange is R&D have been really great for making more effective language models in the last you might be thinking why should I care about language human reasons why language modelling first one is that language modelling is a benchmark task that helps us measure our progress on understanding and language modelling as a pretty general language understanding task right because predicting what word comes next to given any any kind of generic texts that's quite a difficult in general good at language modelling you have to understand a lot of things right you have to understand grammar you have done stand stand logic AND reasoning and you have to understand something about the real world what things would you be able to show me care about is a Benjamin task is because if you're able to build a model which is a better language model than the ones that came for what's it made some kind of progress on at least some of those subcomponents over natural language on what time does Currys in why you might care about language modelling is the component of many many LPS dates of generations eating the progressive tax predictive typing that's the example to show that the beginning of the lecture with typing on your phone or searching on Google this is also very useful for people who have movement disabilities because there any systems that help people communicate using 200 minutes sample is speech recognition ignition you have some kind of audio recording of a person saying something in often it's noisy in hard to make out what they're saying and you need to figure out what words they say sample we have to estimate the probability of diff what is the second set handwriting recognition is an example where there's a lot of noise and you have to figure out what the best intended to say spelling and grammar correction is yet another example where is all about trying to figure out what supplements and that means you have to understand how likely it is that they were saying different things can you sing an interesting application is authorship identification is that you have a piece of text and your friends figure out who likely bro authors and you have text written by those different Wars sample train a separate language model on YouTube the different authors tech because you remember a language model can tell you the probability of a given piece of time ask all the different language the text on the questionnaire certain Waters language model said it's likely than that mean the question is more likely to be written by that examples of machine translation this is a huge application of because it's all about Generation X police station is a task we need to generate and text giving sympatec I like as well not all dialogue agents necessarily or you can build a dialogue agent that generates a text using and search for that images progression so the question was personas examples such as turn on the image captioning the input is audio or image or something that is not text right you can't represent it in the way to be in those examples you will have some way of representing the input somewhere of encoding the audio on image for whatever the reason I caught up now in terms of language models is that that's the inputs we use the language model to get the output to language model generates the output in the way that we saw learn more about those conditional language here's a recap if I've lost you somewhere in Alexa or you got tired now is a great time to jump back in because things going to get a little bit more model is a system that predicts next word your network is a new family new to us a family of your neck the same weights on every step could you some kind of output on each step or some of the service confuse a recurring on network is not the same thing as a language play that in Ireland the great way to build a banquette you can use our names for a lot of other different things certain not mine examples of an using orange to do a tagging task the typing tasks are part of speech tagging and named entity recognition Harry Potter speech ask we have some kind of input text such as the startled cat knocks over the bar is to label or tag each word with its Castletown and knocked it over using orange do this task in in the weight of the pictures which is the URC detecting cioran in OCR new probably a distribution over what tag is this you can take it that way recognition that's all about tagging YouTube the words with what names news Ireland term for sentence cast application is general time to mean any kind of task where you want to take a sentence or other piece of text and then he wants classified into one of several that is sentence ask questions when you have some fun in protect such as like say overall I enjoyed the movie lots and then you're trying to classified as being positive or negative or neutral I'm way you might using rnn to tackle this I might encodes the text using the iron in say what you want is some kind of sentence in coding can output your name and abuse you have a single vector to represent the sentence rather than all the separate fact how do you do how do you get the sentence in coding from the Arnhem can you can finally you can say as your Centre the reason why you might think it's a good idea is because for example in the rnn we regard the final hidden States as this is a thing used to predict what's coming next right so we're assuming that the final hidden state contains information about all of the I suppose that this is a good sentence and codeine and we could use that to protect you know what what sentiment is the sun the usually a better ways to this usually more effective way is to do something like maybe taking element wise Max or an element wise mean of all the Southern states to get your sentences listen to work better than just using the final all the more about things you can do nothing you can use onions for is as a general-purpose encoder module so here's example B question answering but really this idea of our own ends as a general-purpose encoder module is very common and use it in a lot of difference the witches questions where is the Task is you got some kind of contacts which in this situation is the Wikipedia article on Beethoven the question which is asking what nationality was based is it actually taken from the squad challenge which is the subject of the default by up if you choose to do to do the default final project going to be Building Systems that's all he might use an orange the question what nationality was Beethoven you might use those hidden states that you get from this land of the question representation of the about what might happen where is that you have both the context and the question going to be fed some way and maybe you're using our and my contacts as well lots more neural architecture in order to get your answer and which is German morning is acting as an encoder for the question hidden states that you get for running VR and anova question represent the quest play the encoder is part of a larger neurosis the hidden safe themselves they are interested in because they contain into taking the element wise Max on the invite was shown in the previous slides to get a single vector for the question but often you don't do that often you'll do something else which uses the insects point here is it Orange or quite powerful as a way to example so going back to Aaron and language models again they can be used to generate text and there are lots of different occasions so for example speech recognition what's your input which is the audio and as a student asked earlier this will be represented in some way maybe you're doing your landholding of that foreign and language models to generate the output which in this case is going to be a transcription of what the audio recording say conditioning I'm going to talk more about how this works in the late Alexa you have some way of conditioning you are and language model on the input starter generator do you know Chris what are you using Rebels racing standards the new WRC valve so this is an example of a conditional language set an alarm for tomorrow because we have the language components but crucially reconditioning in on some kind of funny samples like with the Harry Potter text when we were just generating text basically unconditionally you know we trained on the training data and then we discuss it with some kind of random season that generates unconditionally this is called a conditional language tomorrow because there's some kind of input that we need to condition on translation is an example also as a condition language model and we're going to see that much more detail in the neck any more questions question about RAM the weather is fine sketcher with other saying have previously anything through the question is do you have a combine or an end with all the types of October so I think the answer is yes you might be no have what are the types of architectures to produce factors of the going to be the input URL or you might use the alfa-2b R&B down into a different type of you on that one questions I have no time terminology call reading papers you might find often this phrase vanilla Orange the phrase minotaur in in that usually means the organisms that are described in this lecture vanilla rnn directory of the more complex kinds of Iron and Fire are you and Alexa spell president enough I'm going to know about next week if you can actually get multiplayer on and which is when you stack multiple rnns on top of each other are you going on about those but we hope that by the time you reach the end of this give me to research paper and see a phrase like lstm with residual connections and self tension and you know exactly what that with all of the thank you that's open today next Millennium problems and fancy arnas 