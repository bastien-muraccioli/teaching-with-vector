hello everyone connect to the second class 34000 at the end of last time I was just showing you a little from the Python notebook of things that you can do with word vectors but I kind of ran out of timer but also I just found a couple of more minutes send Python notebook app on the course page so I'm electric one you can find a copy of it and you can download it so I both stuck up just an HTML version of it ended zip file I dated email files only good to look at you can't do anything with her so you want to if you want to play with it by yourself and down put out of there actors talk about a bit more today and so there were these sort of basic results inspector space ice sleeve similar words from that there was this idea that will spend some more time on today which was the space is not only a similarities together things have similar meaning actually captured considerably deeper and more profound way which is the say that there are actually directions in the space certain meaning so that if you're pointing and One Direction it means this is more so the case the different direction in the meaning space that might be this is the capital of this country or all sorts of different meanings could be encoded in the space way of testing that is to use these analogy quickly show this at the end but just to make sure if you haven't got it since it's sort of everything right to the idea is pair of words what we going to do with we going to stay with his Invicta for King in the space is a Victor fought man in the space We Gonna Do Is we going to subtract as in just good old Victor subtraction that you hopefully won't in your hand when you're out of a class we going to subtract them then Victor from the king Victor and the idea we having out head then is if we do that what will happen left with the meaning of kingship without the men so then there's also a director Victor for woman so we can add the woman vector to that resolving vector and then we could say well in the weekend at some point in the vector space play robots the children find the here and it's going to print out the closest word and as we saw last time lo and behold the answer it I'm saying you go first King and I do it well like men ok yeah that's right sorry ok yeah because it should be man is the king of woman is the something sorry yet I was getting my order of components wrong ok and you know as I was so I guess I was showing some examples last time with nationality word surprising to shocking this actually work Lions of things that you can get meaning in the space so I can games of analogies of Lucifer so I can say Australia is to be a as Frances to I think wine want to take a champagne really good answer with that what synthetic facts so I can say total for the tallest as long as to longest and get set what is the fantasticas bed is too terrible that seems to get out of this some kind of notional make more extreme Direction and get this Direction now the Clinton as Reagan is to not like the answer give us to this one as Obama is to his Regan stand mixer one thing you might notice at this point in this is something else you want to come back to at the end well there's this probably could Clinton's ambiguous write this bill when it's Hillary I said as a few years old so this date it was done in 2014 so in solid often have trump really in as a politician but you know it would have various we both Clinton make sense of probably for sorted for 2014 days and that Bill Clinton dominated so I think what we getting out of this printing and external sort of similar people and Dane b&p the VIII had a thinking primarily of Bill Clinton is this sort of brings up something that I'll come back to write at the end of its side of looks like we've got a sort of a problem here because we just have this string wycherley Clinton and that any possible king of the string Clinton and so minimal have Bill Clinton and Hillary Clinton Ventura maybe you have some friends with cold Clinton as well riding that all mixed together in this Clinton and select seems kind of problematic and that sort of being issues has been discussed some for these words come back to that if you can give a set of words and safe which is the odd one out Middle School so you can do that and it decides that cereal is the alpine out of that's it what I think I just show you is so sorry be nice to look at these words I've drawn them the slide pictures so this is saying to put together picea principal components analysis do that and then I can say give it a set of words hopefully here's my scatterplot and it works pretty well right I've got the wine champagne by European in the coffee and tea countries is the schools animals are down here they're so yeah this really does work with this too play it basically shows you similarity now 1031a hold onto your wallet with discuss before since you were taking something that was 100 dimensional and we're just doing this 2D projection does capturing some of the major geometry of the space the meaning of losing a huge amount of the information so when things end up close together really close together in the original space so they might just been with that 3D projection because there are other patterns that one more dominant and which was the first two principal components what are over trust these things and something if she might think about is how the other ways that I might be able to represent the distances on the way that was more accurate this is very simple to do I'm just going up a PCA to reduce the dimensionality of the Matrix and then I'm transforming with that these word vectors and printing easy to do easy for me to do but if someone's got some clever python capeside like one is after class there be some default way in which you could double label unable to find was I'm just sort of plotting the text when I'm upsetting a little networks clinic rapidly crusader to collide with each other so the better if there's a better ways to point live.ly that one there is that a python for try python computing something today we're going to keep on talking about things you can do at work a little bit at the end about word census so in more detail more about have a sort of a very brief excursion all optimisation but then I wouldn't want to explain a bit more of the space of what people have done and can do with dense word representation is something about can't based approaches to capturing meaning and how they work I'm going to talk for a bit about us a different model of word vectors which was the glove model that mine I'm Jeffrey Pennington on a couple of years ago and talk someone out of evaluation really quite dominant Simon a lot of what we do a natural language processing is Howie valueweight things and how much do we trust our evaluation about I'm word Centre doll hair which is the by the end of the class Alexa sort of understand enough of the way of the way and that you could word vectors such as the ones that are in the sewers send them and where they're coming from and roughly how they work really want to minimise work this class you could think I know everything I need to know after the first week and I'm going to do a final project on word vectors and I'll be ok and you know you could actually do that I mentioned during the class a couple of recent pieces of work on Words actors on the other hand doing things that word vectors as a fairly mind out area so you probably better off and also listening to some of the later parts of the class do you remember we have this idea words of XO it was it rid of updating algorithm that learnt and these Victor representations of words in some sense capture their meaning and away at work was weak and a move position by position to a cork time we had a centre word into and it's trying to predict the words around it by having a probability distribution over words all around and that probability distribution is defined simply in terms of the dot product of the word vectors by the soft Max puncture I want to do is change those vectors in a way that this guy 40 predictions that gives as high probability as possible towards that you tend to see in the context drill red in the little bit more you know what we actually have is we matrices percent of words we have a matrix where each word now vocabulary we have a Victor this is probably going to point as any to say that it turns out that all the major deep learning packages torch etc for their word Victor's the word vectors are represented as rows classes that might not be what you would expect you have expected the other way around but they all put them in rows so they can have Rose 40 6 words in a 5 dimensional vector this outside we also the second vector free to word which is this representation in context particular centre word here word for you know when we doing our complications with taking a dot product between V4 and you and that's then giving lack of products cause after that with running soft mixes on each other's numbers doing it element wise and it's been giving us a probability distribution of a words in the context and the sort of things to notice there last time to make sure you know the Stead you know 1 probability distribution rights on terms of what words we predict we predicting exactly the same probability distribution every position we sort of saying the most likely word one to the left how I most likely word 2 letters health three letters house to be house to braid so it's sort of new sort of finance the predictions just an overall kind what's the distribution of words for a likely to occur in my contacts so always asking for is a model that gives me high probability estimates to all words that occur in the context of this word relatively often O2 at the neck of white sauce surprising when you've got such a simplistic being that it seems like the end of the day it can end up catching so much about the meanings of words and aspects of the meanings of words like and the examples I've just showing you in the iPad on Notebook going to say another thing I was going to say was the other thing that might occur due from this there was like that and and and all the time so that means you word must have like that an oven and get the probabilities right first answer to there is yep that's true all word vectors very strong probability component that reflects that and I mean one of the things work as discussed so on the there are two papers from Sanjeev Arora group in Princeton and one of those papers should have discussed probability high frequency of bag and your crude way of singers high frequency effect is that normally the first component in your word vectors actual frequency effect and if he doesn't Pop it off you can make your somatic similarities better there are other things that we do to sort of deal with high frequency you get this lovely spaces some of Laura Mark last time my remark anyway is what is two-dimensional picture eating like seemingly misleading because in these pic 2 pictures will have these effects Samsung is close to test to be over here and then have to be far away from words that are over here but as you might sort of also want to have the effect that Nokia is close to Finland for a different reason and you can't do that and two dimensional vector spaces don't know one of the does the properties high-dimensional vector spaces of very an insured and one of the ways that their intuitive is a high-dimensional vector space play Close To lots of other words directions so we sort of started to talk about learning these words vector take about a 5-minute and detour into optimisation now this isn't really an optimisation class if you want to learn a lot about optimisation where you can learn more about optimisation 9 and if you do something like Stephen Boyd optimisation class you can learn a lot of optimisation but this is so really baby optimization but just to make sure it runs on the same page here at the end what we did over there where I apologise that my writing was too small but that will give you the chance to win do a homework 2 and you have to write that out to work it out for yourself more in the process so what we had was the cost function that we wanted to minimise and so what we did was we did a bit of calculus 2 gradient of the cost function with respect word vectors with for a variable Stata and then what we want to do is say well if we take a small step in the direction of the negative of the gradient that will be taking a stand on doing that and sort of hymn to the minimum of our space now high multidimensional space a nice smooth Curve like this that might be a horrible and non-convex curve but that's just the idea we've got the Old parameters up the gradient of the objective function using as old parameters play that buy a small alpha which is our steps aizawa running late because we only one and move a little bit each time because if back here if we sort of said downhill is this way and said that way you could kind of completely over should so we only want to go little bit each time so we normally have a small learning rate alpha and 3 subtract a small multiple of the gradient Amazon we get our new premises sorry effectively being worked out component wise as a shown below that we just doing Riverdale that our hope is that will lead us reg we walked down the surface actually did the unbelievably bad but a lot of work on clever optimization but the most basic thing definitely need to know objective function here jfsr was a function of our entire corpus work well the first thing you want to do is you know collect a few billion word language and then say go and build a word to make model for me and so if you have to evaluate enter words and maybe then 10 billion context words if you have a window size of 5 and used to have to do these I'm soft Max calculation what's your gradient having your computer computer for quite a long time step in the gradient and so things are going to go so so slowly so I'm no one does Dad in deep learning system people everyone does is use the Castor gradient to send and in stochastic gradient to stay and wee sample in the simplest case just for this one window work out an estimate of the gradient and we use it as a parameter update an amazingly amazingly noisy estimate of the gradient sort of Doesn't Matter too much because it soon as we've done it we going to choose a different centre word and do it again and again so the gradually we sort of approach what we would have gotten if with sort of looked at all of the centre word take steps as we go we get to the minimum of the function magnitude more quickly this shows the simplest case where we just sampling one window in practice that's not what we normally do we normally sample bunch you know order approximately 32 so if we have a sample that bigger that's been referred to as a mini-batch and we calculator gradient estimate from the minibar two advantages one advantage is that you kind of get listens noisy estimates of the gradient because you've kind of averaged over a bunch of examples rather than just using one advantage which is the one way we really care is if we want her computations to go fast when we using a GPU parallelization of doing same operation a whole bunch of thyme name a lot by using a minibus something like that you don't have to but you know I turn down the details of the guts of the hardware that you know these NVIDIA GPUs whatever they have them sides and they're in powers of 2 so you'll get better speed-ups if you was better as like 32 just the same at 42 is still your favourite number from High School and you're gonna use that as the size of your mini-batch one other interesting thing which actually has some optimization details in it turns out if you think of these stochastic gradients with word vectors very different to some other deep learning problems like vision deep learning go window or even the side of a reasonably sized minibar the Bose mini batch only have lovely speaking a handful of words in at rights of mini battery size 32 and a Windows size of 10 play there are only about 150 different words in it but yet we're building this model over a vocabulary of quarter of a million words or something like that so just about all the elements in this vector as you don't really have this very spot parameter update and so suggest that we actually probably want to sort of only update the word victors that appear whether you can achieve that right the dumb way to do it if we just have this Matrix nearly all Zeros and you say add those two matrices together and there you go and then the question is have a sparse matrix update which only up to play The Sound of the Matrix that contain the words that you've entered into things much faster somebody even clever like doing distributed computation over multiple and computers and sharing your parameters within deathly still only wanna update the word vectors that you've actually been getting a premature estimate for so it's awesome details there then I'm going to skip past them more details a couple of people are staff towards yeah why are these two words vectors the song outside 1 my answer that is it makes that I showed you easy right so that working out partial derivatives for the centre word showed you it's easy and but one set of word vectors world in the same word the centre of word will be one of the choices word when you're working out that soft next for the context word and then you'll get there squared two references that same word and that makes your mouth more difficult it's sort of just a practical thing in the end I mean it's sort of doesn't make very much difference because if you sort of think about it since you're going along through all the positions you know what was the centre word at one point is immediately afterwards a context word of what used to be a context word which is now the centre words of doing the same computation product of symmetric X-ray 4 again so they get pretty similar vector representations oh it seems like in general and get the best results by everything what comes out for your two vectors and you end up with just one victor per word more substantive what's that word tyvek paper you will discover that this on a motor word to fake that they define sort of a family of word the Vic model two main parts of that family choice between the continuous bag-of-words model and the skip g model and what I present it was the skip g model so in the skip g model you've got one centre word and you're trying to predict all the words and context one of the time continuous bag of words model it's the opposite you've got all the outside words and you're trying to use all of them independently like an IP based model to predict the centre word and then the second one is I presented learning this with the method of this using the so-called naive socks Mac so therefore when we run into work things out we were sort of saying ok we won't probably the estimates for the context word can a some over the whole vocabulary and will come up with these probability estimate practice to be a sort of a bad idea because that would also make things mega slow 2 you will get to implement and much more practical way of doing this wish they present in the word to get papers right so the problem is if we using this equation that do the calculator denominator here we doing the entire vocabulary so if you have a vocabulary quarter million words with sort of do a quarter of a million products and exponentials and adding them all to work out that denominator and that sort of thing play bad idea if you want things to be fast open colleagues and came up with this idea of negative sampling would be near enough and so the idea of negative sampling is we going to train binary logistic regressions instead and so it on the train 1 binary logistic regression for the actual world observed what's in the numerator and you want to give high probability to the word there was actually observe what we going to do is we going to sort of randomly sample a bunch of other words they're the negative samples and say they were the one actually seen so you should be trying to give them as lower probability as possible the sort of notation that they use in the paper so slightly different to the one I've used and they actually do maximization not minimization and that's their equation which I'll come back to all we do that he is the sigmoid function so the sigmoid function normally with like this 1/1 plus each other - play the sigmoid function is like a binary case of the soft MAX Function ride that we have two possible outcomes and that your son of a game got an import that is any real number and it's nothing until probability distribution between 0 and 1 which represents is to binary outcomes and to the extent that the numbers positive and ceilings 21 and negative goes down to zero this time we going to take the doctor for the good word we're going to take the dot product of the two vectors sigmoid function and then we Gotta want that probably estimate and to be as high as possible this version which is just right play I'm to look as much as possible like the notation that we used last time here is a new objective function for using digger disarm playing and we've got to turn it's the log of the sigmoid of the observed context word the outside words product with the centre word and we gonna want that to be big on the other hand chosen k words other words and we're going to work out dot product between them in the centre word and we going to want those to be as small as possible minus sign in there which is causing the sign of the 2 things to be different negative sample say it can be reasonably modest number you can just a kind of 10-15 negative samples and networks pretty fine sample some words negative samples in particular for post ring distribution that helps them along with all in playing with frequent word starting point of how you sample words if you use what we call the unigram distribution so that just mean to take words a large corpus and count up how often each one occurs just as words that there's a cold unigram what is unigram to the three quarters power and raising to the three quarters power has the effect of decreasing how often use sample very common words and increasing how often you sample rarer words that's that ok so that everything about words of vehicle I'm going to say sorry see you that couple of years often used as a normalisation turn and so this is saying well if you want the probability distribution of words if you work out power the count of the word for every word in the vocabulary and then these numbers you just some them up over the vocabulary and I'll be some total and we dividing by that so we get a probability when you see the letter z with no explanation it normally means I am a normalisation term to turn things into probability raid over the numerator term light through questions of things I have explained otherwise Christmas yes size window the use.it come back to that in a bit and so a little bit of data on there but yeah we haven't done anything about Dad at the moment we getting a Windows size like 5 which is in the bed one but you know there isn't there hasn't really been any science behind that the people treat their that's what they called a hyperparameter which means that you tried a few different numbers and see which one seems you use in your future work a reason or just because know that play chosen as a hyperparameter performers I mean actually you know for this word to that paper I mean you know it turns out that in the actual paper what people discovered when they started digging through the code which two to their credit they did make available reproducible research the direction a whole bunch of tree front things like these hyperparameters how you sample and how you wait windows and various things to make the numbers better so he know people play quite a few tricks to make the numbers go up which aren't particularly theoretical sometimes in general for lot of these sampling things that's a bad idea play multiple parcels if you just go and then blonde blonde again that's a bad idea but a common technique a lot of the packages used is that they do use the shopping operation at the Beginning so for each epoch they are shuffle the data randomly and then now go through it in sequence and that has the benefits of faster computation from locality eccetera 12 meaning that when you do it differently pocket will work out differently last question I think was talking about taking from the corpus play the actually say sample 20 randomly from the whole corpus vs just on the working from left to right yeah so so you could argue whether or not this was written in the clearest way that right so we're making this dot product win the gating at which is then flipping give the space where on ride because the signal is symmetric around 0 so if we've got some Dot product and then we negated with sort of working out of 1 - probability that's the way in which way actually for the first term turn my one in the probability be high and then for the negative samples with one in their probabilities emo ahead now guess what an algorithm is corpus position by position and your son of doing word playing some premises in your learning something and you know by jovit seem to work based on what we saw in the examples but you know you might have thought that was kind of weird right but we have this whole big pile of day that you know wishful thinking of statistics Rise if you have a big pile of data aggregated and it's sort of seems like there are obvious things you could do here you could say well there's a word using banana let's just see what words occur in the context of the banana and then we'll be able to use those to predict somehow and you know the Birds were traditionally used including even with distributed representation text it's a bit about that so you're fully educated and don't sound like one of those people who were our where no work that happened before 2013 when your network stalker ok so what we could do is we can essentially do the same thing as sort of words of equi could say there's a 5 word window around each word instance that often third was a word token my son in ok we often what does distinguish between a particular kind banana particular instances open in the text and not referred to as so the type token distinction so we could each token of a word and the word five around there and then we could so start counting up which words occur occur with heard and so we can then have a metre parents Scouts we'll have a game sample of this so normally can use the 9:55 but you know I can just use a window of one to keep my account very simple and small I know left or right just like word affected and so 518d baby corpus like this to do is just stay here the Matrix of word co-occurrence accounts within my windows size of one I occurs next to like twice like a curse next why twice at symmetric and all my other accounts here as singletons just give me a big hue shapes of that you could do is just use this matrix directly it turn off data here but you know if you sort just like the word learning what do you do if you expect these two vectors within up kind of similar to each other similarity of the victors directly in terms of these current account it's a little bit unappealing doing things this way right if you have a quarter million word vocabulary that's where you're in this space where in the trillions of the number of cells of this Matrix I want of storage there with your clever and loads of the most of the cells was 0 representation might take a little bit late application models might have sparse the issues because she know a lot of those sold out prison and so might not be very robust traditional answer to all of these things which is well maybe we can have that big colour current account m reduce the dimensionality of a corresponding low dimensional Matrix which preserves most of the information in the original Matrix and you know maybe or reduce things to a dimensionality and somewhere around the size 25 to 1000 as is done with word David a standard most common way of doing this dimensionality reduction and you don't really have to understand all the mess but you get the play with this and homework one which is and for any Matrix for the singular value decomposition which of the wave and take an arbitrary Matrix clothes that into 3 matrices play the centre one is diagonal and house one in what they called singular vectors which are weightings of the different dimension so they decrease in size as you go down 2U nvr then orthogonal basis what are the rows and columns titular it's even simpler in the case where we just have it word vectors because you have a M2 play the same general case set the song orthogonal basis is in have these days really matter cos they end up being used for nothing when you work out the product if you want to reduce melody what do you say the smallest singular value decreasing size then effective rosenkohl play Tracy is and then it says behold I've now reduce these things to a two-dimensional representation from the original three-dimensional representation and that referred to as the reduced svd.se and the classic result is in terms of least squares error in estimation that this these three things ok which is the best pay approximation to the original x squared least squares criterion do this word vectors pi function and I can throw into it please n make word vectors and least once looking really bad but I Give It A Day To CF37 comparison was popularized around the term The Turn of the Millennium at generally went footer word applications under the name of latent semantic analysis of latent semantic indexing and the idea was that you could have these directions that you are finding in this low dimensional space that have meaning and people worked with a quite a bit for techniques like trying to do information retrieval using these approximation work the day never really work very well I think never sete caught on the missus kind of continue to be explored actually mainly in the community with people doing things with word meaning and the son of kind of interesting the kunai to the literature and that there is this guy dog rowdy who did a PhD at cmu I'm in 2005 and basically what he discovered was look if rather than just using RAW can doing more in terms of going with the cow produce results for the much better using RAW counts you have to do something to do with those very high frequency words nearest you could log scale when which is also commonly used in information retrieval another idea is you could just use something like a a ceiling function so you take the minimum of XCOM 8040 steps and that some number like around 100 he used the idea which was also another the Hex that was putting the word to Vick was rather than just treating the whole window the same that you should count words that the closer more sample closer word more commonly than further away word damn you're still having to have a differential count because the words etc plenty of bed rather than using council tool he then started using Pearson correlations which help turn the time to negative and the the size of his head if you then got rid of the negative value in sunset this sounds like a bag of hair and he was able to show the transform count collection then give you very useful words vectors that I'm about to show realise the next a different form several of these exact same counter actually been used in word to take as well I'm about to show exactly that that's actually really interesting little bit of the day they're so yeah yeah so did the thing if you do that you'll not only get word similarity is that pretty good let me show you cleaner so this the precise idea of evaluating with analogies was not something that really been developed so that was actually something that smash Meikle of suggested Sully dog Rosie made this really interesting observation which was he said look once I do these kind of transformations to improve the semantic representation of my word vectors really interesting property emerges what you find is that there expected basically linear component deconstructed space have the thought of a verb to the door of the verb Direction clean Jennifer swim swimmer learn teacher or teacher doctor tree I mean you know it's not exactly perfect but he roughly it's completely clear they sort of a direction in the space that correspond to the door of a and yes we have discussed this idea of doing the analogies test in retrospect is obvious is if you can construct a vector space linearity property they going to do well an analogy is so effectively haven't been to the Victor Spice they do well and allergies because this means that you've got this Direction which is the doer and then you can immediately say that's the door evektor which you can get from subtracting Queen from swimmer and it has isoclean from janitor and then we can add it on to swim and will get somewhere close to swimming SpaceX she did do that this pause some sensors if you have if you kind of do carefully control accounts and so on the conventional methods can also give you good word vector spaces so that was actually the starting off time Guam essentially there been is two schools of work and they're being artwork that been explored more and coxside in anywhere else start counting and transforming cat you know it had some advantages it seems that had some advantages I said you're making soda efficient use of statistics was using the global statistics of the whole matrix directly estimate things and at that up until they're really only been used to capture word similarity and a lot of it would suffer from importance given to large count started to show how they sold both of these problems are the handed beanies new network methods which are kind of Direct prediction methods we're finding a probability distribution and trying to predict the words that occur and they had some advantages electrical sampling means that you're not getting run out of memory hopefully I know we've had some memory problems with homework 14th and prints for your nan is better memory position and if you kept the construct a huge matrix because you're going away but you know you're doing it sample by sample is inefficient use of statistics and so but on the other hand nikolaus work at perform perfectly can you get really well Sullivan wedding to this work the Jeffrey Pennington how much is social media can we sort please ideas inside of her annual net method things with some kind of count Matrix situla we wanted to get the result in a slightly less hacky way that components of meaning being linear or non-linear operations in the vector space that there just some affect your heading or something like this observation of this model was that we could use ratios of co-occurrence probabilities to encode meaning components here is if you have a word like can you say how often are things going to colour curl with a solid should co-occur award and guessed should what is also going to co-occur Lord and some random word wanaka match theme get the opposite pattern with solid and gas so the thing to notice is it's not enough to just have large by itself because large appears both here in here or small appears there and they're the thing that's interesting as sort the difference between these components and they're indicating a meaning component and so we can get it that the ratio of co-occurrence probability please show color should have meaning play Fat other words ratio cancels out to about 1 in this slide out move so it's not how my small and large but these are actually actual counts from a Corpus so we roughly get the mention of meaning between solid and gas about 1 because they're not 2-dimensional meaning it seems like what we want is to run ratio probability m and l Space good business what we want to set about doing well how can you do that I can do that is you can make the dot product the log of the KK probability then immediately you get the effective difference into a ratio of the co-occurrence probability is essentially the whole of the model is that we won't have got products the logs of co-occurrence probabilities here is our objective function here and it's made to look a little bit more complicated but essentially we've got this squared Moss here and then we wanting to say the doctor where is possible to the log of probability and sealed they'll be lost to the extent that they are not the same play complexified a little by putting in biased 2 words word of just overall common ways to kill a curse things or uncommon or dozen one more little trick because everyone does tricks to make the performance better is that we also use open infra capping the effect that very common word pairs performance of the system intellect gave us the glove model of word vectors play the interests of this was you know a lot of the preceding Whitchurch should be there by niece count methods and their beanies prediction method what was the discus the two they showing you how you could have a method that help matrix be done in the same kind of iterative lost space the estimation method is used for the new method vectors work to give good words vector so he is results for the word frog Bolton Toyota obvious but Leo these different kinds of words kinds of pretty 34 that from here and say little bit more about some of the work on evaluating word vectors and this is maybe also chance just talking little bit about evaluation altogether NLP when we do a valuation the first thing that comes up is intrinsic vs extrinsic evaluation there's something with trying to do like model and word similarity with word vectors or we're trying to put parts of speech on words or something we can just have an intrinsic evaluation of staying how good a job did you get are you getting the right part of speech synonyms close together and that sort of normally very easy to do and fast the computer useful to do because it helps us understand the system and a lot of the timers intrinsic valuation clear where are they having done well on that task is really going to help sing natural language understanding robots that we so ardently Desire so also very interesting extrinsic evaluation extrinsically is then same will supposed to use this new stuff real system doesn't make performance go up then sort of definition or what county was a real system but normally that's meaning it's some application that human beings actually care about and like to use sing like website answering or phone dialogue system or something like that that you can put it into that system and the number skip go up volume 41 and do you want to have stuff that works in real task of course on the other hand or what a things are a lot harder than so much more work to do such an evaluation and a run different variants of the system and even or grade sometimes it's hard to diagnose if you're Brave New World victors don't work better in the system you know it might be for sort of some extraneous reason about how the system was built it's of hiding all your magic and if you just change the rest of the system of family so it's good effect so it's kind of hard to do I'm sorry goodness and badness I'm so today I'm only going to say a little bit more about these intrinsic word vector evaluation took quite a bit about these analogy working out the analogies it turns out that normally what people are doing is working out a crew sign between different word candidate to work out which is the word that's all the analogy which isn't tiny wrinkles and it's also one of the trick that people commonly use they forbid the system from returning one of the three words you put into the analogy but nevertheless I'm so this is something that you can evaluate your own house and go socialisation so this love this location so exactly the same kind of linearity property the dog rhodia discovered which means than allergies work sorted by cancer when did making meaning components linear so this is then I'm showing agenda the spy this is showing one between companies and their ceo's and you can also do more synthetic peg so this is showing positive comparative and superlative of adjectives I'm too much Mika love came up with this idea of analogy task build a dataset with a lot of analogies in a solid Play-Doh dataset because it's sort of test a few random different things things that is system work well on that you know it capitals and country and cities in States countries and currencies that are bunch of semantic things that some things that taste so bad worst far let you know even some of the ones I showing before you know there's no farmer is the Clinton ones that are actually in this evaluation set table of result comes from our glove paper so not surprisingly the glove paper evaluation and because there's Al paper that perhaps the things to start to know this is yes if you just do a plane a speedy on couch works the blue badly for these analogy task kind of his dog rowdy showed if you start then doing manipulations of the count matrix before you do an STD you can actually start to produce an svd base system and actually perform these pass adley other things that you will discover right at the top there are 100 dimensional ones and at the bottom there are some 1001 another 300 dimension training on a big amount of takes bigger dimensionality definitely come back to bed at the minute the amount of text makes a difference as well right so we're going up from 1.5 billion words that beginning to these ones down here being trained of a 42 billion words of texting helps unsurprisingly the 42 billion word of text ones work better here couple more Steps From This paper so this is a graph of dimensionality and what the performance is so for the three lines with Greenland semantic the blue ones that synthetic analogies and some score is up to dimensionality 300 things that clearly increasing quite a bit and then it gets fairly flat which is precisely why you find a lot of words vectors and that of dimensionality 300 showing what window size so this is so what we talked about symmetric on both side window size and is it goes from 2468 10 and sort of what you see is a fuse very small window like to that actually work prediction is stronger by syntactic effects a very local where is as you go out the semantic prediction gets better in bed at actually does synthetic gets a bit better as well that's especially the semantic the game the right breast shows that if you only use context on one side your numbers are as good just wanted to sort of sneaking a little cameos of a couple work has sort of us to what things people are doing and with word vectors and so this one was actually by to stand people they are the best this would be the best or if I could say that this was a final Project carpet washer that's not true there's nothing to do head some sort of clever and very messy ideas whether using matrix perturbation theory and sort of showing hell Saladin word vectors actually sort of feeds into the bias variance trade-off if you've seen that and in other parts of machine learning and I'm not even going to attempt to explain their paper and but here it is really well with this paper they gone all talk at Europe's from it and so for this tournament interesting resolve with what you see with these word vectors which is in my kind of surprising so this is showing doing word vector dimensions from 0 up to 10000 so we going way higher than we talked about before and so what you discover which people are known for ages is that this sort of a little blip that somewhere around 2 or 300 which seems to optimise performance clothes sizes but the thing that they were sort of doing a 1 of this theory about its kind of surprising is well surely if you have a humongous humongous number likes and if you're using 10000 dimensional vectors you know you're trying to estimate another two orders of magnitude more numbers to every word just fall apart and because you got hopelessly many parameters the amount of training day that you're trying to estimate these numbers Farm and so the interesting result that they show is the things don't fall apart and that you can essentially go out to the students use dimensionality and the performance days flat and they've got a lot of theory for predicting why that's actually going to end up being the case training late models of Leigh Rangers showing keep on getting better for awhile so you know just go out go sleep how it's doing right so that if you're running a hand for 24-hours your numbers are better than if you only read it for 6 hours and then that's true for a lot of deep learning models sorry so this is the key reason why you don't want to start your assignment the night before it's due program at perfectly you might just not have enough time for it to run so that you produce good numbers at the end of it more what is showing here so these again semantic syntactic and overall numbers so they're sort of two things that so being mixed together here one is if we just look at the overall numbers the highest over here and which is this 42 billion common crawl webpages corpus that gives us the highest overall number but this sort of something else that interesting in this graph which is PDF work well say that you actually find the 1.6 billion tokens of Wikipedia works better than 4.3 billion tokens of news why newspaper article day that and so what sort of actually makes sense value of the job of encyclopaedias to sort of explain Concepts and how they relate to each other right so the previous adjuster matchmoor expository show all the connections between things where is newspapers I'm trying to expose that help things fit together they're just telling you about you know who got shot dead last night or something like that right so sort of interesting facts Wikipedia data kind of relief instead of his differentially useful for making word vectors and Y no info well with a glove word vectors and what do people use those you know I think actually one of the reasons why they work so well is that the original word two vectors that Google distributed search field only on Google News data where else side of Heaven the data inside them yeah so that is all the work on analogy but the other more basic evaluation is this one of capturing similarity judgements and I haven't said much about large sub Literature in the psychology community where people have wanted to model human judgements of similarity a good site person what you do is you find your classroom of psych one undergrads and show them years of words and save rate these things for similarity on a scale of 1 to 10 set data has been collected and you work out the mean over human beings and they give numbers like this cat 7.35 tiger tiger 10 book and paper plane and stocking phone stop and CD and you get number what we doing is wanting to say well let's use distance in the space Play onto these similarities judgement oh well doesn't matter and so that sort of similarity judging has also than being used for evaluating these system so again hear a lot of models this is getting out of paper but so they have these various similarity data so one of the best-known ones that I had on the slide before is this word sim 353 it has three I'm sorry your sort of them modelling a correlation between your judgements of similarity and the ones that came from the human beings I want to say yes problem right at the beginning and how that could be various people and that's perhaps in some sense the simplest case of words being and BBQs when you have names whichever Paul but it's not only true by enlarge human languages mpq us and have lots of meanings that's especially true common words they always have lots of meaning it's especially true of words of existence for a long time loving you very technical words you know carcinoma I think that only has one meaning but you know if you think of any relatively common word and starts and head for a moment you'll find it has lots of meaning this is named such a common word but my rainy the word I got here is pi there's lots of meanings that has meanings like it's a kind of fish yeah so there's a fish that the pipe what else is a pike Spear yeah Solas Spears a pig other kinds of Pikes 6 move on diving move it's a road yeah so there lots of meaning Ballymena pikers also uses the verb to mean the pull out of doing something like we were all gonna go out to a nightclub later but joepie can you suggest coming in this country but you can try that lots of meetings and you know this isn't only true of the word pick any other simple word right you can pick a word like shell field or house or make you know they have a lot of meetings when it comes down to it so you know but so how can this work if we just have one meaning for word interesting question or something actually interested in early on so before the word effect paper came out back in 2012 and we are playing around with new all word vectors we thought boy this is so broken having Only One Sense for a word why don't we come up with the model that has multiple sensors for a word so we did that and we did Anna pretty crude way I guess play We Didn't is say well let common word let's class start over context in which they occur will see if they seem to be multiple clear plasters Experian for that word and if so will just stop pseudo word self it seems like there are 5 + another word for example meant to use his Jaguar what's the word JJ or just call them Jaguar 1 Jaguar 2 Jaguar 3:45 so it's just literally change the word in our corpus according to its class the number and then we run our doctoring algorithm and so we get a representation for each of those senses of the word and basically that works right up the top of Jaguar one next to luxury and convertible here is old version of Mac os called Jaguar any women and remember that one but Jaguars like next to software Microsoft updates that's hopeful he is the Jaguar that's right next to the Hunter I'm confused on this one is Jaguars near call keyboard and Stringer saraband that was basically work crude and also pets problematic for lot of time the divisions between sensors aren't very clear I don't want of sensors actually related to each other and overlapping because when house since is normally arrive as the people stretch the meanings of words it's not that they just randomly wake up the next morning and say I know carpet I could also refer to that has stone can giving you phone right you so take something that you know about like a web and you extended metaphorically to other uses of weight that's more interesting thing so this is the other sunjeev aurora paper that I was going to mention so then what happens if you don't don't have more than one word is the word vector that you learn is what's referred to by physicists and fancy people as a superposition of the word vectors of the different scent there's a superstar superposition just means a weighted ever effectively my meaning of pike is the weighted average of the victors for the different senses of Pi components adjust weighted by their frequency part maybe is perhaps not too surprising that the part is really surprising is well if we just everything these words vectors you think get anything out of the average ride like if I tell you I'm thinking of two numbers than there I said some is 54 what are my two numbers write you a sudden really short of information to be able to answer my question is word factors high-dimensional spaces a lot of words vs the space is so vast thoughts dimensions that actual words or Simpsons netspace cancel this whole literature on codeine compressed since sing some of which was actually done by people with steps department here in these cases where you have these sort of Spa goods in his high-dimensional spaces you can actually commonly reconstructed superposition even though all you've done is done this way to Devon and this paper looks at how you can do this have Aldi's underlying meaning component separated out so Thai have one meaning component does in the space of trousers blouse waste code that makes it what's the meaning of coming on the season teams winning we make sense Lyon goal with equaliser clinching schoolers seem to overlap with this one of tie cable ties and wire ties and things like that are actually able to pull out the different since meaning out of the meaning of the word so that is a kind of a cool thing I just want to one more thing ok racing so far was intrinsic and also might want to do extrinsic evaluation of yy word vectors exercise of people and NLP so much is it turned out that having this meaning presentation meaning just turned out to be very useful and sort of improved what you're toss after that this is doing named entity recognition with just labelling persons locations and organisation call of Mini TAS what people found was if you started with a model without sort of word representations and you throw in your word vectors regardless of whether they were the vehicle glass ones just kind of your numbers go up couple of percent or more and so word vectors we just sort of this useful source NLP system that you build and your numbers went up so there is just a very effective technology and which actually did work and basically any extrinsic task you tried it on 