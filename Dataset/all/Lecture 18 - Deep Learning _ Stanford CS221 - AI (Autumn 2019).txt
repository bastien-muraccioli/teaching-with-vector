ok so let's begin Percival I want to see congratulations you all survived the exam will you have a great day back but you you completed it yeah I just want to say that we had agreed back as soon as we can this year's I'll be great Rashi Anna cancel of Saturday so we can focus on getting the straight back to you quickly after this the course that goes downhill so you guys can kind of light and so after exams to things left so there's The Prodigy an so the final presentation and the poster session for the project is going to be able to leave the Tuesday after vacation and it's like a big oratorium Hall there's a lot of people from industry and academia and it's really exciting to have so many smart people showing off their hard work an and then you have ASK Pizza which is larger yeah Monday ok might say right after you back in vacation is that post a man on Thursday is the last piece as do larger so logic is this is not like my opinion but I think logic when I took the glasses easier than the other it doesn't take as much time so guys are definitely pass the highest point is grass yeah I think that's the general opinion an that being said I wouldn't wait until the last minute still start really then yeah so Gazza the Office US it's so bad today though we're time with this fun event is topic which is this and I say yes because I think I'm one of your probably working an anti-bullying I've heard of it already so one of you have it in your project and today and we're just cannot do very high-level broad pass of a lot of different subjects with indeed open get you excited and give you a shower understanding a lot of different topics so that if you want to take and follow prices like in 24 and 29 event then you'll be and with some kind of background knowledge and it's all about the running you probably heard of it it's really big assassin alas by 10 years but it's actually been around for a long time and so you've been back to the 40s there's this era were people trying to build more computational neuroscience models noticed that the new recommended you know there's neurons in the brain in The Range in these networks and I know that elle'gance arises from the small parts and so they really wanted to model there and the first people to really do this for McCulloch and Pitts soap it was actually legit and it was they were concerned with making these kind of logical circuits out of a network like to Polish an what kind of logical expressions can we implement with Annette I'm back then this was this was all just a mathematical model but there is no backpropagation there are no parameters and there's no in is just trying to roundabout fruit I guess I theorems and proofs what kind of problems these structures can solve an Pok√©mon by ten years later and started moving things in the direction of like training these network he noticed if two cells are firing what together then they should have some kind of connection and that is strong this is wasn't private alterations Lewis actually no format what if it was just very smart people make conjecture and then it was until the 60s there so no network I guess you can see me in the mainstream at work it was thinking about it and excited until 1916 I went skiing Papa release this famous book called perceptrons which is this I think that book other proofs and the were basically talking about the the theorems that were about the limits very shallow net example early I think they've really in the class we talked about the ex example where if you have Newcastle and arrange this integration there's no there's no linear classification country that you can use to separate and Crossland and soda papa in the perceptions account with lot of these I guess you can see the counterexample that what a theorem that really threw that these networks credit really do a lot and at the time it was a bit of a killing blow in Greece so mainstream AI became much more larger no networks were very much into guess the minority there's always people thinking about and working but the mystery may I went definitely tour add a symbol logic-based message that person talking with the last couple of weeks write like I said you're silly people in the background working on it so for example in 1974 he came up with this idea of that propagation that we learnt using the chain rule to automatically update weights in order to improve predict and then later on Roman heart Williams they kind of I guess you could say they popularised this yes you can say rediscovered and where was his findings in a really sad oh hear about that you can use backdrop and it's a mathematical feel well can't look well-founded way of train Annan 80s so today we're going to talk about 2 types of neural networks convolutional neural convolutional networks trace back to the 8th new cognitron that was invented by a Japanese Proxima it kind of laid out the architecture ACN but there is no way of training international paper they use Cancun with feel like hey there is this architecture Lee can you basically we just like by trial and error came up with these numbers to plug I look at how it works now it just seems like insane but back then there was this in there no way is a train and until about 10 years later and so and he applied does he is back up Jewsons and it came up with a raccoon net which was a very thing Jack readings and it was one of the first industrial scale applications of deep console whenever you write a check in and have your bank committed almost all the time there is a machine learning model that reads that check for you those chattering systems at some of the old machine learning use a scale latest over kernel networks in the 90s can a proposed it and then there is this problem with training and I will talk later experiment exploding organic ingredients a man and ridden but 10 years later with I guess you could say maybe a this song to some extent those issues with a long short-term memory networks and we'll talk about that later an and then but I guess if you can still say that Canon in D minor so in 80s you use the la trobe STI in the 90s people were all the support vector machines and inventing new kernel if you go back to machine is this we just Like A linear classifier with the Hemsworth the wave Project dealer into it like a nonlinear subspace the 2000 people finder start making progress at the school idea of Hamilton train his deep networks one layer the time so will put you in one layer and the second layer that on that on and you can build up these represent different kind of payment so this looks like maybe 34 years ago really started taking off and ever since then it's really been in the mainstream and you can as kind of proof evidence towards mainstreaming you can look at all of these application so almost a decade this performance in speed record Cynthia recognises reusing hidden Markov model based like I was in that was the heart of these iron 10 years of answer stagnated and also and what's new in the surprising is that all the big Council IBM Google Microsoft they all switch over from the classical speechrecognizer into coriander and neural network base recognise very quickly in a matter of years and when is companies are offering a scale and you know dozens maybe hundreds of people have tuned these systems very intricate so quickly so radically the core technology behind this product really speaks to its power and same thing with all the recognition so there is this image net.com which goes on every year that says basically like how well can you say what's in a picture the 1st and Selfridges people use these handcrafted features an oversight in alexa.net was proposed and it almost got half the era of the next best submission and then people have been using wrong though and now if you want to do computer vision you kind of have to use this CNN it's just a default you are control everything posters going to have a CNN and same thing with go so Google deepmind had a headache cnn-based algorithms for reinforcement learning in that beat the world champion in this very difficult game and then 2017 it did even better than you need in Southway machine translation Google Translate almost a decade had been working on building a very very advanced who was performing classical machine translation and then all of a sudden the first machine Translation system was proposed in my 2014 25th and then sorry year later they threw away 10 in the decade work on the system in transferred entirely to completely new which again speaks to its power so but what is I guess in a why is this thing so powerful why is it so good and I think and so Broadley speaking it's a way of wearing you can swear that any kind of do you want like a sequence of picture even that reading Like a gamer girl and you can turn it into a gas this letter is being dense representation of whatever information is Qatar balladeer and this is very careful because these actors composition and you can use these components these models of your deep learning system can I play ROBLOX actors in Adam together use this to modify you just the compositionality makes it very facts ok so today we're going to talk about people that won X commercial Max with work on images I guess anything with repeated high structural information in recurrent neural networks which operate over sequences and then if we have time will get to see unsupervised learning types so first for feedforward networks so in the Fabulous crusty chocolate when you're pretty weather predictor if you define like a better w that's your way and then you hit it with some invite and you got them together and I just give you outfit ignore that works Redefined very similar way so you can think of each of these hidden you as the result of a linear prediction so working backwards you so you'll have to find it back can you hit it with an activation what's an activation impots I'm kidding in and you do that with your head can you get your help and then you arrived with your hidden by defining Factor in Hindi with so you use your inputs to compute hidden number and then you use your hidden numbers to complete your final help stacking when you're each each number so it's 1 it's 2 and are all the product I guess you consider all the result as like a little mini Moroccan group together Chester visualise this if you want to go deeper you just rinse repeat so this is you can say this is the one that it's before when you're pretty you just you have a better weights and you apply it to your inputs you apply instead of a vector 2 inputs your plyometric soon plus which gives you a new vector Annan you this intermediate vector this hidden vector with another set of weights and I give you and then you can just rinse so you passed the vector and passive matrix to getting a new vector new pass that do another matrix to get a new vector are you finally at the very end of it with a vector to get a single number arm so just a word about death that that's one of the reasons why these things are really Powerful though there's a lot of interpretations why death is Hope why can I stacking his mistress works well one way to think about it is that it where is representations of the input which are hard so it's going to be some kind of representation is Prime is going to be a slightly higher level for example in image processing system each maybe represent it could represent like Vienna in a pick is pine wood corners is double prime could represent small make fingers is Triple d the whole hand successfully I guess you could say like higher level representations of what another way to think about it is each wire is Connor by step in processing you can think of it maybe like a for loop where it's like the more the morale the more recent you have the most steps you have the more get you high the more processing you able to perform on the info and then what and the deeper the network is and the more kinds of functions represent I am so the an yeah so there's flexibility in that as well but in general there isn't really a good form of understanding of why that is I think a lot of people is there a gap between the theory and the practice so yeah so this I guess just go to show my debtors have also if you input pixels maybe your first layers giving you edit action and your second layer is giving you the eyes or nose is a and then you earlier in a Barbie's giving you hot so just a sunrise so we have these deeper on that and they learn hardcore presentation of the data I guess you could say it's like Guinea altitude prospective you can train them the same way that we weren't you can train them the same way that we were and had a train or when your boss estimate gradient so you had your last time you take the derivative with respect to and then you'll probably get the green it's two step in a day I think of you helpful and this optimisation problem is difficult so non-linear non-convex bye in German we found a few throw like a lot I did add it by computed it then somehow you an so it seems like basically just to review how you train these things in general it's the same as a linear predictor you define a loss function so for example this is great last we'd say I'm going to take the difference between my two output in my predicted output that and any ideas to minimise this and where you do that is you simple dear points from your train dealer can you take the derivative of your premises with respect to this with respect to your loss funk and then you move in the opposite direction of the gradient which would hopefully moved you down and so the problem is a non-contact so when your classifier because it's linear will have just look like a ball where is these things you have these non-linear activation functions and you end up with a very messy looking ever Circus before she doesn't that was the big that was the number one thing I was holding not that difficult to get work and so business thing exchanged is one weak acid community of GPUs which can a paralysed operations specialist and then there's a lot more data an that's not entirely true so there is also a lot of the tricks that we found out recently so freezing and if you have lots of kitchen units then that can be helpful because it gives more it is more flexibility you could say in the optimizer if you have if you over provision if your model has 2 more capacity than it needs then you can be more flexible with the kind of functions that and so we have better after misers so where is STD will make it'll take your step in the same direction by the same amount every time we had these new optimizer is I Gotta Get It then decide how far to move in a direction once you've decided we have drop out which is where you noise the outputs of each hidden unit and that makes the model more robust to it's owners guard against overfitting there is better in this location charges still there things like station Anders things like free training the model on a related dataset before moving on to the day you actually care about and then distress like that which is where you ensure that inputs to your neural network unit are normally distributed they have mean 07 Division 1 and what that does is it allows you you basically take bigger steps yeah the takeaway here is that in general the optimisation problem and the mall arctic sea define are totally toppers it's kind of a black write down statue still not very good ok so we're going to talk about country song and that these operate over image and the motivation is that ok so we have a picture here and we want to do some kind of machine learning processing and we have all the tools that we need to do you can say ok each picture each pixel is an element in a big long dark and then I'm just going to brother Adam but the thing is is that that doesn't take a dinner just the fact that their special structure in this picture so this pixel is going more similar to this picture then this pixel down what if you passed this in 13 m then every pixels going to be treated unique window and so we wouldn't rev that special end decor ideas with convolutions so conversations you have this thing called a filter which is some collection primers and what you do is you 1-year filter over the input an in order to produce each output so for example this filter when applied to this upper left corner produces this upper left corner be out and an application of a filter works cannot like a doctor where are you multiply you multiply all the numbers and then you add up the mobile and so how you produces output as you take yourself there and you'll be sorry just slide it around in the input in order to get your output at the neck though yes so this is concrete so here so where is this was a two-dimensional position because when you're sliding around in both dimensions this is one One Direction and we slide it horizontally across free sample we apply it so One X 0 is 0 0 x 1 is 1 and -1 who is negative so negative and then we do the same I think it was so we would this filter these three numbers in order to arrive at one of these days is this is that where is so if you had so if you had that say you had an 14 facts this is your hidden there so what is 1 H2 H3 in H4 only had 14 pence x 1 next X3 in X4 if you did a regular fully connected Matrix every one of these is going to be next every one of these and your parameters you're gonna you can end up with a 4 by 4 Matrix dude this is what your metre is going to look this is your daddy is you need a new year away for every one of us where is if you do in convolutions it's much more efficient local connector so you have your is 4 X1 x2 Italy is only connected what's receptive what is the inch the filter would be applied to in this case we would I have free weights because we just have the sliding window and you apply at each step so give you a cooking activity be it's much more efficient in terms of primary new you're sharing the same primers at different places in the employer an and it is you the school intuition of sliding around in the empire so it's white I have my filter three things and it is this good intuition if what say this is this is this is 100 this is wine in this is 3 what is an interpreter as my filter really likes whatever pattern is going on in DS3 and doesn't like so much all the other Pat it's picking up yeah you have this nice interpretation for the filter in general what this looks like is in practice instead of Wonderful Tonight show they're very high dimensional volume and so your filter is going to be a cube in input space Inuit sliding around and applying it at every place it can fit in this employee amen the reason why the output is also avoid is because an you have multiple filters so over here for example this blue filter is when you slide it around in it's going to give you this I like clean but then you have a second self this green felt you can also slide around the Empire and that's going to give you a second immense your hidden States and so on this nice demo where basically we have so we have a three dimensional employer I have two filters which are you think I was little cubes and it's sliding is cubes around me in and every application gives you wine outlet in the three-dimensional help volume this is the same picture as before reassignment cubes or to fill in and you did say like layers of the output another thing people do is next pulling so remember that interpretation and have a filter as a as like a pattern detector and what this is saying is you take a region in your employer you run your filters event get your I like Limerick and then you look at regions in the hour and take me maximum activation Terry that en26sn players an interaction there is instead of you looking you're searching for a pattern in a region of the and it's also helpful because remember at the end of day we want to do classification regression something we want to get this thing down to like very small number of have issued high-dimensional then anyway we can reduce its size as good and so this is an example of how these things work is it the pretty straight for basically c you just had your convolutional layers use takamol can I have some poo Lang you go down and down in dimensional will you eventually get down you and a like a distribution of a possible labour this ties into what I said before about that legal walk-in Ali because this this entire network is built up 1234 LEGO blocks in a way it's basically just stuck in them on top of each other and composing them up in order to get a image classifier I'm so tired with three case studies of seeing in order first one is outnet so this was that one that did really well and Imogen CNN for the mainstream for computer Vision an basically it was just a really big mirror that when tricky did was the use Rose instead of arm so the signal that we've learnt about I think we've the signal that we learnt about this is an activation function and it's going to look something like this and what day did what is instead they use the revenue sloths a little more like that in practice that turns out to be easier to train in use and the next one is in Virginia and with Dylan imagenet a couple of years later basically it's it's just a CNN I think the things I know about this one is that it's very uniform so it was 16 layers and there's nothing you can sing it was just a bunch of these LEGO blocks stacked up the entire network is pretty much just look at this picture you can probably read it yeah net something else knows about it is a service trend of deeper I can't like tall and skinny network you noticed that there's a lot of layers but this layer is a very thin and residual networks a Residence and kind of Take That to the nth degree deal with resnet is so most the time you take your into capacitor metre staying out if you add in your input again then that is very helpful because it makes it easy for the model to learn the identity and so you can go the model the capacity or like 100 layers but if you're adding these residual connection which is what you call it when you basically just like adding adanac an then it allows the model to Schiphol later if it decides that's what's best for it so you set the music so also help with training and backpropagation if you take the derivative of the loss with respect to employer it's just give me one this part of the song and so it gives in where you can think of it as it is it gives the error signal can I get highway through the network and it allows the gredients to propagate much deeper into these words know that I mean so resonate Guy 3.6% remember the Alice network the water at 15 this is much better than human performance it will come up later when we talk about this idea of like Reese's joking come up later on the talk about that so Chester sunrise are often employed in image classification idea is that there is you happy still what you are sliding on the in and that lets you won and have it does cannabis like I give local crack a space in the output only depends on a small patch incident and then s and it's the premises are shared deaths has turned out to really matter networx I think to this day it's like people it's like every day this just a different network is coming people hardly found and downs yes I guess yeah how many words should be in silver so the question was how to design these things I think so there's a few different so and you start with something sofa you would start with something that sounds reasonable and then you will do some kind of like a grid search or you do now there's this morning which is really like you have a model decide what your model looks like in most cases you just come out cantu natural hair does it go up again s you look at the literature and say ok someone else the someone point me in the use network XYZ in so I'm going to start with that and then start filling from there and then third is 2 ml take that network has been pre trained on and a task and then apply I will talk about it later but free training networks and applying your task has shown to be very high an so now we're going to talk about the current lyrics the idea here is that your modeling sequences of in this could be things like texture sentences also be things like time series data financial data criminal record is something where the employer feed it pass inputs into itself so has time depending we have this very simple recurrent neural network and it is a function with 1 m and it takes as argument a pacifist and I currently in and then it predicts the next hidden c this is what it looks like if you're right in Kurdish this is what the actual net worth there's an Empire and you feed that into your phone as well as your current state add it just have websites off and most of the time people talk about the third person which is taking cannabis network and unraveling it across I guess it was the unfolding across where every time that you have an input and have a state and then you had your function with carries you to neck yeah curious from heaven your original previously oh I see so the question was what's the difference between this and the setting before when we had when we were the category descent where we were updating our ways that is that is equestrian so the is it before 4 St that was for an was sequential in the train where's this is sequential in so so you do you feeding 10 x 30 and then after all that time then you back propagate wipes for all those so to make that my career SGD it's like you have S1 yy I'm x2 white and you use this at aw right are you ok Daddy and then you update and yeah that's that's an interesting conversation that does this but there's no time within the data itself 4 it's it's more like this it's like how much Mercury is better so it's my guess it's more and you're EX11 x12xe 13 who won YouTube x23 and why and then you use this to update Anne and in this setting when we talk about time or temporary to sequence or talking about a sequence here in the data not necessarily in the learning yes arm ok so to meet this is a more concrete example we're going to talk about a non metric language so this is a model that and is in charge of sucking in a sentence and predicting what is the most likely word that will come next so so each employer recall x and are heading States we call teachers and way this works is we have some function that takes x 1 and a code into a hidden and then have a second phone that takes the hidden state and decodes it into the next we continue by taking both next bus X2 or next home plate and each one our previous against and then we use that a new hidden in coding and then we take new hid encoding and decoding into a neck each time the current employer and the previous state you first created in coding and then second predict or nest 2 steps and the cool thing about this to know is that this now we're building up the doctor's these each eyes and that's exactly working for its exact that in some way captures the meaning for summer of all the ex of interests that we fed up until that time and now we have a vector which compressors all those so Jamaicans very country one way you can build this thing is buy Bisquick Zeros newsticker Matrix no include puncture winter the internet an X matrix in order to get a vector and then it would take it the previous hidden state HDMI not by that didn't m nuvac new adult factors which is your new heading the same thing you take your tenancy pass it through a matrix to get her back Annan send that to result maxitherm director projects into a distribution and probability in general there's this with internal network so if there is a short dependency between the input an output depends on a recent input then the path through this network is very short easy for the gradients to the training at work very bomb ingredients have I have difficulty getting all the way through you so if you remember we talked about gradient descent as a credit assignment problem where the gradient is in some way saying how much so if I change the Internet if I change this by a smaller prepared by smaller how much will the output change in what sense that would be great what is the input and output are super far away from each other then it's very difficult to compute Snaps motivations the input will affect your help reason for that we won't get into it so much basically if you want a computer Green then what you have to do if you have to trace the entire path and that dependency and you look at older person to live longer any x x acer palmatum surpasses very long new multiplier lotto numbers and so if your numbers are less than 1 then the vendor Pro Louis moult and if the numbers are bigger than then that parrot is going to blow up and so that is a problem because it means you're brilliant are going to be tiny and no thanks weather going to be way too big and you're going to just like shoe into some crazy Direction blow up your school close Brothers good thing is that for the Exploding gradient that's not so bad as a quick what people do is what they do is w called cooking ingredients specified sum norm you'll be like any ingredient with a Norm bigger than 2 I'm going to go off if you been to explode in a go to 10 million you can say ok let's begin to some it was wasn't tonight but through the base ingredient problem there's this squad the long short term memory cell which is symmetrical network and but it has 200 sides this is Carnival I think the important thing to know is that your doing and this is basically like your input this is car like your previous hidden state and so what's going on here is your doing another combination you're taking your info adding in your provision state very similar to those residual connections in the resin and so this because you're adding in your previous state it's like adding in your whose input I guess you did and it'll give the gradients a highway you very easily go back in there's another perspective on this so this picture that the location is bye I think these thing to note here is that this so those are you can say are hidden States in this network go in Eltham so you can I guess you could say that there's two hidden that's what people say an so you have one head in state what is uhd and that's the state that you expose to the world and if you send my element have the same API as my iron in then this will be like the equivalent of that hidden state that we had but then you also have this internal hidden state the sea state that you never exposed to the world and so in this picture Andy sorry the notation is a little confusing in this picture corresponds to the h in the Pacific picture so this is the hidden state that you're exposing to the world and then response to c this is your internal hidden state and the thing to know about this picture is that s is just zipping around what's the constant error carousel and it's always internal and it's zipping around this thing in a loop and so what ended doing in practice is it ends like that is a vector in it contains long-term information that useful for the knee over many many arm so if you if you poke around individual dimensions of that then you and then you can find his long-term things being wearing so for example I record as a group you find networks that you find units that track the length of the centre you find UNIS that track synthetic used or brackets but in general you finally things that are just not easily interpreted the one last crew I guess idea that people have used with is working on them is secret secret smells like machine Translate which is where you have two sequences you have an input sequence and output sequence and you want to suck in your input sequence and then spit out your outlets can you do this with was called encoder-decoder Caroline you include your sequence by giving it to iron and I give you a one better which is encoding or compression of the invite and then you take out your seat rice pudding an outside just like you talking about before with the language arm and more recently there is these attention be smart which are very helpful in the case of a river's long sequin so if you look back here X1 X2 X3 are all getting compressed into a single fact what if you have I really want maybe it's hard to save that into you're 200 dimensional vector the capture the death of all that language with just a bunch of numbers an answer the idea behind attention is to is to look back an the way attention works is a very high level is so if you have your invoice you have X1 X2 and X3 and we've run Orion over these employees and so we have three Edge 182 an hour decoding and so we have we have a rnn decoder that has some hidden state will call as one and what happens if you compare Your current hand state with all of the seats in your encoder a new computer number that says how much do I like this stay maybe maybe it like really really likes this number and it's not too happy but this this better and doesn't like this what does is it is it uses these scores to turn them into a distribution call distribution that again how much how much do I as S1 why ETA visa and then what you do is you compute a weighted average aves injectors where the weights come from this distribution and with the service is so stupid an and it there's another way you can of writing this down on the slides I think this serves two purposes so first of all it gives you some interpreter so every time step you can see what part of the employer is it ok what is the input to have a probability mass on and then s what it like you do is it lets you this is the model from the pressure of having to put the entire input sequence into a single bracket now they can do is it go back and retrieve information amen more recently there is what's called transformer models which are your way entirely with the aspect is just attention and with a transformer you have your heating States instead of instead of having some kind of die cutter hidden state that you're comparing to the others what you doing is you just you select it instead and you can pair each one do all generators including itself the number of how much is 1 white Close Ottery and then you compute your weighted average all these hidden States that becomes your next layer an I would recommend taking to 24 and if you stop and Transformers are very cool and the reasonable time I get to get the new ltm as you get cool interpretable Maps like in Translation so there's an attention distribution and points at words that are correspondence so I like economic response to economic in in you can see there indeed and they also do this in computer Vision and you can highlight areas of a picture yes I just a sunrise so you can throw a sequence item and we'll give you a fact and there's this intuition that they are processing input sequentially can I go for a walk but they have a problem with training with agreed in see the blow-up or they shrink very small so lcms are one way of getting a sprog but they're not perfect I still have to save information into one deck the way people go round this is with attention-based models where you dynamically go back into him and retrieve information as you need it now in oclock about unsupervised learning I said before no network we got them to work well recently and one of that is because they need a lot of data bye if you're a smaller labia if you don't have enough money to basically pay for it even if it's a hard problem that just there isn't a lot of data for there's a lot of cases where there isn't enough data to train these very very large mod with millions billions of Iran where on your hands there is tons of unlabeled data download the whole internet an and there's none of this real inspiration from human beings like we are never given labelled what food animal in Watford tonight you just kind of knew observed experience in the world in you use that to inform your future experiences and you able to reason about it and make decisions so yeah so the first I guess think that really get it to his other colours and orange colours is that if you have some information can you turn a compressed representation that information allows you to reconstruct it then presumably and you've done something useful so in no never speak the way that works is you give it some kind of doctor and you pass that doing in colder and which gives you a hidden doctor and then you pass the hidden factor for a decoder which you use to reconstructor in an implicit what's in most cases is you want to take the different special you want your reconstructed input and output to be very same so just a motorway this this isn't deep-learning but principle could be viewed as one of the encourage is you want to come up with a m s u which can be you encode and decode a vet so you x x by you to give you a hidden vector new representation energy transfers X Factor then and give you something as close as possible to your original data no but there's a problem so as we if we have hidden vector of the bunch of units then it's not it's just going to learn how to copy inputs into outputs and so a lot of research on autoencoding and is kind of an supervisor is about how to control the complexity and and make the model robust enough to generate used for instead of just cough so the first pass at now can be using one US so you would do something like the logistic the signal loss and that means that the problem can't be fooled anymore by just copying into the alpine so you're going to have to actually learn something another way of doing is by corrupting the input so you have your input any noise maybe you drop out some numbers from it may be you preserve some numbers of it leave you add drop Michael Galaxy and add that to your employer and then you pass that through so yeah so UK drop so if you Factor is 1234 you can drop out 1 and 4 in September 0 or UK slightly perturbed business so that are close to the bye different night exact and then the idea is that do pasta sin coded in your you pass this corrupted input you both your encoder and decoder then the outlet should be very close to your original uncorrupted an yeah another is a very small in Qatar which has a Google can't probabilistic interpretation so you can think of it as kind of a bayesian network I I think maybe this is more useful to look at so you have an encoder and decoder and they are both modelling and I just probably this this is saying is I want to encode into a distribution over it anyone a function which is in charge of doing and then you want to specify some condition you say ok I want to make x recoverable from my HD and then second this is a term that prevents it from being degenerate maybe good way of thinking about this is instead of so I traditional will take my employer it would not it would send it through some kind of encoder and map into a hidden doctor and that for a decoder and that would reconstruct my in where is a variational autoencoder is going to take my employer anything that interim distribution over possible and then I'm going to do is I'm going to sample from this distribution pass that to my decoder introduce my reconstructed in Anne and the nice thing about this is that since this is a distribution instead of a vector you've been posted some structure on the space points that are close together in the space an app to cimarex hats and then similar as you move through this you should be able to drive the transition from 1 I guess we can start an input to a set don't forget this is experiencing community vision where they say appear this is going to give me like a chair or something an and then down here is going to give me like a table and then if I move for money other than and you constantly decode then it'll work gradually morph into the table it's really cool and and then so then the last method of Earth talk about is motivated by this task so where is the status icon and which is about 100000 examples example consist of a paragraph and then a bunch of questions multiple choice questions play Sonic so the problem here is that there's only 100000 and really the intelligence that this task is trying to get it is can you read a taxi and under which is more general Enniskillen is captured buy more data Nicola it's captured by does all the tax then you can possibly read is billions a word Wikipedia Google you just call the wedding download someone you could live with that maybe I'll be helpful for your Reading an and that is just a perfect case of this other setting we have tons of unlabeled data very small amount of labelled data so recently penalty community at Compass idea called well there's actually Nike Just Do it but there's a lot of people were doing some things but but is leaving for Tottenham what you do is you take a sentence and then you mask out some of the tokens in the employer and then you train a model to fill in those Toca the actual train the model on a bunch of things so they turned it on Tolkien film and then they also with glue two sentences together and an ass tomorrow or the sentences and like would they be a Jason in the text make sense together or not but the idea is basically like have give a bunch of unlabelled text to a model which is just going to can I can manipulate that data in order to learn structure for me without any Express rather than just learning the structure and the find it on the data for a long time then what you can do is win so where is actually being so we talked about Transformers before bruise like a big transfer so they trained this thing and a ton of uncertain tax and just this word filling task belonged unlimited it was they took their pretrained for and they took the and they said it feeding it questions from squad they took questions and then they would go onto it the program contact the news to answer the and then they would whatever factors are coming out of birth annual pass that through a single metre which basically predicts the answer for that squad Quest you did really well so this picture is right when it was released these are all the state of the and models more squad not only be of your tomorrow's by a large margin but also the human performer I guess in the end the intuition the intuition behind was that by doing these seemingly trivial tasks like word filling in next sentence prediction what you end up learning is you end up learning the vectors are coming out of this vectors that say what is the meaning of a word what is the mean what is the meaning of this sentence that meaning isn't like operationalise towards solving any task in particular it's just like in a very general sounds like what is and view this model with an understanding of language and then once I have and then apply it to my very targeted and that is that is kind of the principle behind unsupervised learning so you cannot like makeup these are most trivial prediction task just a manipulator in wearing structure from understand understand what a picture is and then what you do is then you find tuna I am the very small man of little dealer that you have an an that's what the currency the art is in a waterfields is this creature doing more and more unsupervised pre-training with bigger bigger models and bigger bigger date and the field really hasn't found like a limit to this and I'll be interesting to see how far it goes understood those fights just too kind of and recently I guess the biggest things that people have got into get known as working is one better opposition or g so we have these adapted albums that are not as obtuse as SGD doesn't have to move by the same amount every time you have the 1 of tricks like you know fine-tuning on supervised learning pudding ingredients we have better hardware we have better Deezer and that allows us top experiment trane Voyager model pasta and I think I think one of maybe one of the problems with the field is that the theory is isn't and one of his lack we don't know exactly why no network work and y30 but learn good functions despite having a very difficult optimisation surface an yes so just a sunrise we we talk about a lot of different building blocks we talked about how to Leverage spatial structure with we talked about how to feed sequences into recruitment we talked about sequence-to-sequence paradigm for machine translation unsupervised learning method that helps you can jumpstart your downstream I think the big takeaway here is there in the Big Ben is that they are composite it is you take they taking important in the Trinity you can start combining these things and very flexible ways and so in what ways designing these things is a lot like pretty little Lego set you have your building blocks in lstm attention encoder and you can decide how to I want to run this I am here in this also here and this one to attend concatenate the result with the output of the CNN an because of because I guess you can see like a magic about binders even more general it allows you as a programmer to instead make a program for solving a proper allows you to make the scaffold that allowance computer to teach itself how to solve the so and instead of defining the function you want the software to learn you divine a very broad family of that this offer is allowed and then you let it go and run find the best match with yeah so does Robert things about today hope you are having a good day soon break 