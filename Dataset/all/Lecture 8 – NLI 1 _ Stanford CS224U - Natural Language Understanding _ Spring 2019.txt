turn on happy Monday going to go ahead and start off today by reviewing that they got results from last week so there's a refresher classic Bake Off was on the tack of sentiment analysis so it was the 3 classed as positive neutral and negative on reveille waiting on the Stanford sentiment treebank test that had a total 27% answers on and we use the macro F1 score is devolution basically like kicking the accuracy on every class on an averaging all day we had a pretty imbalance test with like 900 of the positive negative and neutral and in general the worst which kind of expected here is a histogram of the squirrel that you have submitted high score was almost and at the mode was around a .5 and you can kind of see library you fall in comparison to her Bassline would actually kind of like a dog basically are we have a the winners of the Bake Off before that the high scores in a low score is and looked the like talking that appeared in the submissions for the top scorers which are defined as like scored that are about above and as you can see it's like you're most weight of a lot of deep learning approaches in people who leverage representation in the lower scores on canvas mixed bag and see some of the like baseline of functions that were in the original as well as like some gloves tokens so it's interesting to see like how far has there been they're not even really outdated but just last year that are now you know in this low score section so that's really Now onto our first place Group 2 and group 18 accusing score of 69 to using their belts and end-to-end approach 13 using Smart l process processing on first thing found in the data over time thinking very smart because of the imbalance and and then actually eat like rejoining the contractions in the dataset and this is kind of like artefact of how is trains so interesting that like I'm doing a poo pipe missing essential what are there in transit send a trained model end-to-end basically by fine tuning for the sentiment that's all good job that team are second place team group 51 is a similar approach using Bert to create a representation and then using a neural classifier on top of that so Congratulations to gu51 as well as score of .651 singer fine tuning on a 50 and then running inference to generally features three sentence and then feeding go into airport shuttle the strategy of psychopathy and playing on examples in the neutral class to do better on that we also wanted to point out and some other interesting approaches in addition to our top Duty shout-out group 9 on which is super well actually getting the same score as a top team but unfortunately can't hide will big off because actually use but but still interesting approach he wanted to put it up sleep formulated the task as a sequence to sequence sing like the strings containing the sentence invitations and I am as the input sequence and then the sentiment label as an output sequence of points equal and basically the only group and to think of wanting the problem that way and actually one of the only groups among the top teams to not use burger Elmo and then just using a two-layer bi-directional at 4 just shout out anyone who tried future engineering unfortunately you didn't make it into the top scoring systems and 1080 team because last year's top 2 stunt actually both use handle like bag-of-words extempore other this year award of the top 10 right on Tayport shows like so do you know the top 2 teams from last year and discourse or incomparable to the 02 they were for the binary task but we're doing this presented here place last year I doubt esteem used pre-processing of tradition and then character and Graham features of length of wolf and then just that managed to get a really really good score on this I just think AC like you know how what were doing last year with like and then you all and feature engineering head like I have been totally replaced with your wife nasa's computer created free train GU15 2lw also you like perafita engineering approach with like very very rich featuring unigrains by gran's indication words leveraging a sentiment lexicon also interaction that could definitely go and questions about representatives from the tap routines here I wonder whether those are the same sister the first team unused pytorch layer on a classifier 16 just used are classified as opposed to the I wonder if it's just the difference of optimise different 1351 it must be that someone just didn't use a shower girl Pacifier rather part of the pipework and can I buy tweet to finalise skateboarding code on the internet in some file somewhere in the current base someone has done some very careful pre-processing get properly into the vocabulary listen while I cried how my sides and everything maybe this will be my new salute thank you take off stuff first I just wanted to say that Amazon give us enough code give $50 codes for AWS to people who win the bake off so the first team got their codes and that winning team there will get codes in will keep that up it has to be just top scoring system numerically because we don't have that many xx some more teams might get extra credit than that but it is nice that we can reward the very top that I posted Bake Off it looks a lot like the procedure for big off wine in the sense that you need to download some data and then at your pretty much first bus times taller to mention that there is a pytorch and numpy tutorial on Friday team produce great notebooks for that I think it's just what's the dose after homepage the tap on the first day it's a bit revisionist but it's kind of like introduction to jupyter notebooks and now you have the stuff for numpy in Python that is really nice because you guys might be thinking about projects you want to do and for your projects you might want final thing I want to do here it's just kind of introduce homework 4 and the Bake Off and the reason I want to do that first timer for versatile to talk for going to talk about natural wood influence all week this week off is about naturally which influence about because the really good and Ally datasets are enormous what you have to do that for a Bake-Off because it would consume a lot of time in a lot of resource for homework for and Bake-Off for where can I do just a word level and tournament task this kind of nice because it's like a microcosm of the whole and my problem it just removes a lot of the overhead from processing large data play sleep and strange in some interesting ways so let me just start by introducing it because they're kind of are three intellectual things that I think I worth calling out the first the framing throughout this problem set the training instances are going to be pairs like hippo and memo develop labelled data sets that just tell you for his at the word level whether the first in tales of the sea binary task very likely because it's constraints are just words you have to make just a few decisions here kind of like in Bake Off one you need to decide how you're going to represent the pair of those largest items in your vocabulary sum up in some embedding space first of all and you can decide when invading space and they can be any in Venice Smart Move based on past Bake-Off sister you something like burger Alma to stop very the cockpit Boeing memo in an embedding space decide how you're going to represent them right so I think what I've suggested here is concatenate nothing else if you the song whenever you wanted to do represent there you're gonna want to fit some kind of mod worst thing would be the could have also very deep networks in representation entirely up to you what you want to do there I feel a baseline that's kind of a shallow deep learning model but you can do really whenever you want this is a microcosm of the hot and Ally prom because f2004 and Ally it's just that hippo in Melo can both be complete sentences are very complicated can we have three labels as your see four outputs this is the same kind of problem it just means that you can order it much more quickly because the data smaller and there's Leicester process that I want to point out here so you have comes with a notebook it's already in your data folder conditions for it I've called the first one Edge destroyed this means that the pairs training and test but also test which are released as part of the maker change level things are destroyed is that you could see a lot of the same vocabulary at training test I'm right till I keep my name training exactly that pair but you might have seen hippo paired with other words and name of paired with other words standard formulation of this problem but it's kind of easy to game you know the systems that result from this often look very good in terms of performance but you might doubt with it have actually learnt to generalize because it could be that they've just kind of learnt to triangulate within the space that they're in to do pretty well on the test that because some baseline switch on the tell you about later tend to be very high for this problem like if I give you only the second hypothesis made you get back how can I focus on that text instead we're going to focus on this condition that called word dis means is that for training Devon Hospital sordid if you saw a hippo a mammal and training see either of those words at test entails therefore that you won't see that you haven't seen 0z strictly harder testing your networks ability not only to do well that words it seemed but also to generalize from that into other parts of the leg one of the possibilities to the embedding is just a random interesting your intuition is exactly right if you have a random initial in bedding then you will produce random results on the not on the edge destroying you could do really well on edges new off Lounge means is that you need to initialise with a rip space and you'll see in the Bassline that I put down here did go out an interesting thing about this is that initialising with glove or I assume with bird and Elmo weirdest through just to show you in a bit more detail in this notebook what these two datasets are like and I kind of built up this argument here for awhile we might want to use the word destroyed condition set a baseline Disney's glove vectors here fundamental representation of the word you just do a glove look up in a way that you guys have done on earlier problem baseline relation function for the two words I just use concatenate classifier as I said before I use this towards shall and classifier from the course Repo show hidden representations and modest number of iterations you guys are probably accustomed to at this point we have this kind of the experiment rapper that should make it pretty easy for you to run experiments so you know model is your do you want to concatenate doesn't really the three points that you want variation why use train and down there for your play Sligo which is pretty good play the go through you in doing homework and the big off chicken build off of that bass line with a rich Annette representations or more and different ways to combine the two cancel for any finally because you can really develop my loss fair so you should be able to try a lot of anticipating stuff that I want to discuss for you but let me just walk through the Rational for the homework so the first one is called hypothesis only baseline as your see as we go through the material people discovered a few years ago you really well I'm many and I did a set if you throughout the 1st memo Paris throw outfit classification purely on the basis of the word mama just out with you I'll because I think it's telling interesting thing that mixes maybe some artifacts and he's datasets but also a pretty time is organised in what it means to be in an internment relationship it's not surprising that many more would be more likely to occur on the right in an internment pair than the last because of its general this fact about hypothesis only baseline restaurant in the Square for his as far as I know deserve this for the snli dataset where's the project in this course in 20 subsequently a number of discovered in quantified to hear is to just see how Strong my part this is only baseline is play Signal there is that giving his progress prayer 9nly problem then basically you this is only Bassline because random is not your baseline at all it's actually there to do that that's pretty straightforward ask you in the Spirit of thinking freely about this to exploring alternative representation to concatenation think about some interesting kind of apriori ways that you might combine the 2 represent what is interesting for an internment space patient or something like that pytorch tutorial night come in I'm going to ask you to do a little bit of work classing one of our pytorch class essentially another layer and a Dropout former regularization it's very easy to implement in Python the mathematical details here but I think you'll find that the important thing is to just use the pytorch documentation add a sequential mod spirit of this is to get you thinking about station for your net technique to avoid over started doing giving you some instructions and we're happy to talk with you about the best strategy for this kind of coding play as usual you do your original impose any rules on this actress on the internet as long as you do something original with the reason I can say that is because the Bake Off will be conducted on a held out data set that I have not all the development you want on the data we provide overfit to any questions about that before we dive into the regular Materia smog on Sunday Prague what we're going to focus on this week is the problem in its fur that's a wonderful problem I think you get something really deep about what we're trying to do in nlu there is an abundance of really interesting data the field has kind of stated on interesting methodology datasets it's pretty easy to crowdsource good ones I mean there are some issues that will discuss but by law production assistant you have multiple data sets that are in the same form I don't encourage having a default project for this course because I think it's interesting for you guys to get creative but if there were default project I would certainly say it lot of the concepts that we've find space of models that you can explore and these data say that there are many Special K free analyser cold RTE until bill came along she started calling a natural language inference lots of seminal work in the space of exploring how you might apply natural which algebra of inferential relation Villas part of the Resurgence of bombing was a student in this course same as for my stew history of this is that Richard socher came and gave a lecture on structured networks for sentiment analysis it was about the time that we were developing the Stanford cemetery it's not focused on nlu at the time for his research but that was an important moment for him go to work on an allied because I think he admired bills work like a cement really good data sets e-thot kind of stuck by the ones that were available play The Really admirable and ambitious thing of saying let's go analyse Chelsea's new deep learning point at point that's what kind of too small to really test Sly and the rest of history that the task has been renamed and nobody says RTE anymore and we have all these interesting data sets and Sam has been a real powerhouse and is responsible for bus snli in multi and Ally which is the two data sets that work time in the Spain I want to mention that hard on my promise right now stress testing all systems to see what they actually learning about cement a testbed for interesting neuromod bus on overview and I'm going to give you a bit of background on the places to dataset snli in multi ani live play do the kind of standard narrative for this course I'm going to show you some hand-built feature functions which I think are really good and interesting bass lines for the problem turn that down I'm going to show you that we have in the course Repo a nice framework for doing experiments on all these kind of lifestyle datasets going to start to look at deep learning is interesting as compared to sentiment because basically you have to text to work with the premise and hypothesis lots of interesting avenues for doing ma you can think about how to model the premise and hypothesis separately and how they relate sentence including bottles change model dream classes and then as an orthogonal development I'm going to show you some stuff on attention really Risen to prominence recently I think an Analyser machine translation were kind of motivating prams for attention in any those kind of to your systems to do find me we'll talk about Aaron else's these things will probably come on Wednesday will see how things go but I think again because of Annalise very rich grounding in linguistics you have some very interesting avenues for thinking about patterns in logical pattern 4 plaits show me have a lot of material so analyse that pie is your main module but for the home I work with his larger dataset an old box so the task and data one is sli in multi-channel that we have for working with second notebook is a whole lot of Madeline Mohammad former Bake Off for what's on at the core readings in my mind we posted a few more but I'll take the core readings are for minotaur 2015 that introduced sli and set up some initial base Tasha adult data seminal paper in terms of introducing attention my problem is showing you recommend some exaggerating so the Goldberg 2015 is that primer on deep learning model recommended in the last no I am particular Dagon at all is a very important Foundation of paper that more less introduce the task of art that's a nice presentation of this natural logical approach that build develop volume has the paper that introduced multi bassline understanding typical Analyser examples on a bit strategically just to give you a sense for the kind of variation that we see you would say that turtle and wingless contradicted label for that we just have yes no and tell me focus on today and on Wednesday you have three-way labels contradiction and tell me contradict each other City there because you can easily imagine a possible world in you can kind of already see that we're not going to be dealing with the notion of logical contradiction but rather something closer to common-sense inference based on Wirral antenatal moves that actually might be closer to just being internment fact because of the lexical relationship between dance and move Samsung you need to bring in there these two turtles are the same towel that's actually I touch you point when you get a naturalistic data and I'm the return to it but kind of what we're doing now is making some assumptions about eco reference across the premise and hope smooth see you again the question is like when you get back to be careful about it I'm not assuming that a turtle dance but rather fa turtle dance I think the flavour is much more like that play some qualifications because this is natural stick data as you can see is actually based on image captions and so there is some commitment to the premises being real it's like turtles Netflix play interpreting night with him like them turn on the lights turn volume up to 2 then you would be there probably be outlier in data if you're asked to do the annotation but you have a defensible point the logician in me wants to say another contradiction the first is not necessarily in Atonement unless I have proper existential closure of the two very you'll have to get used to don't say it's not so much checking that this is the way you might even think about the nli problem is as requires an understanding of commonsense reasoning and kind of world knowledge and that's fuelling the system doing well at really difficult you were the magical world at least something doesn't have to be the same turn off downstairs some eternal that yeah that's play against a turtle 8 doesn't been neutral again like that sort of a logical fact walk no Turtles move does contradict each other classic example from the species Indian refuse to move without boujee steamed indents without paints who was highlighting am kind of linguistic complexity of this reasoning task as it might take you a minute to see that those earning tell me a relationship but also that as part of this you might have to sort some stuff like figuring out that James Byron Dean and James Dean are the same that you might need a lot of running out of a particular car or less logically we don't need much common sense reasoning play different for the next one Mitsubishi motor corp new vehicle sales in the US fell 44-46 present in June fishing sales roles for 34 that sister contradict to make some assumptions about the domain of these claims biological seem like that definitely involves commonsense reasoning Acme corporation reported that its CEO resign entail no resign doesn't because companies can report anything my mum report false play speaking if we were Samantha since we would say that this report has no entitlements with respect to the compliment there at NEC or resign it's a kind of common sense saying given that we know though a reliable source kind of information in fluent element or annotators might be incurred phone Jim finally here because it is the same person because take the example B George the other specified by yeah so in the back of your mind you should be thinking that there's a practical side of this which is that we want systems that could just read through newspaper text for example and figure out what the entitlements are with the Logical relationships are and in that case this isn't a problem we can ignore rather we need systems that are sensitive to that level of distinction and not know how many George Bush's there are at some implicit resolve these two the same uncertainty that you guys are feeling this is wonderful you sound like linguists this Is Us reaction to this problem is usually none of these things have any of these logical relationships it's only troll because I can think of Edge cases where it falls apart I have to just Retreat to the ideas trying to get a hold of what us a reader from this how do you go back finally distinctions between logical entailment and commonsense reasoning and abduction and all these other notions that are in play here turn off summarises this and I would refer you to this tree of articles this is a kind of backing for 30 Stanford people at one time they will stand for how much non logical reasoning we should allowed to creep into this problem I think I won't resolve it but it's an interesting discussion at the top of this kind of summarises the view that I'm trying to convey which is premise justifying inference to the hypothesis common sense assumptions about the world and the speaker and our intentions and self-worth that's the heart of it and any other aspects of this problem is that is meant to focus and kind of local influence with just premise and hypothesis emphasis on capturing variability and use expressions like different ways that you might refer to people is there the same concept might be articulated and soft what happens if you try to be strict about this and say that you're going to look only at logical Intel it's kind of small and really reliable lexico relationship turn the linguist come in and say wait a sec ambiguities are causing me even to doubt the thing that you thought was a salad down and done it all kind of falls apart that's the story I tell myself about why actually makes does a common sense reasoning I might like logicalis important paper about your dragon and colleagues that introduced RTE when we now caught in a lie the teams that major in France multiple applications can indeed be cast in terms of texture and tell me we hypothesize that text you and tell that recognition is a suitable generic task for evaluating and comparing applied semantic inference mod search efforts can promote the development of entailment recognition engine can you spell generic modules across application nice articulation of a dream for an Elisa test if we can get our system basic but pervasive logical these they have would apply in lots of them resonates with me is a cement assist cause I kind of think of entailment and contradiction as being really Central to how we think about language how are you reason with play some sort of this play me speaking people that is like language right your king in this framing we're just to say like you're changing prawns in a light maybe they turn on this understanding or reformulation we can actually reduced them to an analyte ask and I'll be wonderful cause if we could then the models I'm showing you today would have a Lapwing precisely what you mean but I'm really open-minded where do some of the sea from Dragons article phrase detection is a well-known test it's just the task of saying is the tour paraphrase equal in sun cancel van common sense reason conversation would say does the input tax the summary which would be like saying we want at least as a constraint on summarisation that the courgette information retrieval could go in the reverse reaction you could be you could formulate that a saying that you would like return query onli actually answering is one that you can basically reduce the saying that you want the answer to until the question Rillington massage the question into a declared of former little bit so for example if you were willing to replace in English all the wh words with indefinite so that who left became someone less straightforward in a lighthouse when you're really saying you would Sandy left to entail some your notion of other tasks out there that you could try to reduce to the same form naturally inspiring idea that we really gone kind of down to the metal here in terms of understanding reasoning and language which is applicable to lots of different play background this is sort of interesting this is a landscape of different approaches that people have taken to the nli I have logic AND theorem proving doesn't have the earliest approaches inspired by logic AND linguistics put them very high on the depth of representations access because of course they give you very high fidelity pictures of the data along the x-axis I have effectiveness which is kind of whole lot of data does this model do good and interesting things with in a comprehensive way get me limitation of logic AND improving systems is because they tend to be handcrafted the y-axis but low on the develop this natural logic approach which is kind of logically but because the logic is defined over natural in which other answers you have a lot of power to apply it to larger datasets and so it kind of moved us to the right along the x x song sacrifice in terms of the depth of represent can I put symantec grass which was another kind of handcrafted approach that depended a lot on no the alignment between the premise and hypothesis more effective with some sacrifices in terms of interpret ability and self here at the top I have clever handbuilt feature as exemplified in this production system called the excitement open platform good kind of classical system for solving the inner ear follower I put any round variations were going to look at some of those today that point you're very effective but models can be very powerful but very low in terms of them offering a deep understanding of the analyte problem basically just have a lot of superficial associate interesting it happens I put deep learning 2015 2015 when we really introduce this task into the course proper tell us we're not really a slam dunk this was kind of before or just at the time ethanol I was being introduced the systems were being beaten by handcrafted ones it's safe to say that by now in 2019 what's have overtaken the hand experiments that you see my notebooks and sell for easy for you to be the very strong features Bassline using somebody playing switch on the Aladdin dataset architecture influenced St disinteresting historically because that is an attempt to have a dataset where basically you have been confined at least for the parts that were talking about to logical inference prob but I think still really interesting as a kind of test set stress test your system for exam free sly the review datasets likes emyvale 2013 2014 that was the 6th to separate actually distributed as part of the dataset of for the and put it into the Soi format so it's there if you want to play around I have small and had some idiosyncrasy for sly after SLR you'll have a flourishing of datasets better in the same also med analyse for medical store problem very rich vocabulary personalise multilingual in allied datasets haven't arrived from multi ani by a group of face that rhyme with inference collection is again a really large scale from very diverse set of problems from the group Hopper how is SNL I like guy from science questions in multiple choice questions and also webtech so that's also really challenge to adjust the colour of get you thinking about this in the morning either dog and so question answer corpus can be thought of as a kind of nli problem if you want to put it into that database is just a simple version of Anna library you're trying to get just one relation this is a newer development the glue benchmark was released by Sam Bowman and his group it is analyte asks at 5 other tasks that aren't strictly in my problems but I meant to kind of test the dog and assumption that if you have a good and a licence applications for transfer to other task have a lot of people have been working with there's a lot of progress right now on the glue bench very ambitious final project you could tackle that in whole or in part all the different label sets that have been explored over the years for better or worse with kind of consolidated on this three-way one because of Arsenal or that there were four way versions conversion cycle explore in the home mention this before because this is relevant for the homework I have you testing this hypothesis yeah so when did castleman in this in the scores for his final project observed the strength of hypothesis only baselines substantiated by a bunch of other groups including one by Sam Bowman why this whole b any traction on hypothesis in a Primus hypothesis initially that sounds very worrying right because it sounds like there's something deeply problematic about the data that would allow you to leave off so much example and still do well can something to worry about but I'm not myself so worry because I think it has a principled explanation that is more less grounded in the problem itself summarise that in this hearsay to be premises in and tell me easy for me to imagine if I think of the word not hierarchies the word it is basically a huge hierarchy of internment relations for nouns at the bottom of the hierarchy are very specific things like puppy I must everything else else in the hierarchy on at the object so if I if you show me just the word I have a very good gas about whether it's in the premise or the hypothesis conversely something very general like a memo inject or dog very likely to be on the right in that pair of just because words from the vocabulary it's unlikely to be above it because so few words this extends also to contradiction so I think that specific things are more likely to lead to contradiction by the rules of the snli game because questions about event coreference and sell for that if you describe something very specific in these times it probably excludes most other claim specific U2 contradiction on either side the results of these three observations meaning of Lord of the strength of hypothesis only baselines believe that argument means that this isn't really something we're going to be able to naturally Factor out put the internment IRA you want assistance to learn Norton symantec fact if you decided for example that you would get rid of it by balancing your dataset so that every word had equal likelihood of appearing as a premise or hypothesis hypothesis only based go to me that the resulting systems will be solving the problem as we encounter in the world reason my response to this is just to say have in the top of our results table hypothesis only baseline so that we know the starting point for art anyone want to push back on it find if you believe my generalizations are not to be better than round information on in just at work there but I have exported to match my coaching like it released good timing let's dive into what these datasets are like so we're going to look at snli in Malta in July time in Multiple datasets like this at this summary it was released by Bowman at all 2015 all of the premises captions from the Flickr 30K dataset important to keep in mind because image captions are kind of unusual bits of language some means that all of these things in at some level for people who produce them were grounded in a specific image escape the data officers were all written by Crowd work very large cross-dressing task because 500000 hypotheses were written by people and then parts of the dataset were evaluated by other human I should know that some of the sentences reflect stereotypes this is a nice paper from a Hopkins rugby expect people who are doing lots of crowd work kind of relied on some still producing contradiction entertainment Paris that is the end of relying on problematic stereotypes we would like to eliminate from these data sets because we don't want to prepare 28 this I can just say that you should be aware of 553 and examples 10K Dave and 10K test information about the mean length of the examples types of clauses sentences some of them are noun phrases fairly large vocabulary on what 40000 words it has a smaller subset of about 60000 examples that were validated by for additional entity confidence that the label is and that went rather well there was a very high level of agreement for play down here the leaderboard the overwhelming number of papers on it now digest and I think we passed the date where you would just like hope that you were the highest I will think about it somewhere more flexibly so Sam already divided it into a few different model classes where you want to slide Extinction that has emerged it's just that your model is an answer SMI in particular we've entered a phase where ensemble models are just doing that game if you want to just presume raw performance different thing you're doing on in the modelling sense than picking a single model that you think reflection intuition about the data or the prom itself and pursuing it is far as can you answer to a about how you're doing with your screenshot of the crowdsourcing methods that were used it's kind of small it's here just as documentation cloudworkers we couldn't just tell them hey construct a sentence entails that the prime minister's that kind of specialised logical notes found a way of framing it so that it was pretty clear up we're after a but more not if you want to do a deep dive on what the data actually mean 10.4 how the examples were construct some actual examples because these are the ones that are given in the paper text the uniform of a figure in Southeast Asian countries that remember that was a the caption of an image in the country dictionary that the work that came up with was and the man asleep consequent validation phase all 5 people agree that that was a contradict pretty straightforward you can kind of see that can't it looks anyway like contradiction is very consistent good morning you want to specially around what the Neutral Academy places that I picked again just to reintroduce this idea that it's not so obvious however define actual examples from the corpus but I think they by the standards that were doc about cycling the Pacific Ocean and a boat sank in the Atlantic Ocean with the label this contradict those two things could happen together logical sense in which seas are contradictory assume that what we're doing is talking about a single image from the image maker 30K dataset likely that this contradict each other that only one of them could possibly be true and that's Ascension which this is a country extreme case that same came up as a Ruth Bader Ginsburg was appointed in supreme court and I had a sandwich for lunch today by this logic he would say that those were contradict because they couldn't both be true trueview my be describing the same event in sun at the mercy of the day that we have all this is worth keeping turn the other dataset I want introduces multi and Ally games in 2018 similar in terms of its design with some added new one so first from a bunch of different genres fiction government reports asleep website the switch word corpus and some travel guides went for the dowsett you have one condition where you test on the same genres I'm completely different genre the 9/11 report play switches America's fundraising letters non-fiction call idea because you might think that away again of stress testing your system are forcing it to go outside of its training experience very large has the same kind of additional annotations for validation Christians that I think to avoid kind of a lot of hill climbing on the test set that is released only as a cowgirl increase the number of times that people can evaluate check out the project page I think it links to leaderboard captions for the images have been back money in for me picture of a soccer game happening in this is like a man kicks the ball something happening in the same photo so basically like how comprehensive can we can I think you can't can on anything I think image caption things that reflect white humans regardless alien they have their own interesting biases open summer temps like it central with a visual Genome Project to do meaning of a sword that's not so naturalistic to really comprehensive the label what's in the image what was done pretty much like a 30K it's much more natural an interesting social and linguistic space top of it in an unusual there's an opportunity here which is that you might leverage the actual images this is been done we are not at all that you solve the problem by reasoning on only about the premise the image a representation of the image of self really interesting thing about the multi in allied dataset so same in the group where's the data set a bunch of annotation properties of the individual examples example they just labels and some as being active passive interesting as you expect that to be kind of meaning preserve open question whether your system can be sensitive to that kind of Transformers what's the categories predicate like belief or claim assay conditionals with a cold references crucial play systems just ignore where can I find hinge on the nature of those call referential long sentence does it contain modals and occasion these two things be called paraphrases are there quantifiers in the sentence these are kind of quantifying the difficulty of the play nice you'll see this letter when we do era now so you can use these categories for a kind of out-of-the-box error analysis you can ask different systems which of these categories are they doing well on and where are they Ariel what's around this out to see you know about it this is also in those notes code for dealing with these datasets sly train Reading everyday might rain reader and then you have the two decorators matched in should be good like easy ways for you to interact with the data and feed into you'll have to get used to record example has the underlying data has not only the strings if it was multiply annotated all the decisions the Editors made binary and non-binary parcels of the examples which are automatic courses that come from Stanford corenlp pack but you're a good person you access to very rich restructures if you want never going to how you do matters example it's there for you to download a bit of additional code that just shows you the interface that we provided for dealing with the annotated talk to you making you so because I think it's really exciting to sister by looking at how it doesn't different classes were very easy for us any questions about the data sets at any level before we started dive into the modelling process embark on a familiar plat it will do is look at some handout feature functions and some kind of classical when you're mad where is the fine just two very simple baseline you see a lot in the literature the first is what I've called word overlap five creating a feature hear every word that occurs both in the premise and iPod I give you pretty sparse mod because you're really looking at only the overlapping pasta with word cross product does play the future words that occurs in the premise my partner the four crosspatrick incentive you think about the rest two sets of words you look at every pair of that you can draw from that pair of sets leads to really enormous featurespace a model with this feature function on all of SML you end up with well over a million and a half Dimension of very sparse feature space that's like if I have Toby is a dog is a big dog overlap thing just keeps track of Toby dog isn't big word of cross products increase this enormous list of all the pairs that I can form from those two baseline things hear something else I hope class of features that you might think about that could come from word that so before word that is a kind of very high fidelity handcrafted resource nation about Word level relationships in particular fish and chips and some contradiction information that is in the form of auntie down here it's just give you a little bit of code that shows you how you might create feature function that hierarchy so is a quick example if I just look up puppy since dog paw the sense of the word puppy is kind of ambiguous and word as capture just the first of those since that's then you get a kind of work and serve a prospective because the first the most free their data lots of methods you could use Cippenham and sulphur how to create a word that feature hypernyms in happenings and I'll just show you an example I have the puppy moved in the dog dance turning features would give me a feature for puppy and dog features would give me moved in can you could do with this cold or you could do this four letter word that relationships and I think are about pretty interesting so the spaces Mexico entitlement and contradiction in father ideas so you can do lots more stuff with wordnet I think about some edit distance features between the premise and hypothesis maybe at the word level I should you word overlap but maybe it even more natural to think about word differences may be there much more of the story is told by is in the hypothesis but not in the premise for the ref environment based features that we came to an older idea from an Ally that it's kind of important to figure out how the to the premise and hypothesis relate to each other negation of course I think it's worthwhile having separate features that would key intimidation as well as other lexical classes because all of the stuff is really deeply affect being an attitude in modals and hedges and all that stuff a lot of stuff then I think it's relevant for Centre because of the weight modulate claims that we relationship suppose they are not in word maybe even the effort of hand coding them add named entity features that's kind of like James Dean going to be exhausted rather just to get you thinking about different ways that you could key until lexical information constructional information and also were all knowledge all that stuff should be brought reframe D'Angelo Prague that is that the pattern that's repeating for all this coat stuff it's Hannah standard language for thinking about running experiments in a productionize way bunch of code for that let me just walk through it at a high level just to show you how easy this can be Peugeot all new pointer repository to the That s My Home Australia have defined word overlap fires my future Funk have these rappers outfits off you can as part of the experiment Piper location and filler with the parameters into other things better part of model setup different from the Stanford sentiment treebank we have these reader object setup train reader 10 and the reason for that is they have this extra argument called sample percentage send percentage 0.1 really important for development because if you leave that office a certain percentage equals 9 going to be run on the 4th 550k examples Bristol slow to a crawl because every on way that you would develop on the status small subsets which are randomly select do you like your system has stabilized expand out to a larger uncover the problem set train reader and then it's going to look a lot like the SST right so you have the train reader argue word overlap 5 and fit softmax Swiss Army knife you have a lot of other options but those can be left off switch on takes to run an four basic experiment there is a dictionary stores all the information about your experiment for reproducibility and error what's the weather things that you did I would encourage so first camera selection on train subsets again so unlike SST where smaller that you can kind of do a lot of experiments and a pretty manageable level snli every experiment is a major investment you just won't make that much progress if you're not thinking about how to do smaller stuff that won't form the full picture your didn't suggest you two ways that you might do that when it comes to hyperparameters take me to set the circus will be on train subsets so as my home as usual word overlap five will be my text with cross validation that's going to look at a lot of different values of the regularization strength c how long to type so L1 and L2 so that's a pretty big greater premises you have to have a lot of models especially with CV at 3 there tomatoes and forget if you do it on the full dataset it will just take too long what should my take is to settle hyperparameters based on a small subset of the day that you're done at 10% get printed arceaux this model Josie of one and penalty of L2 living made that selection forward now same feature function fit softmax classifier with preselected parameters where I just had codeine I selected for my earlier xxx go for play you got some of the benefits of exploring the hyperparameters comments or concerns you want to kitchen trolley play random but because the day was that is almost exactly bad subset out erasing an interesting the approach I just sketched which is send how do I know how represented the visit of the truthful dataset and in particular space that you form and that tiny subset might be very different from the one that you get from the full training set example that 90 pulley affect how much regularization you want I think that's a weakness of the current of just like that that one a method that you might not that weakness but maybe selection with a few in a ratio is machine learning model number of epochs of train 40 pack might be manageable but if it takes a thousand for your model to convert the real cast he might take would be as I've done here cross-validation small editor find the perimeter for the model and fitting there it's maxiter there for logistic regression and set it to a very small valley the four grid search that I wanted to do before it's much less time even if I do it over the whole dataset as I'm doing down here the iteration number of Solo get a report down here L1 position primary not L2 slightly better play that forward implicit assumption you making there is that the future for training a puppy surpass it was really bad early on is not going to send her emerges as the hero of the story maitre once bad always bad and conversely the ones that are good right out of the gate are probably the ones that will be good even if you run them offering you a guarantee because I think no such guarantee can be off good compromise should.you have on training time or I'm computer resource mix and match these because both of these things you things are very on the entire day staying here and then we might wrap up I just wanted to show you how easy it is in the framework to set up and run a hypothesis only and I think it's pretty obvious there is the future function hypothesis only unigrams fi is T1 and T2 it's only Auntie 081 use fit softmax classifier with preselected params experiment 65.3 F1 race that is very play spine which will be like 3334 you have powerful this can be so if your mother when you feel like all that's wonderful compared to random contextualizing in this way is it worth checking to see how much information is in the premise actually is compared to random I just wanted to show you just in case you didn't know this site has all the any classifier mod models that just make use of the like label distribution I just did the divergent thing actually using one of those 3 points this is only baseline is a very sore that baseline doing all the exact same operation as our model but only on the secondary or should it be just when is it given the mobile that you're developing what is the true Nature of the hypothesis only base for example if I picked wordcross product feature function that undefined if I'm working only at the hypothesis I like well the unit being like is a tough thing to say if you've especially if you do if you find a lot of features that make you supposed to turn off features on light just to fall too contemporary models I think these questions are typically more straight Robert the diving into those let me leave this for next time so next time we're going to look at sentence including mod representation for premise and hypothesis same ones which more let's just put the two together single typically in rnn of sometime attention mechanisms in do some erinalice and if we get through that as usual decent will leave some time for you to just cold 4020 mm 