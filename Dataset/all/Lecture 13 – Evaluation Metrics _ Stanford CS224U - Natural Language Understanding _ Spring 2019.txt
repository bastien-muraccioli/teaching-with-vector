term for the day the shop that stopping method have much left but I think they're really interesting top I have a notebook on m go on go through the entire no but there's a lot of material there but I thought I would do is just got through the section that's on kind precision and recall because I think if I do that not only are those probably going to be the most used metrics for projects in this course but also you can try and get a feel for level finger I'm trying to do with just to get you to think Richard way metrics are doing what they're not doing where they might have faults in self-worth this is going to take over Atticus has a very exciting mini lecture on generalization and adversarial testing as I've called it here take whatever time it takes and then just wrap up and if there's a little bit of time that's where you guys can just start to work on your project chat with the members of the teaching team here and then next week representations I'm going to try to make you what's the uses of bird and Elmo in the transformer next Wednesday will have a panel discussion on nlu an Indesit memorial Day and then we kind of wrap up with some other loose ends again in the Spirit of trying to introduce topics and content that will help push your project for rude that's basically what we're all about From Here On Out for discussion I really appreciated all the comments that people made last time there was a bunch of stuff that never thought about for the thought have any questions or concerns occurred to you about the things we covered which is kind all of this stuff we covered a lot of ground position cross-validation baselines hyperparameter optimisation and classifier comparison Google has there been busy which is there hyperparameter search what they use for under 1 l to China's help with the hyperparameter tune turn the pretty or me to these big companies that have so much money that they can just passed away on hyperparameters is definitely an element of blasting away impressionist degree for sli which is a leaderboard a kind of follow closely there is a split UC sometimes that simple models with relatively few parameters have really high score strongly correlated with them by them originating in companies like Google really wide calf hyperparameter validation to find settings of the model that were really really good finding articulate intricate models that are guided along by people's human intuition about the data with less hyperparameter optimisation because they are guided by into it Jackster position don't know much about Google visit like product or something they just use internally what's the paper up stop something in 2000 applicable no I mean I remember it from that called idea from the Healthcare paper with a spent 200000 hours optimisation using visio but I guess I don't really know much about what it's all about 50 Lisburn by making resources available remind assessing models without convergence which you could think of it another way of calling out nearest Carrefour 20114 the scores when everybody was using just a quick question issues really arose because those models me might not even notice it but they do have thresholds they do run for a number of it convert write to you just kind of don't think about the fact that this is one of your hyperparameters possibly before whenever threshold was set on the number of epochs talk about this too much until recently but now with neural networks cannabis centre Stage who has taken centre Stage because these models really convert different rates between runs side of this is that convergence is not really what you want because what you might implicit we be doing with kind of early stopping is some regular reserve the benefit in your solution PowerPoint here it's just that the performance of these models on test data is often heavily dependent on when you decided to stop and how it related to conversion my hair is going away sometimes in Marlow with a low final error turns out to be great and sometimes it turns out turns out to be worse slither.io that's what I was just trying out with this now regularize avoiding overfitting to the training data is a whole new set of problems right because it's at least one more hyperparameter b a very complicated want to deal with because about having to run systems to varying degrees in directing with Ali Raza hyperparameters and of course the more they need to run the more expensive this all gets in some generalized sense passive responses that you can have to this so grouped under the heading of incremental dubset testing play the idea here is that as your model is training comentale you say every 100 parkour every 10 for every 1000 whatever you can afford running a valuation on a dab set see our performance is doing I just want to mention that all the pytorch models for this course that are included in your Repo so if you give fate an optional xdev and Devastator set of argument they will run evaluations on Excel collect information and make use of it you could show a learning curve you could use it to make a decision you want to stop say that in my view at least given my understanding tensorflow is head of the curve in all of this the tensorflow classes for this course piggyback on the new estimator model it's capabilities and they have lots of sophisticated things in terms of rewriting your model to disk rewind back to an earlier stage of really good performance it's easy to do that no pytorch models are not currently set up for that kind of thing all of this is just that you have your dad's see how you doing on it and use that to inform testset evaluate exercising again that absolutely under no circumstances should you do this with your test set that would just be like an incredibly informative of cheating stop opportunistically when you saw the best perform that is 4 a lot of this for here that you know the lower rate doesn't always correspond to good performance here's an actual pair of learning curves I have macro F1 here and the number of iterations this phone of the model performance got really good and then kind of leveled off over here error vs iterations the area just kept getting lower and lower this was arguably having a negative effect or no effect at all I went way down start over I mean I guess it's a wild world then you might see that versions of this might be something that you actually use this kind of debugging so for example if your learning curve sorry your hair doesn't look approximately like this something wrong in the optimisation setting that's causing it to oscillate or even like have the larger where's me of that corresponds to good behaviour and it over here on this left-hand what's the nearest just you might observe both of these things that's another advantage of tensorflow is that tensorboard monitor your model is due get both and don't take one as a proxy for the other that's kind of best response to all of this the weather is just describing things before we were still kind of in the motive saying I know that my model various bioparc I know that that affects test that performance but still what I want to do for you is report one single number on a test set open minded Lee about this that's kind of misleading because what I'm telling you is that there's lots of variation along with learning curves as you as you train on more data first response is to accept that incremental performance Plus here we should be assessing these models much more information about the variation that you actually observe actually new ways of arguing for your model as I show you in a second the Indie planning in principle if you think about the theory of these models should be capable of learning anything really thinking about is how efficiently they can learn or have stable their learning patterns are in so forth and that's probably ultimately were your real argument lies an example of this from my own work from a paper that I did last year with Nick Dingwall background here just quickly for this model called mittens warm start for glove sete mittens are warmer than gloves really the picture is that what you could do is start from my collectors that you had downloaded from the web glove on them retrofitting learning objectives for a specialised dolce we do this kind of updating for a bunch of different domain Louise show that you can do well updating we actually evaluate them is kind of extrinsic so devaluation is the inputs to various kinds of other machine learning models and in this case prediction task for Healthcare talking about people being diagnosed with different diseases what is an rnn that for each token is predicting whether it's concern for a diagnosis or ruling out a diagnosis or whether it's a positive diagnosis for the disease in kind of Clinical tax the kind of thing that you've seen her in an electronic health record fundamentally here is that we didn't do a lot of hyperparameter tuning was that would be expensive first of all for these ornaments because I need to run for a long time for convergence the two models of my main competition is like mittens vs clinical xcloud some gloves just trained on the text me have available and not start from the downloaded vectors that competition in all we really care about is the Delta between see whether mittens is better what is the Zion Ensor kind of the same like in your and the inputs are scaled and similar ways mittens and he's very slavers of blood where does an argument to just set the hyperparameters like the dimensionality to the network and its activation function and salt did observe variation in how these models would learn because of renovation in there Toolstation in Southport it was important to show these learning curve hereby category Texas is the number of training epoch Texas F1 I like this really for picture bark I can say if you look at Macro ever modest argument overall a bit faster show me a Ford to do like a 1000 training epoch start for this domain is actually help time if you can run to 10000 the difference between those two models has disappeared Cortex glass also see the can you run these things have kind of leveled out tomatoes are better than generic love factors or random initialisation like that's the yellow in the emigre full picture but it gets pretty nuanced if you look at these different category so like I'm not sure that model really that argument really holds for concern for example there it's very hard to see a difference and the blue stronger for positive a period Tyler you know kind of like in early phases of learning meetings is good but in the fullness of time a lot of the differences disappear at least between those two competitor very different picture I would be giving you a very different story here if we had decided to just performance for each one of these categories are the best point of performance for macro average in right there my look like a stronger cut and dry argument in favour of me that would be a bit opportunity I think things like open and honest reporting and how these models are doing this is like I guess I'm saying that I would love to see plaits like this in paper as you all cough and a lot of pressure operation Overlord in terms of Culture to give a single macro F1 number something like that but if you feel there's a room for this kind of Monuments thing then go for it you have names for support quite hear St Aldates different here is the random initialization and other kind of random things about how these models are learning that brings us to this other point here the role of random premier initialisation right so can you set up the weights for your model in the setting initialisation in these kind of convex optimisation setting might be determining a lot about how the model ultimately fares I will say that even simpler models can be impacted by this if there's multiple optimal Solutions then they could get stored in One Direction or Another curling results from a couple of years ago is this paper that kind analysis of recently proposed neuro which models finding of this paper is that if you ran me sis differences between an essentially disappeared because the way in which they have been randomly initialised was having a big impact outcome for the martyr put a damper this paper on the feeling that recent proposals were vastly better than earlier once because washed out the differences were small or not call tracing two different initialization disturbing also for sting different initialisation testocles related same here which is kind of the extreme version of this heading of catastrophic failure so you you have an unlucky initialisation of your mum it's not only bad performance but like effectively 0 performance is there the smart the good and open thing to do is to report the number of times that they had how is that your paper so that you don't destroy the results table but it is important thing to be forthcoming about how often this was happening and how much it was shaped means and confidence intervals that you report there's going to be publishing in a results were there not eating student they are you think there is this kind of like just random hill climbing that happens because people will be doing every time they attempt one of these papers and occasionally people get someone else is that what you think is happening concern I think it is happening I think it's a concern the story that supposed to match on his office but that's kind of the story of this paper here all of it brings me to the idea that we should be reporting more like this that show us the impact of all of these different choice seeded on a single summary number stating the degree to terrible progress on his problem ok going to bring up the snli leaderboard there so many systems that have been entered onto that leaderboard now who the thing of study and saying ok broadly speak models are doing well and I cleared there's a clear answer is it some kind of ensemble of deep learning modular PC individual numbers that have been overstated friend is clear to substantiate this a little bit in The Notebook I didn't reproduce it here but problem in in neural networks the xor problem logical connectives to the cases where P&Q have different truth values otherwise zero and it's famously not a problem that a linear classifier can learn only if we're you won one of the original arguments by the Deep learning founders you can learn Excel with network it's Colin you definitely can theoretically do this thing that you can't do it when your model without adding a budget interaction but in reality feed forward network state of 10 times and entirely owing to the fact that this is a small problem and so the initialization really matter get a feel for yourself which this is actually solve empirically anterior this kind of summarises what I've been saying so report scores for multiple complete runs with different randomly chosen initialization Priors that variation with Confidence intervals for statistical tests a return to this pragmatic note that I keep sounding here so arguably these observations are incompatible with things like mcnemar's test which depend on One run true bounce that against the fact that if your system takes 2 weeks to train now you have the opportunity to do just a few runs Neymar stats might be better than nothing in terms of informing us about I think I can summarise everything that we did last time in this time Curacao fishing should be based around a few systems that are related in ways that illuminate your hypotheses play with the best models baseline staff that we talked about volume excess should be given its best chance to shine and that's like hyperparameter tuning and all that stuff make the strong argument in this way we need to be realistic about how many experiments we can actually afford to save dollars or time or resources test Search to play no role whatsoever in optimisation of model selection this is a note that we stand it a lot because we did all those Bake Off ensure that this is to have the test set locked away until the final batch of experiments that will be reported in the paper thing that you can do if you don't have a test set play Dad cross validation anywhere fixed barrier examples cross validation in report the mean try to make a case that your kind of hands off about all of that you setup your models with a lot of hyperparameter tune hands off and then just report the final honest with yourself about the extent and specially advantage in the model that your favourite you kind of want this to be report this resume very careful extremal comparisons in Multiple runs on the same splits especially if you're doing deep learning stuff Mario can perform in very different ways on the same data depending on optimisation and moon sunspots and whatever else is happening in the unit any new questions for you comment speaking 1 m point architecture azure high level Argos is it necessarily bad that people get lucky and do happen to do Miranda milk successful it's not only bad part yeah like getting lucky could be something there is a setting in which you then at least I was worried about 2 is like do that special stuff only for the model that I want advocate for that's where you're stacking the deck in your favour hundreds of times what's the time I model losses and I report the situation in which my model happened to probably know in your heart that's not an honest report happening out there in the world and I guess I was saying like we can do is a kind of push the community with our own choice reporting mean and confidence intervals on all of those runs value from those that get when you get lucky good that actually recitation like that what means word for trusting that's a great point and I feel like as we get better and getting analytic insights settings where you got lucky will provide more and more intelligent because you could what if you did report standard thing with this kind of initialisation settings where the ones that deviated the most difference between stucco properties and what have you then took that on Western in a systematic way and found it reproduced patient scheme named after you because then you get your name in all these different packages electric stinging but keep asking questions in office hours it's great to be discussing older something about metric so this is a massively long notebook that's why I say like let's talk about basically just the first part of this classifier metric saying because that would be enough to give you a feel for my thinking about disgusting girl if you're doing a problem with regression or sequence prediction later sections here intern fixing just the way I'm going to today and talk about their trade-offs you are so different evaluation metrics in code different Valley different values in the centre of Lakewood to be value of a system different diocese and weaknesses no none of these metrics is perfect because all them encoding different values and values can vary choose your metrics carefully and motivate destroys when writing up and presenting it I would love to see passengers wear if you even if you doing something that's the default for Prague articulate why it's the choice that you reviews some of the most prominent metrics in NLP find number also articulate what values they encode and what weaknesses they have relationships between them because one way in which this can be kind of overwhelming is that basically the same metric is described and given different differences don't mean much work you should not feel confined to the metrics in this notebook right per my first item um2 motivate new m snooze for existing metrics depending on what your goals are if you're working on an established water pressure from readers and referee have already been adopted for the pro Communities within an LP that have I totally said Patrick no matter how problematic turn the strap with feeling like you know there's a problem asterisk order argue against those norms and motivating can you paper one a report the Old metrics as a kind of foundation and then talk about why a new metre another one for your prob I cannot really work in this area but for language modelling I feel like everybody is in the state with reporting for plex everyone does it cause everyone expect that everyone else will expect that they do it but nobody believes it play everything I say just by way of high level is the psychic model evaluation usage guide here is great it has lots of notes about how to make responsible use of the metrics they provided and they provided lots of them in The Notebook that I'm going through here I've hidden a lot of this but I just need to find the metrics just so that you can see how they advice to you would be to use the sidekick ones tested and tend to have more options just do this classifier metric thing in the kind of linger over this is kind of nice because this is the star 100 the most and feel like you are ready fully understand and the my ideal will be that I wanna problem at times confusion Matrix gives a complete comparison of how the Observer almost here relate to each other small simple sentiment problem posnoc neutral Poznan where is the stand that made the Scott 15 positive examples play trollcraft off-diagonal is saying like 410 of the true positive cases the model predict negative 100 of the true positive cases the model predict seen it before simple one remember about these things classifier probably did not predict into this space should a probability distribution over those three labels puzzle egg and neutral struct this table does the Thresher play that I'm going to free example pick the highest of the probability values and just say that is the true label what shape does spinal picture here of course I meaning for thing to have done Pacifier say a predictive a text from to make it easy think of a binary prob call rugby 50% classifier reliably predict for the zero label 0 to 20% 2244 the positively confusion matrix would look like you about everything wrong the different threshold that your mother would be perfect keep that in mind that this can happen especially with very imbalanced classes that the classifier just never predicts high values for thing discriminate going to this today but metrics like average precision and precision recall Curve they can expose exactly that kind of behaviour in your classifier and give you a really strong argument that you're doing well confusion matrix would look bad charities that we might care about the food distribution and that's been completely hidden by this confusion matrix but this is nonetheless the basis for the metrics that I want to go over with you now we are all probably think of it some level when we think about assessing how good a system is at least before you enter this field you will be the right mate the sum of the correct predictions divided by The Sum Of All predict same confusion matrix only just highlighted that I am this is doing is something those values and dividing by the total of all the values in the table are you getting accuracy it's used for 10 just know the bounce I did this for all the match here obviously 02 one the best value is encoded by accuracy can you that kind of core value that we have 4 classifiers that is how often are correct station as I show you as the accuracy of a classifier on a test that will be negatively correlated with the negative cross entropy loss which is a common lost for classifiers in an important sense of your classifier is probably optimising accuracy or rather an inverse value of x how to make a very natural fit I want to return to that when we think about how this is all interacting with labour accuracy as the metric it is what your classifier is doing save accuracy it does not give her class metric for multi-class problem make my bed kind of new ones important thing is the accuracy just completely fails to control for size and balances in the class example consider this variant here Pacifier EX1 only ever predict neutral neutral is a large category really good classifier from the point of view of accuracy if accuracy is 87 what for this confusion matrix vs 814 the one before feel like this is a better model actually discriminating between that labour this one which is just stupid we always getting neutral and benefiting from the fact that neutral is a massive category while we have never used accuracy in this class because you could only ever trust it even a little bit bounce frog other part of this sort of framework at constructive here is just what's related to accuracy just want to say that accuracy is inversely proportional to the cross entropy loss and the cross entropy loss and interned accuracy can both route be related to kl-divergence that is just is predicting a probability distribution I think about learning from a probability embracing the probabilistic nature of your Pacifier and of the day probably use kale divergence as your metric and it would also be what your model is optimising in some rescaled like nice long network of related thing connect what your model is doing mismatched it's what your model is doing but it's probably not what you want to report as your man report something like macro left straightforward microf keeping in mind and in my experience it's worth keeping in mind because aprender tuner start turning against your two metric which is Macro F1 and pick the model that maximizes that value remember that your model is doing something different and might actually be staying you outside of the space that you actually want to limitation search that word count not because the macro and one is this headache all the sticks thing about how you doing on an entire subset of your date have to be creepy if I'm wrong here but some kind of lost it was more like a reinforcement learning it was directly like differential loss on your classifier no problem no it's accuracy you know like in bounce passes this is what leads to precision-recall and the reason I like that we're doing this one in particular is play gun through the note stationery call is what people have in their back in the back of their mind so like when you get to like blue scores blue scores are kind of precision recall what sometimes called accuracy is also precision-recall down regression metrics are kind trying to do something that's like bouncing precision and recall the heart of it so precision in these things is the sum of the correct prediction some of all the gases Arsenal using matrices it's the diagonal values divided by the column sum colouring Heroes Reading precision values for the problematic all neutral classifier kind of undefined because we met 0 Gas is going down so we end up dividing by 0 sing us tomatoes 20 mind that it was strictly speaking under finding this affects the bound if you think very carefully mathematically you can quite say that the bands are 021 about what you do in the case that you haven't all Zeros called high precision for this neutral category they're about the band quoted by precision the way out with articulate this is that it includes a conservative value penalises incorrect guessing on a pro class dangerous Edge case very high precision for a category by really guessing this one here example three what I did is just make a minor change once free Chevy correct is not a good classifier and something to it of sense decision is perfect for positive and negative because it just kind of withheld Albert this two cases where it could do well example 1 precision was pretty low here but it's intuitively kind of got a better grip on the positive and negative categories kantipur part of precision is recall the sum of correct predictions divided by The Sum Of All true instance using matrices if the diagonal / the Rose songs recall values for example 1 PT about all these metrics that are related to escorting is that recall trades off against precision in which it is kind of making up for the faults of the of 3 again one gets for positive one for negative supposition is perfect but recall is dismal for both of those categories because in withholding it's guesses for those classes it end up making a bunch of recall start like the F1 to listen to be pretty bad straightforward bounce play this is a nice bouncy also recall and codes of permissive only missed true case precision was conserva it's dangerous Edge cases that you can achieve very high Recall for a category by always guessing it lots of incorrect gas call Callum into deeper you think about the calculations that sees only the correct gas find about this over guests actually so for the Neutral category online of guesses about neutral miss none of cars actually very high down here it's 1 please get a category and you're sure to have perfect recall that's the some of it but in turn your precision will be hit switch reviews possessively in the score Cardiff Iris balance between precision and recall standard way of doing this is to combine precision and recall the other harmonic mean ubeda that can be used to emphasise precision or recall always bouncing them equally turn down here value is down to the Express again you should be thoughtful about which one whether you default to the sequel down do you think about this is like a bunch of people in this course are working on things like hate speech and toxicity quick use case having a system that's going to help with interventions for that problem open question whether you want the system to be biased in favour of precision or recall very few human resources for doing manual review of your mother predict precision do you want to make really good use of those humans and have them see only things that your model has high confidence are like top have the Trader messages that really hard of Heaven filtered out of your community or going to make it in optimising for there at a higher level is like saving human time if you have a whole fleet of human who can help you with this task biased in favour of Rico reviewing a lot of score a lot of texts where actually it was fine do just let them onto the path depending on what you're trying to do but I think whatever your actual situation is it unlikely that precision-recall perfectly balance morning justification for us perfectly bouncing them all the time is that we don't know what else you're trying to accomplish in the for that Friday basic example Forres here and I think line with what we think of about how the model is doing on this test simple 2nr in strictly speaking but for positive and negative because you had undefined precision for 4 positive and negative scores also undefined but typically that will be mapped as euro psych it will be mapped as 0 and it will print out of bunch of warnings for you M43 set a timer for season for positive and negative but very low recall and that's nicely reflected in the fact that the F1 scores for parsing leg or also very low 0 and 1 and you have a guarantee that the it will be between precision and recall value included by scores his one way that I tried to articulate it so it's an attempt to summarise how well your classifiers predictions for certain class k a line with the true instances where what I'm trying to bring out with a line it's like not only give you credit for the correcting full and Mrs in the car one in a symmetric way with all these things precision and recall are keeping each other in check somehow for the scalp and if you go through The Notebook on your own you'll see that theme recurring again and again weaknesses of Earth's core validation for the size of your dataset class that you're focused on or outside of it other thing that you might highlight here is that if you think about like focusing on pause here attention to the older row value Allen Valley pause everything that's off of those rows and columns all the value illustrate that what I did is your example One the corner you have 1000 state of said I change that to 100000 same across these two dates keep me in mind because obviously these are very different problems is a very different datasets and classifiers doing very different things because massive what is insensitive to that only the Neutral F1 chair that value is on its role turn mind when you do this F1 your kind of ignoring one aspect of the data class base adds Corso di similarity for binary vectors which you might remember from the first used to assess kind of how well you are a models predictions online on a set of Uruguay that's equivalent to F1 especially think on a token basis for effective Project and then as I said the intuition about scores this.com just a few these macro average thing so macro average step score uniball vs course which category that's one we've always used example basically what we're doing here it's just taking all the per class values and averaging here to the weaknesses so I default to Makro F1 because I like the fact that it's giving each class no matter its size equal weight in the final calculation and the justification for that is that in an op a very often it's the smallest classes that we care about them no sense to give more weight to the ones that are large cos maybe they're easy and Anna keep in mind that thinking about macro averaging is metrics my Outlook out of step with having actually performs in the world turn it loose on real data same class balances the data set it was developed on it makes a lot of correct prediction meaning for thing in terms of people experiencing the sea little large class doesn't mean that it any less meaningful if you think about performance in the world so in a funny way your macro average score could end of oversteer a tiny class that you never experienced as a user with the sis just as much to the kind of example that it sees all the time and keeping in mind in terms of practical application these two kind of balance each other so you can both over and understand how well your system is doing by doing macro app about system evaluation in this context Bridge choices are really we did at scores psychic reports these which just gives a weighted average based on the class size micro average score scores qualifications are exactly identical to accuracy inherit all the problems vacuous favourite option and in fact you know buying sport accuracy is it in as a poster choosing the McConville who did microwaves included here for complete stop there cos I want to be plenty of time for Atticus and I also feel like now you're armed with a kind of framework for thinking about this what value does it in code what grounds does it have it's worth asking that for any metric that you encounter and certainly if you decided to propose your own metric you'd want to kind of fully comp different consideration play R&B on Spotify where did scores will have that kept rebel favour large glasses what you want to do really depends I'm guessing encouraging you to think about what you're trying to achieve trying to do energy symmetric Accord justification for macro averaging it's just that we often do care about those small classed as we really want to get track I can be kind of dispiriting if feeling well improving on the smart glasses but you chose waited or micro F1 any impact on the number Honda performance I double my score on this tiny class and where did Mac and waited F1 says I don't care I got a little 1 over flat all of those games agent human the traitors what type of metric would be best suited for something like that you know during the answer was the Turing test have some kind of accuracy calculation right wasn't it like calling it like two out of three times of something I forget classification with a human and a machine they have to call the machine the human and the human machine like they had to swap symmetric but the lesson of time is that do people confuse humans for robot and the reverse all the time sorry I guess we did never real Turing test the human who is most often rated as a machine dear ex Shakespeare and people's rationale was no human could know that can you turn it up the Atticus America's only course assistant give you a little presentation on evaluating nlu models with harder generalisation task cell play summer start by Japan overview in the framework that Chris has been talking about which is due patient framework an arbitrarily setting aside pentesting except status introduce you to some adversarial testing literatuur where people are trying to develop more challenging generalization tasks to probat the capability volume model will conclude by sharing some of my Reece additionally your framework are very familiar with by this point it's what we've been doing in her homework and in most of her Bake Off where we find a dataset for it and then LU2 arbitrary split this dataset into training and testing new train models on the training set and evaluate on unseen testing examples play in big 3:30 we did something a little bit different than this so and they go after if you remember we did Annalise on single words we're just trying to predict an internment relation four pairs of work just joined case we found this standard framework where we are betrally set aside some examples for testing and some for train destroyed case we did something a little bit ensure that testing no words would be shared between these two generalisation next models to generalized unseen vocabulary like the high-level theme of this presentation is that I'm going to encourage you to consider breaking from the standard evaluation generalization tasks that are difficult well motivated in that questions about my locate ability introduced into the adversarial testing l Turner start with the example of question answer consider the research question Kenny model there to comprehend the passage of time very high level in ambitious research question hear that actually tried answering Stanford question answer dataset a resource that you might when using your project this dataset look like this is going to be the passenger tax in the question and the output will be the answer in this case is it go back that way you might think to answer a research question can a model earn to comprehend a passage of text model achieves human-level performance on the status able to comprehend passages and text do you think that then you'll have beaten humans at the start well done ring Souls like everything's great to do it's not actually quite that simple the station that these models don't understand language quite as deeply as you might hope they do JL actually unit to see whether this is the case training examples from the squad dataset and they systemically perturbed them to create a new adversarial test set that they then evaluate models on I'm using this as a new evaluation metre example before prediction of John la which is the correct answer but what they do is they append a single misleading sentence to the passage tomorrow Dean is the quarterback that the question is asking which is just incorrect play Gerry a new adversarial test set using this technique 16 publish models trained on squad drop for my 75% 5% F1 score on the original test 36% F1 score on this new adversarial test is not soft you might have suspected unnatural idea is we've identified a hole in the generalization capabilities of models trained on the squad dataset please Hall you say ok let's just take these types of examples include them in training it does work when we include examples in training does models learn to ignore the final sentence and they're now this patch model will now make the correct prediction of John Elway there's new patch tomorrow trend on these types of examples is now vulnerable to a different adversarial testing sentence instead of depending we see that who seems to be deeper than just growing in more training data and having a model become more robust patience perturbation of the data doesn't generalise to even similar perturbations like this or they didn't do it manually so there what is a algorithmic and then I think it's Verified by mechanical Turk that is consistent with the passage they have like an algorithm generation process for these types of sentences that are misleading but then I believe that they have other own mechanical verify whatever sentence they put in is going to be consistent with everything didn't do another experience is not this one it's I think it's in the same paper where they just add tokens we don't have to be grammatical anything just to trying to make the models fail and they get the F1 score down to 6% on that patra cereal testing day Amazon swans a little more convincing in that this is English in experiment the kids dentist 6% is kind of like throwing at random stuff that's not even really going to be grammatical English and also something you can look at your squad 2.0 tries to address this make it a little harder by adding the option to say where is the answer to the question in this passage remove from question answering to NI where in the last couple of years possible permutations I think that's a good question that I was really interested when I read this paper is not pretending upending but what have you had a training and testing data where you inserted this misleading sentence at every possible location in the paragraph good think bad I'm not sure the tomatoes and even be able to let general as across those two things that's really interesting because remember that this is like a one-way till like mess with the model but an infinite classics that's like an interesting question spell moving answer nli in last couple of years there have been a growing number of more difficult generalisation task expose the fragility of models trained on the sly in multi and IDs dataset what are the experiments that isolate the capability of models reform lexical semantic reasoning how to create serial testing vehicles is the take a promise from the snli dataset Google words that premise they exchange it for another word to generate a hypothesis sentence is the adversarial examples here and each of these examples the label for the sentence get me from a relationship between a pair of words in this paper they expose that models don't have quite the robust train semantic reasoning that you would hope they would when being trained on these massive naturalistic datasets the first one in how do we know that someone holding a saxophone play gates in about their paper and more to the nature of how ethanol I was create I was created with image captions describing little scene and what were told to do is generate a caption that must be true so they're giving a captain of an image without that generating internment relationship they say must be true of the scene for a contradiction relationship they are asked to generate a sentence that is not true at this scene that kind of like results in this more like fuzzy idea of contradiction then I kind of like strict particle lady of what are cereal about it is that these pairs don't occur in the sly today said only the first sentences do single word an exchange that word for something else this could be something that might occur in sli Eid around adversarial testing is that like it should be similar enough to what were training on that it's a Justified tour Waylon this compared to the sli tested cereal is they're doing worse than they are on the actual sli test getting better play you better so yeah so actually in this paper they have a bunch of different they break it down two different classes of words they work with they have like planets like fruit buy Class some of the classes they do very well on and then some of the classes they seem not to be exposed to those types of words as much actually see your full breakdown so if you would actually use they have one thing they do is playing it so like replacing Venus with Pluto and it seems that models trained on escalators are really bad at that I guess because it's 10% of something but in the other classes of words that I guess I'm much more familiar they get above what they get on the test at present yeah I think it's a good idea to really like take a look at the details in the steps of papers because they like really can exposed like a range of New Oscott capabilities lexical semantics there's also experiments that tried you determine whether models are capable of compositional semantics two examples from this paper by me and when what's one day just take these subject in the object and they swap there is two boring channel is there any snow child pulling woman on in this example what they do is they take Lynn a taking additive and that sentence and just move it to a different now understand that these sentences are not in the intimate relation because moving words matters a lot that's kind of the whole thing around compositional semantics what they found is by doing this they tricked the models that are trained on sly play often mistake these types of examples for and Helmut relationships yellow does this my words and tell me the experiment isolator specific compositional frame it's I think a little more like the other ones I was talking about where is just like is more why then dizzy just found that makes like a little more that is not surprising to me because particular composition of frame that I don't think with occurred that much when with little image captions something you might be wondering is like if they don't wear in lexical semantics and they don't learn compositional semantics daily doing here right I guess what I would gas is that models are learning a specific slice of lexical semantics and compositional semantics that is kind of hand tailored to the sly test set which community has been hill climbing on since it's release is actually an exciting thing for your geysers projects because intimidating to trying to get a new state-of-the-art score on sli dataset but serial testing datasets no one's been hill-climbing on them so I think there's a lot more room for innovation and improvement that you could do in your projects move on from adversarial testing on to ensure load of my research research I've been constructing artificial natural language inference datasets examples that look like this human does not kick any large rock contradicts no human angrily takes some rock in hard to pass as humans but quantifiers negation adjectives adverbs tell us a riddle tent with this dataset was to stress test Myles with learning the scandal sentences design a task specific mod the country NN model which can have jointly composed as a premise and hypothesis together by a widening of the words and composing them together up this tree structure specific model that should be particularly good at doing this kind of reasoning the first thing I did was just do a standard evaluation on my data where are betrally set aside some examples for testing and some examples for training you can see from this results table standard neural models and mytask Pacific man all she's very high accuracy on this valuation split where did some digging I found that the standard error models failed to encode the identity of verbs nouns adverbs and adjectives is pretty serious Casper S model performance perfectly what I realised was that valuation on my dataset was far easier than I expected it to be when I thought about it more I realised that every combination of qualifiers in the gate occur in my training data 100 100 g of salt these complex function words are going to appear in every possible order during train I'm all I could do well it's just one that memorises these quantifiers in the geisha assign them labels can you do is that one in construct more challenging training and testing skip that ask a question about one of model is able to do keep it very high level on baseline model that performs natural logic reasoning is I constructed a simple baseline model that does perform natural logic reasoning tomorrow to Carnon pleasantly define and ideas fair play slime model and I can set a training dataset to be fair if the simple baseline can learn at the perfect solution from the training data being bad neuro models if they're able to form the type of reasoning my baseline model is able to form should succeed at the same generalisation tasks Wi-Fi was that standard arrow Models film miserably at this new more challenging generalisation play task-specific model fails to G perfect performance pretty interesting that even something with a hard encoded to restructure was enabled assault this task I guess the calendar of here is that at first I use the standard evaluation which was interesting but by breaking away from that kind of standard framework as able to ask a farm or deep question about the capabilities of my models that's kind of just a high level of moral of the story is you should like think deeply in carefully about what you're wearing from your experiments because often that would be far easier than you expect them to breaking away from this kind of stated evaluation of arbitrary creating a training and test more challenging generalisation tasks that answer interesting specific question 