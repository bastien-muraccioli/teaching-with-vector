Alexa started so Chris is travelling this week so he's not here let's say this day we've got Margaret Mitchell who is a senior research fellow tell us about the least work defining and understanding and improving the situation with bias in artificial intelligence has a background working in NLP deep learning so I'm really interested to hear it tell me ok I'm not sure this mic is exactly picking up number 780 this work is the product of 800 different people collaborators that I tried to put a pier Stanford arceaux John's Hopkins for Facebook and Microsoft are all represented you haven't seen the set of slides before how do you say here just shouted up put off stop store bananas with stickers on them you can start your own light and better claus's you know bunches of bananas with stickers on them on telling a story to get kinda crazy meant to say yellow banana like this we might say green bananas are might say unripe bananas play me like this we might say right bananas or bananas with spots on them lightsabers that are good for potato bread beginning in a bit like this or something like this in the real world we tend not to mention the yellow NES this is because yellow is prototypical for bananas video prototypes stands from prototype Siri which goes back to the early 70s are coming out of the work out Eleanor Ross and Carly this idea that there's some stored Central notions of objects and that we access as we are operating throughout the world agreements about whether these prototypes or actual examples of objects or something like a distribution over what's likely but there is general agreement that we do have some sort of sense of what's typical all of the things in the world notice and talk about the things that are a typical this is a riddle That I heard a middle school that work a little bit more at that time heard it before turn on a terrible accident in a rush to the hospital in critical care set the boy and exclaims today or where he has them on the right otherwise known as a female what is a contrast with dark play die when they first sort of put forward this riddle a Boston University they found that the majority of test subjects overlooked the possibility that the doctor could be a men self-described feminist that this kind of ways of talking about things and assumptions that we make necessarily something that speaks to negative lyrics to how we actually store representations in our minds and how we access their presentations as we interact in the world this affects what we can learn when we're running from text from 2013 where they took a look at what was sorted most likely what would you wear it if you were just wearing from Rotten what were some things that were common in the world actor something like murdering was 10 times more likely than blinking this is because people tend not to mention this typical things that go without say and to mention things like blinking and breathe atypical events like murder the kind of things and he can learn from tax that we put out in the world because it's been subject to all of these filtering processes that we have as humans before we are communicate in particular is known as human reporting bias which is that the Frequency with which people right about actions outcomes or property collection of real-world frequencies of the degree to which a property is characteristic of a class of individuals but there's a lot more about how I actually processing of world and what we think is remarkable everything a system pin lock machine learning paradigm one of the first steps is to collect and potentially annotate training data is there a model can be trained Media can be filtered rank mate aggregated generated in some way people see the output this is a relatively straightforward pipeline but at the very start collecting the data itself or a host of different kinds of human biases so things like stereotyping things like Prejudice things like a racism and that's embedded within the day before we collect it collecting data further biases become introduced so things like sampling errors confirmation bias outgroup bias and I'll talk about these feel free to ask questions as I go interactive throughout play some The biases that I think are relatively important for work in AI machine learning go into but some of the ones that I've sorted become the most aware of working in this space I'll this SATs and I'll go through it Elizabeth not reporting bias earlier which what we can learn from Tess sample of a kind of bias that really affects what we can learn from Texas election set a timer that we a long time so we get dinner annotated we do something like Amazon's mechanical Turk of workers across the world it's not even sort of uniform distribution it's actually concentrated in India the US and in Southern Europe so there's leaves outer South America that she said Africa this week's had a lot of China timer things that will be able to learn about the world when we have things and 80 then via sacra imaginary by its which is the tenancy to Sialkot members as more alike than in group numbers and this is going to affect what people are able to describe and talk about when annotating things such as emotion example we have used to like and durable puppies on the liquor and I looking at his fork all different black cats very different in different ways but the two puppies look at the cat and I C for cat understand how that also extends to human cognition and how we also processed this is the sense we have that the alright that were in the people that we interact with those of the kinds of people that are new play Elvis is somehow less nuanced has less detailed to them it's a trick or my spy on us in order to help us process the world but it affects how we talk about it and it affects further Howie and it sleeps to stuff like biased a representations so it's possible that you have an appropriate amount of data for every possible human group you can think of your data it might be the case that some groups are represented less positively than others and if we have time or go into a longer and longer example of that this is a issue that came up when you were getting some annotations for inclusive images competition Anna take things like bride and wedding and Groom and we found that given three different kinds of bride wedding and Groom images set alarm or Western European American got the appropriate label words to Scots out of more generic person council labels able to actually he's out what's actually happening in these images issuer bias in interpretation when the model outputs it's decisions so when when is shoe is confirmation bias the search for interpret favour recall information in a way that confirm spread the single build end-to-end systems and try and test their hypotheses what kind of just testing it towards things that we want to be true and analysing the results in a way that will I help confirm what we want to be true the generalization which is coming to conclusions based on information that's to general or not specific enough that having a lot of times in the analysis of deep learning model resource series that there's some kind of general conclusion that can be taken away when we is actually just an effective release feud state close the related to overfitting which is kind of the machine learning version of overgeneralization which is your soul making predictions and outcomes but it's based on a small set of possible features so it's not actually capturing the space of the correct features for the outcome the desired output prediction crackly there's also correlation fallacy which is confusing correlation with causation and this happens a lot again and talking about what machine learning models are running and models are running in particular we're just because things happen together doesn't mean that one is causing the other boats models don't tell you anything deep learning models directly don't tell you anything about the causal relations and so it's easy to say that some output that is predicted based on a correlation is actually something that scored examples of this chair apprentice who was automation bias and this really affects the machinery models who put out there in the world that then get used by people in systems like justice system tendency to favour the suggestion predictions of models that output predict different kinds of questions of another human even in the face of contradictory evidence telling you this this is the score this is the risk of this individual they were more likely they think it's true because it came out of her mathematical play sort of see the something more objectives and the more mathematical that something's going to be more true than humans somehow automation bias rather than is kind of pipeline that we have in machine learning human bias coming in at the very start in the data and then human bias coming in and data collection annotation and infinite getting propagated through the system as we train on that data and as we start pulling outputs based on at data as people at on that data and it sprayed the feedback loop where the times of things that we output for people to act on further training data for input amplifying even further is different kinds of implicit biases the bias network effect or bias laundering I like to call it human data perpetuates human biases and then as machine learning or deep learning learn from him and data there is also suppliers network effect play wanna stay clear of the idea that if I say by Oasis someone says bias that equals bad it's a little bit more new one stone that so there are all kinds of things that people mean when they're talking about fire the same bias can be done in some situations and Dad and some situation we can we talk about the bias of an estimator which is the difference between the predictions and the and the truth the Grand Tour the bias term in linear regression also have committed by a seasoned it sorted out that in the beginning and all of those are negative or have to be happy be seen as negative so optimism is another kind of bias that we can have that affects our worldview and the way we sort of processing recency bias in in confirmation bias just ways that our minds can like and handle the explosion of all the different things that can be true in the world and put it down to something tractable that we can sort it operate within the real world algorithmic bias is what a lot of people mean in headlines and what that we're talking about more about unjust unfair or prejudicial treatment of people that can output other automated decision system is really an unjust unfair or prejudicial treatment of people in this face right now is focusing on trying to understand what does it mean to be on just from an algorithm what does it mean to be unfair from an algorithm and how can we handle this how can we sort of mitigators issues in order to be able to keep the building technology that's useful for people without worsening social divides and I got the Guardian put it really well a few years ago and it's all done that works might be said to write their own program towards goals set by humans using data collected for human purposes good even by accident the computer is what a fight injustice Alanis amplifi injustice idea about what that can mean play safety planning research that's taken off in a past few years is predicting criminal behaviour how many of you like Millie with predictive policing half of the class algorithms to predict areas to deploy offices where crime is considered to be likely to occur train offers is based on where police officers have already gone and made a rest so the system does simply burning the patterns of biospec humans have in where is the seagull and where they are trying to decide to find crime and then reflecting and system hornbill on some of the top spots where people have been arrested the same of nuts the same thing as where crimes have been committed a rough something made it means that the other areas that might be explored for prime don't get exported Paul that worsens the situation and some neighborhoods get really acutely focused attention on them and heightens the chances of serious repercussions for even minor infractions that means arrests and that means the feedback loop of day that you will get an arrest in this place if you go there sort of related issue in that space is a predictive sentencing there is a really nice order called that came out from propublica a few years ago discussing defendants are booked in jail they respond to a questionnaire called Campus and their answers are fed into the software system that generates scores that correspond to the risk of recidivism that's the risk of making a crime again the questions I used to gather data on the different socioeconomic status neighbourhood crime employment status and other factors in order to reap some predictive prediction of an individual's risk happening is that it ends up focusing on the queue bias issues that humans have and propagating it back sting that looks like an object of score what more likely and to be convicted of a crime if you're right even if you baby exact same time system for pick-up on practice back to say that people who are blacks are more likely to have like this and would like to make a plan again automation the output of a system office of overgeneralization correlation fallacy confusing things that are occurring together as being somehow causal area of research and startups looking at predicting criminality and particular from things like face images a cold face septum they're based in Israel and they claim to be able to individual images with computer Vision and machine learning technology for profiling people and a viewing a personality is only on her facial image anything like high IQ White Collar offender padophile and Tara and clients are homeland security what's another what's other countries dealing with sort of public safety issues details about their methods are sources of training data or their quantitative research automation bias people will tend to think it just works even when it doesn't work well there was a paper that came out within a similar line predictive criminality or reporting to predict criminality from individual face images handsome results send some more details about the data that we could play Dig into that understand where are these kinds of planes coming from an article that was posted on archive near the end of 2016 using less than 2000 + we put images of faces coding wanted suspect ID pictures from specific region even though song is very small training datasets you're able to predict whether or not someone was likely to be a criminal greater than 90% accuracy and they got so lost in this this idea that it's sort of funny that read to just take a step back and realise what actually happening so for example one of there really great it's signing claims was at the Angel Theatre from nose tip 220 corners is an average 19.6% smaller for Criminals band 4 Non criminals smiling and you know exactly what kind of images people would use when trying to put out one of criminal pictures probably not really happy pictures what's in a confirmation bias get so lost in the correlation in the feedback Loops that you end up overlooking is all really obvious kinds of things example of selection bias experimental bias highest correlation Galaxy and feedback Loops are coming together to create a deep learning system that people think it's scary and can do things that it can actually do what are the issues with this was that the median loved it like it was all over the news and has been similar kinds of things happening again and again and wants to sell that story and so it's part of our daughter's research shows that people who work on this stuff to be very clear about what the technology is actually doing and make a distinction between what you might think it's doing and what is actually doing another issue that has come up recently and is claiming to be able to predict internal quality but specifically ones that are subject to discrimination and lots of opportunity the weather was this work that came out that claim to be able to predict whether or not this homosexual just based on single face images report to know that the images of the use in the study included images that were from dating websites for people self identify as straight or gay and identified as well they were looking for a partner who is straight or gay the sources of the training data before I go on TV just from that what the issue might have been was actually anything about rainbows but that's really unfortunate to do with the presentation of the the social self when you're trying to break them but attractive partner on a website and what to do with how you work updated conclusion on supported at all by the data or by the study but seems like consistent with a pre-made of hormone theory of sexual orientation gay men and women tend to have gender atypical facial morphology authors actually were prenatal hormone Theory special doctor in the name so maybe that's the thing this is a professor Tucson I'm ready if anyone wants to see me my colleagues decided we play around with this a bit of what we found was that a single decision tree and so I'm kind of assuming you guys know what a decision tree is son wearing makeup away glasses but it's pretty close to the accuracy reported in the paper about the internal that says nothing about any of that and doesn't know about the physical presentation the things that are on the surface more about how people are presenting themselves and what is happening internally that's recently kind of being overlooked is that deep learning and somehow it sorted that somehow magically going beyond surface level working on the surface I'm working well and in the face of confirmation bias and other kinds of bias factors it's easy to assume that something else is happening that's nice examination of for example simple baseline play radio tracks and these kinds of things can just be ignored and not noticed it all temple experimental bias and correlation fallacy now I'm gonna talk to talk about measuring algorithmic bias so I just said a lot about different kinds of biases that come in in the day there in the collection in the interpretation of the results actually quantitatively measuring different kinds of biases see things that immersion a few different works and really Tyson nicely to a lot of fairness work is this idea of December gated evaluation what's the date of the valuation you evaluate across different subgroups as opposed to looking at one single score for your overall testing dataset training testing data split you can a train on there on your given training day that your test on your giving testing Danube recall escort things like that but what that masks is how well the system is actually working across different kinds of individuals in a Costa different subgroups and so one just straightforward way to handle this is to actually evaluated with respect to those different subgroups so crazy for each sort of subgroup production pair volume eight look at women face to text in men face detection and look at how the the error rates are or different or similar part of this is to look at things intersectional and combining things like gender and race at the same time and see how those are the areas on those sorts of things are changed and holder different across different instructions My Fire by Kimberly crunch and she pioneered research race theory and to discuss the story of Emma deGrasse in feet who was a woman at General Motors decline of the company is hiring practices discriminated against white women can a court opinion the judges ruled that retired many women for a secretarial positions factory roles cannot have discriminated against black women do was look at the intersection of the two and understand that the experience there might be fundamentally different than any of the experiences Fabrics in isolation becomes true when you start looking at errors that are regularly made in deep learning systems so we've been able to uncover a lot of different kinds of unintended errors by looking at only at the December dated evaluation but also at intersectional to segregate evaluation I'm gonna walk through a bit harder going to be review for most of you but I think it's really important to understand this because it also tiles to how we measure fairness and when we say like talking about This Is Us ok are you ok so you have more predictions and references play hey look a positive binary classification protea where is the country says something is true and model predicted through it positive if the Bantry says it's false salsa tonight is that the kind of different issues that arise are false negatives and false positives so in positives sell something is negative but the model predicts that it's positive vice versa basic kind of basic breakdown of errors you can get a few different m actually previously mapped to a lot of different fairness criteria something like a female vs male patient results precision and recall which is relatively common in an LP equal recall across the Sudbury name as the furnace criteria of equality of opportunity through the mass that I mean this is basically just adjust the main points it does that give him that something is true in the ground truth model should predict that it's true at equal rates across different subgroups for the equivalent to having the same recall across different subgroups play on decision across different subgroups is equivalent to a furnace criterion called predictive Perry has Fearless been defined again and again and originally if somebody has definitions came in 1966 following the civil rights act of 1964 they were reinvented a few times and most recently reinvented in 2016 sort of boiled down to this December data comparison across the metric centre being roughly equivalent to what we get from the confusion matrix specifically in classification systems furnace metric to use what are the different criteria you want to use to look at the differences across down to the trade-offs between false positives and false negatives so this is the same problem that you're dealing with when you're just figuring out how to evaluate generally there's no one fairness criterion that is the fairness criterion to rule them all deciding which one is better than the other is the same as kind of trying to decide which is better precision and recall right it's what the problem is that what you're interested in measuring false positives might be better than false negatives and so you want to prioritise something like a hospital delivery this privacy and images so here a false positive is something that doesn't need to be blood gas board that is kind of a bummer something that needs to be blurred as not and I can be identity theft it so much more serious issue and it's important to prioritise the evaluation metrics that stress the false negative rates negatives might be better than positivism spam filtering so you could be an email that spam not hot so you ceiling annoying pause the Hero by email for the spam and removed from your inbox loved one you can be a can be a lost job offer something like that I can unintentionally time just outcomes and some of the things to do for some of the things to be aware of you either lack of insight into sources of bias in the data and a model insight into the feedback Loops from the original data that's collected as an example of what humans do to the data that's and repurpose reuse after Nan and then further fadden so I could careful disaggregated evaluation looking at the disparities that differences between different subgroups in order to understand this buyers this difference across the Sudbury human biases in interpreting and accepting and talking about the results which then disciples in the hype around AI right now what is the influence how it I evolves think of this in terms middle term and long-term objectives today we might be working on some specific task or something like that active I have a slightly longer to objective of getting a paper published or if you're an industry like getting a product launched whatever it might be from there we might see your next Endpoint that's getting a warder you know maybe concert famous for something for a few minutes and then like that and what is a longer-term objectives that we can work towards as well at the same time play a positive outcome for humans and their environment I'm just kind of focusing on his local is local atom and sort of local paper by paper-based approaches to solving problems you can also kind of think about what's the long-term objective where does this get me as I trace out and evolutionary path for artificial intelligence down the line and 10 years 15 years 20 years address vs by thinking know how can the work I'm interested in now that Focus to help others and balls talking experts and kind of going outside your bubble speaking across interdisciplinary Fields by College of science which I just talked about so let's talk about some things we can do what is data the issues of bias in fairness remodels really come down to the day machine learning and deep learning it is really not seen at 6 few datasets that people use their Altair that's what people use and there's not a lot of analysis done on on how well these datasets different truths about the world how problematic they might be wide area that needs a lot of future needs a lot of feature additional what's the weather in understanding the data Skuse and the correlation excuse in the correlations that might be problematic in your data then you can start working on either models that address those or data augmentation approaches in order to sort of make the data set a little bit better or little bit more representative of how you want the world to be if I send products to abandon the single training set testing set from similar distribution approach to advancing the party magic Sandeep letting you know we tend to have the training set and a testing site and then that's what we sorted benchmark conan.io retires but the point is as you move around if it testing starts for going to get vastly different results by keeping in the sister of one training testing testing dataset Caroline you're really likely to not notice issues that might otherwise be there and one way to really focusing on them is having a hard set sisters that you really want to make sure the model does well on so the other things that are particularly problematic would be really harmful to individuals and if they were to experience the output collectors in a small tester and then it's really easy to evaluate on that test as you benchmark improvements on your model as you add different kind of things to your model in order to see not just hire models doing overall in terms of testing data sets but how well you're doing in terms of this example you really want to do well on that you know that is going to be a problem if it doesn't do well that you might want to prioritise and to fix above daggers deprivation in overall accuracy also important to talk to experts about the additional signals that you can incorporate water help with this understanding that eschews called facets and it's just available there and it's a really handy kind of visualizer for slicing understanding you know what some of the differences are between different subgroups and different representations and you can sort out again and explore a bit more so this is just a sort of help people come to terms with the data that are actually using and and where there might be unwanted associations or missing missing Hannah Futures another approach that's been put forward recently specifically on the data side is this data data sheets for data sets up this is the video that when your release the data said it's not enough to just released the dataset with like some pretty graphs and like talking about basic distributional information you need to talk about food at annotators where where they were within the Entertainer agreement was with their background information was motivation for the dataset all these other kinds of details so now you actually know that this isn't just a dataset this is a data set that has these specific diocese there's no such thing as a data set that you think by it's in some way virtue of the fact that it's collected from the world's at the subset is a is a biased set of the world in some way the point is to make it clear what it is how it is biased what are the what are the various Pisces that's that and put nobody in the dataset so that's one of his ideas between behind datasheets for datasets releasing his status as publicly what's better well get to machine learning here are a couple technique for they like to use I'll talk about 2 is bias mitigation which is removing the skin or for problematic help moving stereotyping sexism racism trying to remove these kind of effects from the model this is also sometimes called the buyer singer run by saying that's a little bit of a misnomer because you're you're generally just kind of moving around by a space on a specific set of words for example and so to say it's unbiased this is not true but you are kind of mitigating bias with respect to some certain kinds of information that you provided with including which is an adding signal for desired variables so that's kind of the opposite side of his medication so increasing model performance with attention to subgroups or data slices with the worst performance address including adding signal for underrepresented sub qualitatively wireless multi-task learning turn that you guys have study multitask burning which is great and so tell you a bit about a case study what I did in collaboration with a upenn world wellbeing create a system that could over clinicians if there was a suicide attempt that was a man play 18 understand the feasibility of these cancer diagnosis when there are very few training training instances available so that similar to kind of the minority problem in datasets work we had two kinds of deer when was the internal data which was the electronic health record that was either provided by the patient or from the family including mental health diagnosis suicide attempts or completions if along with a the uses of the present social media data internal data that we do not publish on but that we were able to work with conditions on and ordered understand if our methods are actually working turn on data the proxy data the stuff that we could have a publisher on and talk about was based on Twitter using regular expressions in order to extract phrases in porterfield that had something that was kind of like diagnosis so something like I've been diagnosed with X or I tried to commit suicide I'm kind of the proxy data and the corresponding social media fields for for those individuals for the actual diagnosis the movie art in clinical medicine until this work that has been more recently but it's sort of the single task logistic regression logistic regression software you have some input features and then you're making some output protection and slight true or false layers and start making deep learning which is much healthier have a bunch of tasks in order to do a bunch of logistic regression tasks for clinical environment can you spell site ask wearing which is taking the basic people in Monaghan adding a match of heads to it predicted jointly at the same time and here we had about self diagnosis we predicted things like depression anxiety and post-traumatic stress disorder engender because this is something that said the connacht installed that actually I have some correlation with some of these conditions and that they actually used it and making decisions themselves for whether or not someone was likely to attempt suicide or not I have also used this idea of comorbidity so multitask morning is actually kind of perfect for comorbidity in clinical I have one condition you're a lot more likely to have another so people who have post-traumatic stress disorder are much more likely to have depression and anxiety inspire T10 to become or bed so people have one off and have the other this point of the idea that perhaps are some underlying representation that is similar across that can be leveraged sanity falling model with individual heads further specifying a each of the different kinds of conditions I found was that as we moved from logistic regression for the single pass deep learning to the multitask deep learning we were able to get significantly better results suicide rest case where we had a lot of dinner as well as the post-traumatic stress disorder case where we had very little data the behavior here with a little bit different so going from logistic regression to single task deep learning when we had the suicide Bridge single taxi pudding model working better than a logistic regression model but when we have very few instances this is where the Deep learning models really struggle they want more and so the logistic regression models were actually much better cards for the comorbid different kinds of conditions for different kinds of tasks and that related to you know whether or not the person might be coming suicide we were able to answer accuracy way back up again and and it's roughly 120 at-risk individuals that we are able to collect our in a suicide case that we wouldn't have otherwise been able to risk what are the approaches with coconuts was to contextualise and consider the apical dimensions of releasing this kind of technology it's really coming in and I'll keep papers to give example the area where we decided that giving examples of like to press language could be used to discriminate against people like you no job interviews or something like that you know what sort of armchair psychology approach that was important to talk about the technique in the utility of low-fat ask Worthing in a clinical domain and for bringing inclusion of underrepresented the fact that there was a lot of risk about depression and anxiety and how does kind of things could be predicted and so we tried to take a more balanced approach year and since then I've been putting at the considerations in all of my papers and is becoming more more common actually so another colour of approach that's now turning it on a tab or you're trying to remove some effects and mitigate bias in some way is adversarial note password so just not sorry about the other Siri ok I didn't adversario case that you have a few heads is predicting a mean task another one is predicting the thing that you don't want to be affecting your models predictions something like whether or not someone should be promoted based on their performance reviews and things like that I don't want that to be affected by the ideal gender is Independence of a promotion decision you can you can create a model for this that actually put that independence criteria in place by saying a I want to minimise my last on a promotion on maximising my last on the journey that is just predicting tender and then getting the gradient so removing the effect of that singer this is another adversarial approach so you might have been to Norwich like Turner never settle network so this is like to discriminators two different task when is trying to do the tasks that we kill about and the other one is removing the signal that we really don't want to ring interplanar dancing predictions the way of putting this into practice that the probability of your output predicted that given the the ground truth and you're sensitive active you like Sundar is equal across all the different sensitive attribute 34 plus all the different genders whatever party of Opportunity in supervised learning being put into Thurles definitions it's equivalent to Eco recall across different subgroups in earlier and that's a model that will actually implement that are help you achieve that singer classifiers output decisions should be the same across some sort of characteristics given with a cracked assistant should be Central Park so I'm going to go until the case that you know and enter and Google has been working on my colleagues have been working on that is an NLP domain in the office of the Aspire sisters find out more about this work on in papers at 8 as in 2018 and that start with toriel 2019 ringing mitigating unintended bias in text classification just came out of conversation AI which is a this Scotland accept Google it's a kind of spin-off company conflict for that Focuses on trying to stop team is trying to use deep learning to improve online conversations elaborate with a ton of different different people to do that works is so you can find out to on suspected ppi.com so giving some phrases like you're a day put out a tax if your score associate it's that like pt-91 associating frequently attacked identities with toxicity so this is a kind of false positive bias so I'm a proud tall person gets a model toxicity score of PT 18 I'm a gay person gets a Texas in a model score of 0.69 has the the term Gate has to be used in really toxic situation to learn that gay itself is toxic but that's not actually what we want and we don't want these kinds of predictions coming out of a model the voice of largely caused here by the dataset imbalance again this data cannot coming and orangutan frequently a text I don't know he's a really overrepresented in toxic comments board of toxicity towards lbg TQ identities play horrible to work on the staff at like really truly affect your personal and one of the approaches that the team took was just to add non-toxic and data from Wikipedia so helping helping model to understand that these kinds of terms can be used in you know more positive thoughts of contexts challenges with measuring a how well the system was doing is that there's not a really nice way to have toxicity evaluation in real-world conversation it can be kind of anyone's gas years of a specific sentence and if you really want to control for a different kind of subgroups or intersectional subgroups that it could even harder to real good data to evaluate so properly so what the Tina doing was developing a synthetic tape time is like a bias mad libs where you take templates sentences and you use those for evaluate evaluation you'd want to use in addition to your target downstream kind of dataset but this helps you get at the buyer sees specifically so some templates phrase like I'm a proud blind person and then filling in different subgroups don't want to release the model unless you see that the scores across these different kinds of template sentences with some what sentences are relatively kind of the same across all the different model runs summer sunshine that they made in this was that the data sets Anna Tobias and they didn't do any cars or now so because they were just trying to focus in particular on the toxicity problem you guys know I like your bread and butter augmentation approaches and both Wikipedia kind of approach as well as actually collecting positive statements about lgbtq identity Google when we go out in Antarctica two people identify as queer are people with a friend to do in my pocket about this in a positive way we add this as a date actually no that doesn't can be a positive thing to measure the performance here again it's looking at the differences across different subgroups and trying to compare also the subgroup performance to some sort of general distribution so heavy is a received and where is the is essentially the probability that a model will give a randomly selected positive example a higher score than a randomly selected at negative example where you can see from toxic comments and non-toxic comments with a auc this is example with a higher UC for the models during a relatively good job at separating his two kinds of comments Tobias if they've defined in this work so low subgroup performance means that the model performs worst answered your comments than it does on comments overall introduce to measure this is called subgroup auc another one of subgroups what's the somatic lift course comments from some subgroups fire to the right there's also the same background positive and negative shifting to the left the way to the right of the left and there's just kind of different metrics that can define it please there is obvious through not only just looking at you know qualitative examples am and general valuation metrics but also focusing in on some of the key metrics to find for this work is part of a sea based approach what's the significant differences in the original release switch still account for any of yous unintended by ACS and downstream releases which did which incorporate at this kind of normative data that said the sort of things that we thought the model should be learning the last thing to keep in mind as you sorted developing and work towards the deeper better models is to release responsibly this is a project I've been working on with a ton of different people come on cards from other report a little bit of like the next step after data sheets for data sets where information about the data model cards for model reporting Focuses on information about the model so it catches what it does how it works why it matters key ideas here is disaggregated intersectional evaluation there was not enough anymore to put out human-centered technology that just tell some vague overall score associated to it you actually understand how it works across different subpopulations and you have to understand what the data is telling you that example details but model card would have and who was developed by with the intended uses so that it doesn't start being used in his that's not intended to be used actors that are likely to be affected by disproportionate performance of the model so different kinds of identity groups things like that the metrics that you're deciding to use in order to understand the fairness of the model or the different performance of the model across different kind of subgroups of factors turn about the evaluation data and training data pause ethical considerations so what were some of the things you took into account or what some of the risks and benefits play relevance to this model and an additional caveats and recommendations so for example in the conversation they are case they working with synthetic data so this is a source of limitation of the evaluation that's important to understand because it can tell you a lot about the biases but doesn't tell you a lot about how it works generally play komponen in the quantitative section of the model card is to have this both intersectional and a separate evaluation and from here you trivoli get to different kinds of furnace definite set a parody across subgroups across you're getting to something that is mathematically fair these kinds of approaches take into account all these kind of things we can move from majority representation of data in r models like the first representation from oracle AI thanks 